{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-14T06:55:25.871335Z",
     "start_time": "2024-08-14T06:55:25.867751Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "wandb.errors.term._show_warnings = False"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:55:25.885531Z",
     "start_time": "2024-08-14T06:55:25.881980Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.login()",
   "id": "fd8919ef9148c5cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset that i am using has two configurations:\n",
    "\n",
    "| Name    | Train | Validation | Test |\n",
    "|---------|-------|------------|------|\n",
    "| Split   | 16000 | 2000       | 2000 |\n",
    "| Unsplit | 416809| n/a        | n/a  |\n",
    "\n",
    "I will be using both configurations and test with a smaller corpus for training and then a bigger one."
   ],
   "id": "987f3628c73a920e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:55:30.234105Z",
     "start_time": "2024-08-14T06:55:25.900769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitted_ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
    "# unsplitted_ds = load_dataset(\"dair-ai/emotion\", \"unsplit\")\n",
    "\n",
    "# df_unsplit_train = unsplitted_ds['train'].to_pandas()\n",
    "df_train = splitted_ds['train'].to_pandas()\n",
    "df_test = splitted_ds['test'].to_pandas()\n",
    "df_validation = splitted_ds['validation'].to_pandas()"
   ],
   "id": "856b404006b1ede6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Fields\n",
    "The data fields are:\n",
    "\n",
    "**text**: a string feature.|\n",
    "\n",
    "**label**: a classification label, with possible values including: \n",
    "\n",
    "0 -> sadness\n",
    "\n",
    "1 -> joy\n",
    "\n",
    "2 -> love\n",
    "\n",
    "3 -> anger\n",
    "\n",
    "4 -> fear\n",
    "\n",
    "5 -> surprise"
   ],
   "id": "5857a135962f40b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:55:30.239704Z",
     "start_time": "2024-08-14T06:55:30.234614Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.head()",
   "id": "733ad5fcf12db0f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The number of data for each label. It can be seen that the data is a little unbalanced in the splitted training dataset. Same story applies to unsplitted dataset",
   "id": "7614a91df1874624"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:55:30.244779Z",
     "start_time": "2024-08-14T06:55:30.239704Z"
    }
   },
   "cell_type": "code",
   "source": "df_train['label'].value_counts()",
   "id": "b108d812f1182545",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    5362\n",
       "0    4666\n",
       "3    2159\n",
       "4    1937\n",
       "2    1304\n",
       "5     572\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:55:30.247827Z",
     "start_time": "2024-08-14T06:55:30.244779Z"
    }
   },
   "cell_type": "code",
   "source": "# df_unsplit_train['label'].value_counts()",
   "id": "d1572e572ab8bd78",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SnowballStemmer:\n",
    "\n",
    "- After processing the word through all these rules, the stemmer produces a stem—a simplified version of the word that represents its core meaning. This stem is not always a valid word in the language but is a useful representation for analysis purposes.\n",
    "- For example, “running” becomes “run,” “studies” becomes “studi,” and “better” becomes “better” (sometimes the word is already in its simplest form)."
   ],
   "id": "a4113a174b390936"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:55:30.588306Z",
     "start_time": "2024-08-14T06:55:30.248838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "list(api.info()['models'].keys())"
   ],
   "id": "45e1f5f090e7a994",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasttext-wiki-news-subwords-300',\n",
       " 'conceptnet-numberbatch-17-06-300',\n",
       " 'word2vec-ruscorpora-300',\n",
       " 'word2vec-google-news-300',\n",
       " 'glove-wiki-gigaword-50',\n",
       " 'glove-wiki-gigaword-100',\n",
       " 'glove-wiki-gigaword-200',\n",
       " 'glove-wiki-gigaword-300',\n",
       " 'glove-twitter-25',\n",
       " 'glove-twitter-50',\n",
       " 'glove-twitter-100',\n",
       " 'glove-twitter-200',\n",
       " '__testing_word2vec-matrix-synopsis']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:55:52.051986Z",
     "start_time": "2024-08-14T06:55:30.588815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ss = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm')  # english tokenizer trf -> accuracy | sm -> efficiency\n",
    "word2vec = api.load(\"word2vec-google-news-300\")  # Load the pretrained Word2Vec model\n",
    "print(\"models imported!\")"
   ],
   "id": "206d9268bc57e5d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models imported!\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:56:56.842105Z",
     "start_time": "2024-08-14T06:55:52.051986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_preparetion(sentence, nlp):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z@#.\n",
    "    sentence = re.sub(r\"[^a-zăâîșț@# ]\", \"\", sentence)\n",
    "\n",
    "    # # Tokenize the preprocessed sentence\n",
    "    tokenization = nlp(sentence)\n",
    "\n",
    "    # 4. Remove stopwords and empty tokens and split sentence into words\n",
    "    list_text_preprocessed = [\n",
    "        word.text for word in tokenization if word.text not in sw and word.pos_ != \"SPACE\"\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(list_text_preprocessed)\n",
    "\n",
    "\n",
    "def text_vectorization_word2vec(sentence, model):\n",
    "    words = sentence.split()\n",
    "    words_embeddings = [model[word] for word in words if word in model]\n",
    "    \n",
    "    # if there are no words in the word2vec\n",
    "    if not words_embeddings:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the word vectors to get a single sentece represenation\n",
    "    return np.mean(words_embeddings, axis=0)\n",
    "\n",
    "def text_vectorization_word2vec_weighted(sentence, model, train_tfidf_dict):\n",
    "    words = sentence.split()\n",
    "    words_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        weight = train_tfidf_dict.get(word, 1.0)\n",
    "        if word in model:\n",
    "            words_embeddings.append(weight * model[word])\n",
    "    \n",
    "    # if there are no words in the word2vec\n",
    "    if not words_embeddings:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the word vectors to get a single sentece represenation\n",
    "    return np.mean(words_embeddings, axis=0)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Preprocessing\n",
    "df_train['text'] = df_train['text'].progress_apply(lambda x: text_preparetion(x, nlp))\n",
    "df_test['text'] = df_test['text'].progress_apply(lambda x: text_preparetion(x, nlp))\n",
    "df_validation['text'] = df_validation['text'].progress_apply(lambda x: text_preparetion(x, nlp))\n",
    "print(\"PREPROCESSING!\")\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = CountVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(df_train['text'])\n",
    "train_tfidf_dict = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "X_val_tfidf = vectorizer.transform(df_validation['text'])\n",
    "X_test_tfidf = vectorizer.transform(df_test['text'])\n",
    "print(\"TF-IDF!\")\n",
    "\n",
    "# word2vec\n",
    "df_train['embeddings'] = df_train['text'].progress_apply(lambda x: text_vectorization_word2vec(x, word2vec))\n",
    "df_test['embeddings'] = df_test['text'].progress_apply(lambda x: text_vectorization_word2vec(x, word2vec))\n",
    "df_validation['embeddings'] = df_validation['text'].progress_apply(lambda x: text_vectorization_word2vec(x, word2vec))\n",
    "print(\"WORD2VEC!\")\n",
    "\n",
    "# weighted word2vec\n",
    "df_train['weighted_embeddings'] = df_train['text'].progress_apply(lambda x: text_vectorization_word2vec_weighted(x, word2vec, train_tfidf_dict))\n",
    "df_test['weighted_embeddings'] = df_test['text'].progress_apply(lambda x: text_vectorization_word2vec_weighted(x, word2vec, train_tfidf_dict))\n",
    "df_validation['weighted_embeddings'] = df_validation['text'].progress_apply(lambda x: text_vectorization_word2vec_weighted(x, word2vec, train_tfidf_dict))\n",
    "print(\"WEIGHTED WORD2VEC!\")\n",
    "# df_unsplit_train['embeddings'] = df_unsplit_train['text'].progress_apply(lambda x: text_preparetion(x, word2vec))"
   ],
   "id": "bb361d4e99374c0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [00:51<00:00, 312.42it/s]\n",
      "100%|██████████| 2000/2000 [00:06<00:00, 318.41it/s]\n",
      "100%|██████████| 2000/2000 [00:06<00:00, 319.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING!\n",
      "TF-IDF!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [00:00<00:00, 59220.36it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 59128.84it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 59134.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD2VEC!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [00:00<00:00, 37708.05it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 38450.75it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 36339.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTED WORD2VEC!\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:57:26.432650Z",
     "start_time": "2024-08-14T06:56:56.842614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save preprocessed dataset\n",
    "df_train.to_csv(\"./data/split_train.csv\", index=False)\n",
    "df_test.to_csv(\"./data/test.csv\", index=False)\n",
    "df_validation.to_csv(\"./data/validation.csv\", index=False)\n",
    "# df_unsplit_train.to_csv(\"./data/unsplit_train.csv\", index=False)"
   ],
   "id": "77e032e389305706",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T06:57:26.446329Z",
     "start_time": "2024-08-14T06:57:26.432650Z"
    }
   },
   "cell_type": "code",
   "source": "df_train[:100]",
   "id": "beefb04dc41ec056",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 text  label  \\\n",
       "0                                  nt feel humiliated      0   \n",
       "1   go feeling hopeless damned hopeful around some...      0   \n",
       "2              grabbing minute post feel greedy wrong      3   \n",
       "3   ever feeling nostalgic fireplace know still pr...      2   \n",
       "4                                     feeling grouchy      3   \n",
       "..                                                ...    ...   \n",
       "95    feel like throwing away shitty piece shit paper      0   \n",
       "96  starting feel wryly amused banal comedy errors...      1   \n",
       "97  find every body beautiful want people feel vit...      1   \n",
       "98  hear owners feel victimized associations assoc...      0   \n",
       "99  say goodbye fam sad crying feel like heartless...      3   \n",
       "\n",
       "                                           embeddings  \\\n",
       "0   [-0.19498698, 0.1408081, 0.061035156, -0.08772...   \n",
       "1   [0.10611979, -0.01570638, 0.005818685, 0.07367...   \n",
       "2   [0.045369465, 0.06301626, -0.105163574, 0.0296...   \n",
       "3   [0.12252372, 0.025983538, 0.008736746, 0.06814...   \n",
       "4   [0.18334961, 0.21044922, -0.14233398, -0.03942...   \n",
       "..                                                ...   \n",
       "95  [0.092681885, 0.009773254, -0.048070908, 0.111...   \n",
       "96  [0.048014324, 0.08087158, 0.0011461047, 0.0941...   \n",
       "97  [0.025824653, -0.019510904, 0.05770535, 0.0679...   \n",
       "98  [-0.024902344, -0.024559868, 0.005533854, 0.00...   \n",
       "99  [0.037121, 0.038829986, -0.007452102, 0.102466...   \n",
       "\n",
       "                                  weighted_embeddings  \n",
       "0   [-0.7648797, 0.81191665, 0.64800817, -0.506402...  \n",
       "1   [0.6533486, -0.23902734, 0.17611901, 0.4855068...  \n",
       "2   [0.41217908, 0.46653095, -0.67341155, 0.289130...  \n",
       "3   [0.9110772, 0.13422604, 0.13133731, 0.53929806...  \n",
       "4   [1.0802882, 1.1289341, -0.57756287, 0.03978543...  \n",
       "..                                                ...  \n",
       "95  [0.69135594, 0.035815842, -0.21879485, 0.70745...  \n",
       "96  [0.506407, 0.5669559, 0.09637899, 0.76437205, ...  \n",
       "97  [0.063290104, -0.14269626, 0.4970071, 0.387071...  \n",
       "98  [-0.23187801, -0.25392118, 0.14278063, 0.09517...  \n",
       "99  [0.19839634, 0.22893402, -0.09233187, 0.675283...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>weighted_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.19498698, 0.1408081, 0.061035156, -0.08772...</td>\n",
       "      <td>[-0.7648797, 0.81191665, 0.64800817, -0.506402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10611979, -0.01570638, 0.005818685, 0.07367...</td>\n",
       "      <td>[0.6533486, -0.23902734, 0.17611901, 0.4855068...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grabbing minute post feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.045369465, 0.06301626, -0.105163574, 0.0296...</td>\n",
       "      <td>[0.41217908, 0.46653095, -0.67341155, 0.289130...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.12252372, 0.025983538, 0.008736746, 0.06814...</td>\n",
       "      <td>[0.9110772, 0.13422604, 0.13133731, 0.53929806...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.18334961, 0.21044922, -0.14233398, -0.03942...</td>\n",
       "      <td>[1.0802882, 1.1289341, -0.57756287, 0.03978543...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>feel like throwing away shitty piece shit paper</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.092681885, 0.009773254, -0.048070908, 0.111...</td>\n",
       "      <td>[0.69135594, 0.035815842, -0.21879485, 0.70745...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>starting feel wryly amused banal comedy errors...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.048014324, 0.08087158, 0.0011461047, 0.0941...</td>\n",
       "      <td>[0.506407, 0.5669559, 0.09637899, 0.76437205, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>find every body beautiful want people feel vit...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.025824653, -0.019510904, 0.05770535, 0.0679...</td>\n",
       "      <td>[0.063290104, -0.14269626, 0.4970071, 0.387071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>hear owners feel victimized associations assoc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.024902344, -0.024559868, 0.005533854, 0.00...</td>\n",
       "      <td>[-0.23187801, -0.25392118, 0.14278063, 0.09517...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>say goodbye fam sad crying feel like heartless...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.037121, 0.038829986, -0.007452102, 0.102466...</td>\n",
       "      <td>[0.19839634, 0.22893402, -0.09233187, 0.675283...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:29:48.455899Z",
     "start_time": "2024-08-14T07:29:48.437271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = df_train['embeddings'].to_numpy()\n",
    "X_train = np.vstack(X_train)\n",
    "\n",
    "Y_train = df_train['label']\n",
    "\n",
    "X_val = df_validation['embeddings'].to_numpy()\n",
    "X_val = np.vstack(X_val)\n",
    "Y_val = df_validation['label']\n",
    "\n",
    "X_test = df_test['embeddings'].to_numpy()\n",
    "X_test = np.vstack(X_test)\n",
    "Y_test = df_test['label']"
   ],
   "id": "63122e0143aa5a57",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SVM",
   "id": "8e9fe55e6ef2bfb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:26:45.137538Z",
     "start_time": "2024-08-14T07:15:44.647911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "svm = SVC(verbose=1, probability=True)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation\n",
    "kf = StratifiedKFold(n_splits=2)\n",
    "\n",
    "# GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=kf, n_jobs=8, scoring=\"f1_weighted\", verbose=1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ],
   "id": "d570de2646bb0d8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[LibSVM]Best parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.707199974807395\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# KNN",
   "id": "7d7d1ac8d43d6539"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:29:54.778059Z",
     "start_time": "2024-08-14T07:29:52.234273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 7, 11, 15, 19, 23, 31],  # Number of neighbors to use\n",
    "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation\n",
    "kf = StratifiedKFold(n_splits=2)\n",
    "\n",
    "# GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=kf, n_jobs=8, scoring=\"f1_weighted\", verbose=1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ],
   "id": "94779a44267f81a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 14 candidates, totalling 28 fits\n",
      "Best parameters: {'n_neighbors': 23, 'weights': 'distance'}\n",
      "Best cross-validation score: 0.5738612957041768\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:29:54.975022Z",
     "start_time": "2024-08-14T07:29:54.778059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_probas = best_model.predict_proba(X_test)"
   ],
   "id": "ad2a0f08eb2e55a0",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:29:54.986968Z",
     "start_time": "2024-08-14T07:29:54.976025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(Y_test, test_predictions)\n",
    "print(report)"
   ],
   "id": "d39b423d5e9d9e62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.79      0.67       581\n",
      "           1       0.68      0.80      0.73       695\n",
      "           2       0.60      0.21      0.31       159\n",
      "           3       0.67      0.43      0.53       275\n",
      "           4       0.62      0.42      0.50       224\n",
      "           5       0.60      0.09      0.16        66\n",
      "\n",
      "    accuracy                           0.63      2000\n",
      "   macro avg       0.62      0.46      0.48      2000\n",
      "weighted avg       0.63      0.63      0.61      2000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:30:07.777396Z",
     "start_time": "2024-08-14T07:30:06.964557Z"
    }
   },
   "cell_type": "code",
   "source": "run = wandb.init(project='Emotion', name=\"KNN-classification-embeddings\")",
   "id": "d7f2db0e7aafd51d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\allex\\Desktop\\git_repos\\faculty\\PML\\emotion-classification\\wandb\\run-20240814_103006-w9710y5l</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lol-2/Emotion/runs/w9710y5l' target=\"_blank\">KNN-classification-embeddings</a></strong> to <a href='https://wandb.ai/lol-2/Emotion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/lol-2/Emotion' target=\"_blank\">https://wandb.ai/lol-2/Emotion</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/lol-2/Emotion/runs/w9710y5l' target=\"_blank\">https://wandb.ai/lol-2/Emotion/runs/w9710y5l</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:28:51.176567Z",
     "start_time": "2024-08-14T07:28:51.173679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "labels"
   ],
   "id": "3ead9cc6b238afba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:30:11.549646Z",
     "start_time": "2024-08-14T07:30:08.727986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wandb.sklearn.plot_classifier(best_model,\n",
    "                              X_train, X_test,\n",
    "                              Y_train, Y_test,\n",
    "                              test_predictions, test_probas,\n",
    "                              labels,\n",
    "                              is_binary=False,\n",
    "                              model_name='SVM')"
   ],
   "id": "184c184f12fbb13c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \n",
      "wandb: Plotting SVM.\n",
      "wandb: Logged feature importances.\n",
      "wandb: Logged confusion matrix.\n",
      "wandb: Logged summary metrics.\n",
      "wandb: Logged class proportions.\n",
      "wandb: Logged calibration curve.\n",
      "wandb: Logged roc curve.\n",
      "wandb: Logged precision-recall curve.\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T07:30:25.656785Z",
     "start_time": "2024-08-14T07:30:11.550647Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.finish()",
   "id": "eb4b53c019edcfa7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.077 MB of 0.077 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a5b54e40b8240be8b0e093112759940"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">KNN-classification-embeddings</strong> at: <a href='https://wandb.ai/lol-2/Emotion/runs/w9710y5l' target=\"_blank\">https://wandb.ai/lol-2/Emotion/runs/w9710y5l</a><br/> View project at: <a href='https://wandb.ai/lol-2/Emotion' target=\"_blank\">https://wandb.ai/lol-2/Emotion</a><br/>Synced 5 W&B file(s), 5 media file(s), 5 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240814_103006-w9710y5l\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
