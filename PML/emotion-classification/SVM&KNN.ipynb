{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-13T19:15:29.406032Z",
     "start_time": "2024-08-13T19:15:29.402507Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "wandb.errors.term._show_warnings = False"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:09:31.340785Z",
     "start_time": "2024-08-13T19:09:31.335716Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.login()",
   "id": "fd8919ef9148c5cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset that i am using has two configurations:\n",
    "\n",
    "| Name    | Train | Validation | Test |\n",
    "|---------|-------|------------|------|\n",
    "| Split   | 16000 | 2000       | 2000 |\n",
    "| Unsplit | 416809| n/a        | n/a  |\n",
    "\n",
    "I will be using both configurations and test with a smaller corpus for training and then a bigger one."
   ],
   "id": "987f3628c73a920e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:09:55.408040Z",
     "start_time": "2024-08-13T19:09:50.652715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitted_ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
    "# unsplitted_ds = load_dataset(\"dair-ai/emotion\", \"unsplit\")\n",
    "\n",
    "# df_unsplit_train = unsplitted_ds['train'].to_pandas()\n",
    "df_train = splitted_ds['train'].to_pandas()\n",
    "df_test = splitted_ds['test'].to_pandas()\n",
    "df_validation = splitted_ds['validation'].to_pandas()"
   ],
   "id": "856b404006b1ede6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Fields\n",
    "The data fields are:\n",
    "\n",
    "**text**: a string feature.|\n",
    "\n",
    "**label**: a classification label, with possible values including: \n",
    "\n",
    "0 -> sadness\n",
    "\n",
    "1 -> joy\n",
    "\n",
    "2 -> love\n",
    "\n",
    "3 -> anger\n",
    "\n",
    "4 -> fear\n",
    "\n",
    "5 -> surprise"
   ],
   "id": "5857a135962f40b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:09:58.279995Z",
     "start_time": "2024-08-13T19:09:58.274452Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.head()",
   "id": "733ad5fcf12db0f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The number of data for each label. It can be seen that the data is a little unbalanced in the splitted training dataset. Same story applies to unsplitted dataset",
   "id": "7614a91df1874624"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:10:00.033334Z",
     "start_time": "2024-08-13T19:10:00.029791Z"
    }
   },
   "cell_type": "code",
   "source": "df_train['label'].value_counts()",
   "id": "b108d812f1182545",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    5362\n",
       "0    4666\n",
       "3    2159\n",
       "4    1937\n",
       "2    1304\n",
       "5     572\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:11:38.060645Z",
     "start_time": "2024-08-13T15:11:38.055588Z"
    }
   },
   "cell_type": "code",
   "source": "df_unsplit_train['label'].value_counts()",
   "id": "d1572e572ab8bd78",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    141067\n",
       "0    121187\n",
       "3     57317\n",
       "4     47712\n",
       "2     34554\n",
       "5     14972\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SnowballStemmer:\n",
    "\n",
    "- After processing the word through all these rules, the stemmer produces a stem—a simplified version of the word that represents its core meaning. This stem is not always a valid word in the language but is a useful representation for analysis purposes.\n",
    "- For example, “running” becomes “run,” “studies” becomes “studi,” and “better” becomes “better” (sometimes the word is already in its simplest form)."
   ],
   "id": "a4113a174b390936"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:10:05.445457Z",
     "start_time": "2024-08-13T19:10:05.288454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "list(api.info()['models'].keys())"
   ],
   "id": "45e1f5f090e7a994",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasttext-wiki-news-subwords-300',\n",
       " 'conceptnet-numberbatch-17-06-300',\n",
       " 'word2vec-ruscorpora-300',\n",
       " 'word2vec-google-news-300',\n",
       " 'glove-wiki-gigaword-50',\n",
       " 'glove-wiki-gigaword-100',\n",
       " 'glove-wiki-gigaword-200',\n",
       " 'glove-wiki-gigaword-300',\n",
       " 'glove-twitter-25',\n",
       " 'glove-twitter-50',\n",
       " 'glove-twitter-100',\n",
       " 'glove-twitter-200',\n",
       " '__testing_word2vec-matrix-synopsis']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:10:28.952542Z",
     "start_time": "2024-08-13T19:10:07.943642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ss = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm')  # english tokenizer trf -> accuracy | sm -> efficiency\n",
    "word2vec = api.load(\"word2vec-google-news-300\")  # Load the pretrained Word2Vec model\n",
    "print(\"models imported!\")"
   ],
   "id": "206d9268bc57e5d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models imported!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:24:15.084829Z",
     "start_time": "2024-08-13T19:23:29.139798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_preparetion(sentence, nlp):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z@#.\n",
    "    sentence = re.sub(r\"[^a-zăâîșț@# ]\", \"\", sentence)\n",
    "\n",
    "    # # Tokenize the preprocessed sentence\n",
    "    tokenization = nlp(sentence)\n",
    "\n",
    "    # 4. Remove stopwords and empty tokens and split sentence into words\n",
    "    list_text_preprocessed = [\n",
    "        word.text for word in tokenization if word.text not in sw and word.pos_ != \"SPACE\"\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(list_text_preprocessed)\n",
    "\n",
    "\n",
    "def text_vectorization_word2vec(sentence, model):\n",
    "    words = sentence.split()\n",
    "    words_embeddings = [model[word] for word in words if word in model]\n",
    "    \n",
    "    # if there are no words in the word2vec\n",
    "    if not words_embeddings:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the word vectors to get a single sentece represenation\n",
    "    return np.mean(words_embeddings, axis=0)\n",
    "\n",
    "def text_vectorization_word2vec_weighted(sentence, model, train_tfidf_dict):\n",
    "    words = sentence.split()\n",
    "    words_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        weight = train_tfidf_dict.get(word, 1.0)\n",
    "        if word in model:\n",
    "            words_embeddings.append(weight * model[word])\n",
    "    \n",
    "    # if there are no words in the word2vec\n",
    "    if not words_embeddings:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Average the word vectors to get a single sentece represenation\n",
    "    return np.mean(words_embeddings, axis=0)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Preprocessing\n",
    "df_train['text'] = df_train['text'].progress_apply(lambda x: text_preparetion(x, nlp))\n",
    "df_test['text'] = df_test['text'].progress_apply(lambda x: text_preparetion(x, nlp))\n",
    "df_validation['text'] = df_validation['text'].progress_apply(lambda x: text_preparetion(x, nlp))\n",
    "print(\"PREPROCESSING!\")\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = CountVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(df_train['text'])\n",
    "train_tfidf_dict = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "X_val_tfidf = vectorizer.transform(df_validation['text'])\n",
    "X_test_tfidf = vectorizer.transform(df_test['text'])\n",
    "print(\"TF-IDF!\")\n",
    "\n",
    "# word2vec\n",
    "df_train['embeddings'] = df_train['text'].progress_apply(lambda x: text_vectorization_word2vec(x, word2vec))\n",
    "df_test['embeddings'] = df_test['text'].progress_apply(lambda x: text_vectorization_word2vec(x, word2vec))\n",
    "df_validation['embeddings'] = df_validation['text'].progress_apply(lambda x: text_vectorization_word2vec(x, word2vec))\n",
    "print(\"WORD2VEC!\")\n",
    "\n",
    "# weighted word2vec\n",
    "df_train['weighted_embeddings'] = df_train['text'].progress_apply(lambda x: text_vectorization_word2vec_weighted(x, word2vec, train_tfidf_dict))\n",
    "df_test['weighted_embeddings'] = df_test['text'].progress_apply(lambda x: text_vectorization_word2vec_weighted(x, word2vec, train_tfidf_dict))\n",
    "df_validation['weighted_embeddings'] = df_validation['text'].progress_apply(lambda x: text_vectorization_word2vec_weighted(x, word2vec, train_tfidf_dict))\n",
    "print(\"WEIGHTED WORD2VEC!\")\n",
    "# df_unsplit_train['embeddings'] = df_unsplit_train['text'].progress_apply(lambda x: text_preparetion(x, word2vec))"
   ],
   "id": "bb361d4e99374c0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [00:36<00:00, 444.22it/s]\n",
      "100%|██████████| 2000/2000 [00:04<00:00, 445.64it/s]\n",
      "100%|██████████| 2000/2000 [00:04<00:00, 448.50it/s]\n",
      "100%|██████████| 16000/16000 [00:00<00:00, 61255.47it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 60969.92it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 61689.11it/s]\n",
      "100%|██████████| 16000/16000 [00:00<00:00, 39502.55it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 36999.86it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 36344.37it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:20:24.614660Z",
     "start_time": "2024-08-13T19:19:56.200527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save preprocessed dataset\n",
    "df_train.to_csv(\"./data/split_train.csv\", index=False)\n",
    "df_test.to_csv(\"./data/test.csv\", index=False)\n",
    "df_validation.to_csv(\"./data/validation.csv\", index=False)\n",
    "# df_unsplit_train.to_csv(\"./data/unsplit_train.csv\", index=False)"
   ],
   "id": "77e032e389305706",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:20:25.519543Z",
     "start_time": "2024-08-13T19:20:24.615663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load preprocessed datasets\n",
    "df_train = pd.read_csv(\"./data/split_train.csv\")\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "df_validation = pd.read_csv(\"./data/validation.csv\")\n",
    "# df_unsplit_train = pd.read_csv(\"./data/unsplit_train.csv\")"
   ],
   "id": "ba467d40d9dff2c4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:24:23.921564Z",
     "start_time": "2024-08-13T19:24:23.908953Z"
    }
   },
   "cell_type": "code",
   "source": "df_train[:100]",
   "id": "beefb04dc41ec056",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 text  label  \\\n",
       "0                                  nt feel humiliated      0   \n",
       "1   go feeling hopeless damned hopeful around some...      0   \n",
       "2              grabbing minute post feel greedy wrong      3   \n",
       "3   ever feeling nostalgic fireplace know still pr...      2   \n",
       "4                                     feeling grouchy      3   \n",
       "..                                                ...    ...   \n",
       "95    feel like throwing away shitty piece shit paper      0   \n",
       "96  starting feel wryly amused banal comedy errors...      1   \n",
       "97  find every body beautiful want people feel vit...      1   \n",
       "98  hear owners feel victimized associations assoc...      0   \n",
       "99  say goodbye fam sad crying feel like heartless...      3   \n",
       "\n",
       "                                           embeddings  \\\n",
       "0   [-0.19498698, 0.1408081, 0.061035156, -0.08772...   \n",
       "1   [0.10611979, -0.01570638, 0.005818685, 0.07367...   \n",
       "2   [0.045369465, 0.06301626, -0.105163574, 0.0296...   \n",
       "3   [0.12252372, 0.025983538, 0.008736746, 0.06814...   \n",
       "4   [0.18334961, 0.21044922, -0.14233398, -0.03942...   \n",
       "..                                                ...   \n",
       "95  [0.092681885, 0.009773254, -0.048070908, 0.111...   \n",
       "96  [0.048014324, 0.08087158, 0.0011461047, 0.0941...   \n",
       "97  [0.025824653, -0.019510904, 0.05770535, 0.0679...   \n",
       "98  [-0.024902344, -0.024559868, 0.005533854, 0.00...   \n",
       "99  [0.037121, 0.038829986, -0.007452102, 0.102466...   \n",
       "\n",
       "                                  weighted_embeddings  \n",
       "0   [-0.7648797, 0.81191665, 0.64800817, -0.506402...  \n",
       "1   [0.6533486, -0.23902734, 0.17611901, 0.4855068...  \n",
       "2   [0.41217908, 0.46653095, -0.67341155, 0.289130...  \n",
       "3   [0.9110772, 0.13422604, 0.13133731, 0.53929806...  \n",
       "4   [1.0802882, 1.1289341, -0.57756287, 0.03978543...  \n",
       "..                                                ...  \n",
       "95  [0.69135594, 0.035815842, -0.21879485, 0.70745...  \n",
       "96  [0.506407, 0.5669559, 0.09637899, 0.76437205, ...  \n",
       "97  [0.063290104, -0.14269626, 0.4970071, 0.387071...  \n",
       "98  [-0.23187801, -0.25392118, 0.14278063, 0.09517...  \n",
       "99  [0.19839634, 0.22893402, -0.09233187, 0.675283...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>weighted_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.19498698, 0.1408081, 0.061035156, -0.08772...</td>\n",
       "      <td>[-0.7648797, 0.81191665, 0.64800817, -0.506402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feeling hopeless damned hopeful around some...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10611979, -0.01570638, 0.005818685, 0.07367...</td>\n",
       "      <td>[0.6533486, -0.23902734, 0.17611901, 0.4855068...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grabbing minute post feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.045369465, 0.06301626, -0.105163574, 0.0296...</td>\n",
       "      <td>[0.41217908, 0.46653095, -0.67341155, 0.289130...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.12252372, 0.025983538, 0.008736746, 0.06814...</td>\n",
       "      <td>[0.9110772, 0.13422604, 0.13133731, 0.53929806...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.18334961, 0.21044922, -0.14233398, -0.03942...</td>\n",
       "      <td>[1.0802882, 1.1289341, -0.57756287, 0.03978543...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>feel like throwing away shitty piece shit paper</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.092681885, 0.009773254, -0.048070908, 0.111...</td>\n",
       "      <td>[0.69135594, 0.035815842, -0.21879485, 0.70745...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>starting feel wryly amused banal comedy errors...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.048014324, 0.08087158, 0.0011461047, 0.0941...</td>\n",
       "      <td>[0.506407, 0.5669559, 0.09637899, 0.76437205, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>find every body beautiful want people feel vit...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.025824653, -0.019510904, 0.05770535, 0.0679...</td>\n",
       "      <td>[0.063290104, -0.14269626, 0.4970071, 0.387071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>hear owners feel victimized associations assoc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.024902344, -0.024559868, 0.005533854, 0.00...</td>\n",
       "      <td>[-0.23187801, -0.25392118, 0.14278063, 0.09517...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>say goodbye fam sad crying feel like heartless...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.037121, 0.038829986, -0.007452102, 0.102466...</td>\n",
       "      <td>[0.19839634, 0.22893402, -0.09233187, 0.675283...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:31:08.429647Z",
     "start_time": "2024-08-13T19:31:08.409454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = df_train['embeddings'].to_numpy()\n",
    "X_train = np.vstack(X_train)\n",
    "\n",
    "Y_train = df_train['label']\n",
    "\n",
    "X_val = df_validation['embeddings'].to_numpy()\n",
    "X_val = np.vstack(X_val)\n",
    "Y_val = df_validation['label']\n",
    "\n",
    "X_test = df_test['embeddings'].to_numpy()\n",
    "X_test = np.vstack(X_test)\n",
    "Y_test = df_test['label']"
   ],
   "id": "63122e0143aa5a57",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T19:33:28.795846Z",
     "start_time": "2024-08-13T19:32:32.480280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "svm = SVC(verbose=1, probability=True)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [2, 3, 4],\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation\n",
    "kf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=kf, n_jobs=8, scoring=\"f1_weighted\", verbose=1)\n",
    "grid_search.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ],
   "id": "d570de2646bb0d8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 36 candidates, totalling 144 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# GridSearchCV to find the best parameters\u001B[39;00m\n\u001B[0;32m     17\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(svm, param_grid, cv\u001B[38;5;241m=\u001B[39mkf, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf1_weighted\u001B[39m\u001B[38;5;124m\"\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m grid_search\u001B[38;5;241m.\u001B[39mfit(X_train_tfidf, Y_train)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Best parameters and best score\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest parameters:\u001B[39m\u001B[38;5;124m\"\u001B[39m, grid_search\u001B[38;5;241m.\u001B[39mbest_params_)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PML\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1013\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1014\u001B[0m     )\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1018\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_search(evaluate_candidates)\n\u001B[0;32m   1020\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1022\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1570\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1571\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1572\u001B[0m     evaluate_candidates(ParameterGrid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_grid))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    957\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    958\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    959\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    960\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    961\u001B[0m         )\n\u001B[0;32m    962\u001B[0m     )\n\u001B[1;32m--> 964\u001B[0m out \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    965\u001B[0m     delayed(_fit_and_score)(\n\u001B[0;32m    966\u001B[0m         clone(base_estimator),\n\u001B[0;32m    967\u001B[0m         X,\n\u001B[0;32m    968\u001B[0m         y,\n\u001B[0;32m    969\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[0;32m    970\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[0;32m    971\u001B[0m         parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[0;32m    972\u001B[0m         split_progress\u001B[38;5;241m=\u001B[39m(split_idx, n_splits),\n\u001B[0;32m    973\u001B[0m         candidate_progress\u001B[38;5;241m=\u001B[39m(cand_idx, n_candidates),\n\u001B[0;32m    974\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_and_score_kwargs,\n\u001B[0;32m    975\u001B[0m     )\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m product(\n\u001B[0;32m    977\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(candidate_params),\n\u001B[0;32m    978\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39msplitter\u001B[38;5;241m.\u001B[39msplit)),\n\u001B[0;32m    979\u001B[0m     )\n\u001B[0;32m    980\u001B[0m )\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    983\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    984\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    985\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    987\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PML\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     73\u001B[0m )\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PML\\Lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PML\\Lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PML\\Lib\\site-packages\\joblib\\parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1762\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "val_predictions = best_model.predict(X_val)\n",
    "val_probas = best_model.predict_proba(X_val)"
   ],
   "id": "ad2a0f08eb2e55a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T18:57:10.833052Z",
     "start_time": "2024-08-13T18:57:10.825520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(Y_val, val_predictions)\n",
    "print(report)"
   ],
   "id": "d39b423d5e9d9e62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.80      0.64       550\n",
      "           1       0.71      0.66      0.68       704\n",
      "           2       0.68      0.37      0.47       178\n",
      "           3       0.58      0.53      0.55       275\n",
      "           4       0.68      0.42      0.52       212\n",
      "           5       0.72      0.36      0.48        81\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.65      0.52      0.56      2000\n",
      "weighted avg       0.64      0.62      0.61      2000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:41:24.290408Z",
     "start_time": "2024-08-12T18:41:23.280975Z"
    }
   },
   "cell_type": "code",
   "source": "run = wandb.init(project='Emotion', name=\"svm-classification2\")",
   "id": "d7f2db0e7aafd51d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\allex\\Desktop\\git_repos\\faculty\\PML\\emotion-classification\\wandb\\run-20240812_214123-h7a0mgtc</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lol-2/Emotion/runs/h7a0mgtc' target=\"_blank\">svm-classification2</a></strong> to <a href='https://wandb.ai/lol-2/Emotion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/lol-2/Emotion' target=\"_blank\">https://wandb.ai/lol-2/Emotion</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/lol-2/Emotion/runs/h7a0mgtc' target=\"_blank\">https://wandb.ai/lol-2/Emotion/runs/h7a0mgtc</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:41:04.235798Z",
     "start_time": "2024-08-12T18:41:04.232153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = df_train.label.unique()\n",
    "labels.sort()\n",
    "labels"
   ],
   "id": "3ead9cc6b238afba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:34:22.550730Z",
     "start_time": "2024-08-12T18:33:24.815320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wandb.sklearn.plot_classifier(best_model,\n",
    "                              X_train_tfidf, X_val_tfidf,\n",
    "                              Y_train, Y_val,\n",
    "                              val_predictions, val_probas,\n",
    "                              labels,\n",
    "                              is_binary=False,\n",
    "                              model_name='SVM')\n",
    "\n",
    "wandb.finish()"
   ],
   "id": "184c184f12fbb13c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: \n",
      "wandb: Plotting SVM.\n",
      "wandb: Logged feature importances.\n",
      "wandb: Logged confusion matrix.\n",
      "wandb: Logged summary metrics.\n",
      "wandb: Logged class proportions.\n",
      "wandb: Logged calibration curve.\n",
      "wandb: Logged roc curve.\n",
      "wandb: Logged precision-recall curve.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.034 MB of 0.034 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "159078c44e4c49d4b6f0dea07add0828"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">svm-classification</strong> at: <a href='https://wandb.ai/lol-2/Emotion/runs/db6tqi98' target=\"_blank\">https://wandb.ai/lol-2/Emotion/runs/db6tqi98</a><br/> View project at: <a href='https://wandb.ai/lol-2/Emotion' target=\"_blank\">https://wandb.ai/lol-2/Emotion</a><br/>Synced 5 W&B file(s), 5 media file(s), 7 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240812_211523-db6tqi98\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:36:56.033074Z",
     "start_time": "2024-08-12T18:36:56.029535Z"
    }
   },
   "cell_type": "code",
   "source": "Y_val.value_counts()",
   "id": "cc1e96f37386298",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    704\n",
       "0    550\n",
       "3    275\n",
       "4    212\n",
       "2    178\n",
       "5     81\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:37:19.660850Z",
     "start_time": "2024-08-12T18:37:19.657307Z"
    }
   },
   "cell_type": "code",
   "source": "val_probas",
   "id": "870457239c3ebdc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.97940159e-01, 8.94932782e-06, 1.14015911e-05, 9.71676376e-04,\n",
       "        8.01150403e-04, 2.66663028e-04],\n",
       "       [9.99007582e-01, 1.04310969e-05, 2.08927982e-05, 3.47123559e-04,\n",
       "        2.90308696e-04, 3.23661824e-04],\n",
       "       [4.49186376e-05, 1.07201915e-01, 8.92726273e-01, 1.72892243e-05,\n",
       "        3.60782549e-06, 5.99659767e-06],\n",
       "       ...,\n",
       "       [1.80117452e-04, 9.91505307e-01, 6.46288024e-03, 6.11106504e-04,\n",
       "        5.20248034e-04, 7.20340519e-04],\n",
       "       [3.02788136e-04, 6.66461139e-01, 3.32174979e-01, 2.19534473e-04,\n",
       "        5.96615042e-04, 2.44944195e-04],\n",
       "       [2.05086383e-03, 9.83388117e-01, 2.39579210e-03, 2.11680342e-03,\n",
       "        8.02862173e-03, 2.01980226e-03]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T18:41:27.356829Z",
     "start_time": "2024-08-12T18:41:27.112294Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.sklearn.plot_roc(Y_val, val_probas, labels)",
   "id": "c8b725feb53ac719",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T19:03:16.094448Z",
     "start_time": "2024-08-12T18:44:13.376376Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.sklearn.plot_learning_curve(best_model, X_train_tfidf, Y_train)",
   "id": "c80cabb7fc5fbbba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "49d5ead33de2eb56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
