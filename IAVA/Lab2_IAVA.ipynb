{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "f32851025d2e42b89facf894eb731c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a1b7575682149bbb1500dbf279a3120",
       "IPY_MODEL_639fd14e413843feb28e1ddffc945101",
       "IPY_MODEL_8e8dd6762f7d444a973609bfdd2aa7f8"
      ],
      "layout": "IPY_MODEL_10981ae0919f4b398ac3982edf59f1d1"
     }
    },
    "6a1b7575682149bbb1500dbf279a3120": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5c41780b8774ca5832aeb414d05a9ae",
      "placeholder": "​",
      "style": "IPY_MODEL_688c4da68ad74a87b957e1ee8676c673",
      "value": "100%"
     }
    },
    "639fd14e413843feb28e1ddffc945101": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd700a5851f849ae8810a04b32fda061",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_43142e789d64446dbadeed06cc54fb94",
      "value": 170498071
     }
    },
    "8e8dd6762f7d444a973609bfdd2aa7f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87d9291e4b214379aeed908dec5b1c07",
      "placeholder": "​",
      "style": "IPY_MODEL_11f1bf86e5114dcda6bd6081b53fdc36",
      "value": " 170498071/170498071 [00:01&lt;00:00, 97188482.16it/s]"
     }
    },
    "10981ae0919f4b398ac3982edf59f1d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5c41780b8774ca5832aeb414d05a9ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "688c4da68ad74a87b957e1ee8676c673": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd700a5851f849ae8810a04b32fda061": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43142e789d64446dbadeed06cc54fb94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "87d9291e4b214379aeed908dec5b1c07": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11f1bf86e5114dcda6bd6081b53fdc36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPquMWOmYa_D"
   },
   "source": [
    "# Antrenarea unei retele de clasificare\n",
    "\n",
    "Obiectivul acestui laborator este de a introduce conceptele de baza necesare antrenarii unei retele neuronale. Pytorch ofera posibilitatea de a incarca si procesa setul de date rapid si eficient. In acest laborator vom folosi setul de date CIFAR-10, pentru care vom rezolva problema de clasificare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzgFBIRQ1YOc"
   },
   "source": [
    "Importarea bibliotecilor care vor fi folosite in acest laborator:\n",
    "\n",
    " * *matplotlib.pyplot* pentru grafice\n",
    " * *torch.optim* pentru optimizatori\n",
    " * *torch.nn* pentru lucrul cu retele neurale\n",
    " * *torch.utils.data* pentru lucrul cu seturi de date\n",
    " * *torchvision* pentru seturi de date oferite de repository-ul Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L8uJfA201Bif",
    "ExecuteTime": {
     "end_time": "2023-10-16T12:50:49.133897700Z",
     "start_time": "2023-10-16T12:50:48.409931500Z"
    }
   },
   "source": [
    "from IPython import display as dspl\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7hHS0O7nI4y"
   },
   "source": [
    "## Dataset (1p)\n",
    "Clasa *torchvision.datasets.CIFAR10* este o subclasa a clasei abstracte *torch.utils.data.Dataset*. O astfel de clasa este folosita pentru a ingloba datasetul si pentru a returna elemente din dataset.\n",
    "\n",
    "O clasa derivata din *torch.utils.data.Dataset*, trebuie sa suprascrie 2 metode:\n",
    " * \\_\\_len\\_\\_(self) -> aceasta metoda returneaza numarul de elemente din dataset si permite folosirea functiei __len()__ din Python.\n",
    " * \\_\\_getitem\\_\\_ -> permite folosirea operatorului de indexare din Python __[ ]__ pentru a obtine un element de la un anumit index din dataset \n",
    "\n",
    "\n",
    "Exemplu clasa derivata din torch.utils.data.Dataset: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "\n",
    "### Cerinte\n",
    "  1. Printati numarul de exemple din datasetul de antrenare si de test (0.25p)\n",
    "  2. Printati exemplul de la indexul 0 (0.25p)\n",
    "  3. Printati valoarea maxima si valoarea minima din prima imagine din datasetul de test (0.5p)\n",
    "\n",
    "#### Hints\n",
    " * Un exemplu din dataset este reprezentat de un tuplu care contine o imagine de tip de date PIL.Image si un int reprezentand clasa imaginii\n",
    " * np.min(a) -> returneaza minimul dintr-un obiect de tipul np.ndarray\n",
    " * np.max(a) -> returneaza maximul dintr-un obiect de tipul np.ndarray\n",
    " * np.asarray(a) -> returneaza un obiect de tipul np.ndarray. Functia trebuie sa primeaca un obiect 'array-like'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Crearea instantelor pentru setul de date CIFAR de train si de test\n",
    "cifar_train = torchvision.datasets.CIFAR10(\"./data\", download=True)\n",
    "cifar_test = torchvision.datasets.CIFAR10(\"./data\", train=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:50:51.081792900Z",
     "start_time": "2023-10-16T12:50:50.098103400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PvoP1C_3-3dA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "f32851025d2e42b89facf894eb731c10",
      "6a1b7575682149bbb1500dbf279a3120",
      "639fd14e413843feb28e1ddffc945101",
      "8e8dd6762f7d444a973609bfdd2aa7f8",
      "10981ae0919f4b398ac3982edf59f1d1",
      "a5c41780b8774ca5832aeb414d05a9ae",
      "688c4da68ad74a87b957e1ee8676c673",
      "cd700a5851f849ae8810a04b32fda061",
      "43142e789d64446dbadeed06cc54fb94",
      "87d9291e4b214379aeed908dec5b1c07",
      "11f1bf86e5114dcda6bd6081b53fdc36"
     ]
    },
    "outputId": "9f320542-6713-43d8-903d-35231494d3a8",
    "ExecuteTime": {
     "end_time": "2023-10-16T12:09:04.370352200Z",
     "start_time": "2023-10-16T12:09:04.355844600Z"
    }
   },
   "source": [
    "#TODO: Scrieti aici codul pentru cerinta numarul 1\n",
    "print(len(cifar_train))\n",
    "\n",
    "#TODO: Scrieti aici codul pentru cerinta numarul 2\n",
    "display(cifar_train[0][0])\n",
    "\n",
    "#TODO: Completati sub codul pentru cerinta numarul 3\n",
    "print(np.min(cifar_train[0][0]))\n",
    "print(np.max(cifar_train[0][0]))\n"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=32x32>",
      "image/png": "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\n",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDiNHsJ7GBpGkcyuTncxP5VcuL77Kd1xNIN6jgueTV/Vp47VA4jQsW2qBwM1a0rwjH4guoJrlzFJOhaGF+gUYCn8TmvIcm3eXU9VJJWXQyY7/ZObedZ0lI3bWyO3Xn2p9/qV5ax7IxNK2Mja/b8a7W/+HEsiyJMj3FyUxEWfbg/1rlZrWSyla1uBtlhOxgexHFJNbjfLqkyW0htbnUYv7REhtEl3vsUFuvQf56V219qzW8l3qOn2sSoFiSB2G7y1XgjH+eTWBcaPIbp5rAs0YYkoAfWqhvNUhWVN8yhiPl2kcjisadZTVl/wSnFXNu08Wyx3y3eqPJPEnzKQwXa/t6DtWDdSz6vrTyQsht7mbKK/cE81SnRREDPFvGfmQgkZ9cetPi1ForXVWsrKWa4ghRYyVIRQxwce4FOalNWMpRtsf/Z\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "255\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO0Ju49dp_xp"
   },
   "source": [
    "## Iterare prin Dataset (1p)\n",
    "\n",
    "Deoarece clasa datasetului implementeaza functia \\_\\_getitem\\_\\_(), se poate itera prin dataset cum se poate itera si printr-o lista sau alt obiect iterabil. \n",
    "\n",
    "### Cerinte\n",
    " * Odata la n pasi, printati clasa exemplului curent si afisati imaginea.\n",
    "\n",
    "#### Hints\n",
    "  * A fost importata libraria matplotlib.pyplot as plt\n",
    "  * Functia plt.figure(figsize=(float, float)) returneaza o figura de dimensiunea oferita ca parametru in *figsize*\n",
    "  * Functia plt.imshow(np.ndarray) plaseaza o imagine pe o figura\n",
    "  * Functia plt.show() afiseaza figura"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9v_DcyDBHVQu"
   },
   "source": [
    "n = 10000\n",
    "\n",
    "for idx, example in enumerate(cifar_train):\n",
    "  if idx % n == 0:\n",
    "    # Aceasta functie sterge ce a fost afisat pana la momentul curent\n",
    "    dspl.clear_output(wait=True)\n",
    "\n",
    "    #TODO: Completati codul aici si afisati cateva imagini din dataset\n",
    "    print(example[1])\n",
    "    display(example[0])\n",
    "    # Aceasta functie opreste procesul pentru 2 secunde \n",
    "    time.sleep(2)"
   ],
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=32x32>",
      "image/png": "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\n",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD0rxh/bR1KD7FJdR2wQZaDd6ndnA6/d6+9cpLq1zDDNt1q5DAqjRS3BVySDjGeQAQMn3rtvFur/wBnNZlrG5uYWlMbNGPkjJ43P7c/nXkni3SNfvb4S22i3+9yzAqnBG445HHTFaQn0OerzR1R1EU2oXxVRNql5BkeYFlYr0OenXnA+la3hq01H+27GS6+3u8eQxmD7EXaeBn8B+ArnPCk+s+GYFm1HRtSWJm+VkhMjDPrivX9PvodSso7qHfscdHQqw+oPIpyn0QUry1ejPGoPGGpalF59yViuRI6TKhOBtY8D2xVf4eeONXvvEd5o2qXTLBPF9os1Cj92oJ+UfUY/KtvT/gy80V/caprNzb39xdPKhsZD5aIegKsOTXADwtrt98RrPStMintnsoPs8t5LA3lqBuy2ehBzxg96jSxXLLqe067rep6fZQm1MB3RjdLLkZJHYVy2l/ETV1vJYdUitPKX5opICwJx1BzxSN8NfF8kXlN4isAvH3YH5x35ri/EfhDxnYeI9O0Un+04Loqy3CQERxHOGyexA9aSshOLeqP/9k=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iud_GoZJwK9U"
   },
   "source": [
    "## DataLoader (1p)\n",
    "\n",
    "In torch.utils.data este definita clasa *DataLoader*. Aceasta este un wrapper peste o clasa de tip Dataset si este folosita pentru a abstractiza procesarea pe mai mult threaduri, concatenarea exemplelor in batch-uri, si extragerea exemplelor in mod aleatoriu din dataset.\n",
    "\n",
    "Pentru a crea un obiect de tip *DataLoader* se foloseste constructorul care are urmatorul header:\n",
    "\n",
    "  \\_\\_init\\_\\_(dataset_object, batch_size=1, shuffle=False, num_workers=0, collate_fn=None)\n",
    "\n",
    " * dataset_object - obiectul de tip Dataset care va fi inglobat\n",
    " * batch_size - dimensiunea batch-ului care va fi returnat\n",
    " * shuffle - determina daca exemplele vor fi extrase aleatoriu sau nu\n",
    " * num_workers - numarul de procese paralele care vor incarca datele\n",
    " * collate_fn - o functie care face preprocesari pe N elemente returnate de obiectul de tip Dataset si le concateneaza intr-un batch. N = batch_size\n",
    "\n",
    "Clasa DataLoader implementeaza si functia *\\_\\_len\\_\\_()* pentru a returna numarul de batch-uri din dataset.\n",
    "\n",
    "Obiectul de tip Dataset creat anterior returneaza un tuple-uri care contin o imagine de tip PIL.Image si clasa imaginii de tip int. O retea neurala din Pytorch opereaza pe tipul de date torch.Tensor. Prin urmare obiectul de tip DataLoader trebuie sa returneze obiecte de tip torch.Tensor \n",
    "\n",
    "### Cerinte\n",
    "  1. Iterati prin cele doua obiecte de tip DataLoader si printati doar primul element. (0.5p)\n",
    "  2. Printati shape-ul celor 2 tensori  doar pentru primul element (0.5p)\n",
    "\n",
    "#### Hints\n",
    "  * functia __to_tensor__ din *torchvision.transforms.functional* creaza un obiect de tip torch.Tensor dintr-un obiect de tip PIL.Image\n",
    "  * functia __torch.tensor__ creaza un obiect de tip torch.Tensor dintr-un obiect de tip np.ndarray\n",
    "  * functia __unsqueeze()__ din clasa torch.Tensor creaza o noua dimensiune intr-un tensor. Aceasta este echivalentul functiei __expand_dims()__ din numpy. Exemplu: Daca avem un obiect de tip torch.Tensor, *t*, care contine un vector cu 10 elemente (shape [10]), *t.unsqueeze(0)* va returna un boiect cu aceleasi valori dar cu shape-ul [1, 10]\n",
    "  * functia __torch.cat(tensors, dim=0)__ primeste o lista de tensori si ii concateneaza de-a lungul dimensiunii *dim*. Exemplu: functia primeste o lista cu doi vectori cu shape-ul [1, 10] si *dim=0*, rezultatul are shape-ul [2, 10] (batch 2 ?). Daca *dim=1*, rezultatul va fi [1, 20]. Echivalentul numpy este functia __concatenate()__\n",
    "  * functia __size()__ din clasa torch.Tensor returneaza shape-ul tensorului"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8C_MvX_MIYMv",
    "ExecuteTime": {
     "end_time": "2023-10-16T12:09:18.918449500Z",
     "start_time": "2023-10-16T12:09:15.046643900Z"
    }
   },
   "source": [
    "\n",
    "def preproc_fn(examples):\n",
    "  \"\"\"\n",
    "    Functia primeste un batch de exemple pe care trebuie sa le transforme in tensori\n",
    "      si sa le puna intr-un batch de tip torch.Tensor.\n",
    "  \"\"\"\n",
    "  processed_images = []\n",
    "  processed_labels = []\n",
    "\n",
    "  # print(processed_images)\n",
    "\n",
    "  for example in examples: # example este un tuplu returnat de obiectul de tip Dataset\n",
    "    pil_image = example[0]\n",
    "    #pil_image_array = np.asarray(pil_image)\n",
    "\n",
    "    tensor_image = to_tensor(pil_image)  # Transforma in obiect de tip torch.Tensor imaginea din example -> 32 x 32 x 3\n",
    "    tensor_image = tensor_image.unsqueeze(0) # Adauga inca o dimensiune la inceputul imaginii -> 1 x 32 x 32 x 3\n",
    "    processed_images.append(tensor_image)\n",
    "\n",
    "    label = np.array([example[1]])# Creaza un obiect de tip np.ndarray din labelul exemplului\n",
    "    tensor_label = torch.Tensor(label)# Creaza un obiect de tip torch.Tensor din label\n",
    "    tensor_label = tensor_label.unsqueeze(0) # Adauga inca o dimensiune la incepului labelului\n",
    "    processed_labels.append(tensor_label)\n",
    "\n",
    "  torch_images = torch.cat(processed_images,  dim=0)\n",
    "  torch_labels = torch.cat(processed_labels, dim=0)\n",
    "\n",
    "  return torch_images, torch_labels\n",
    "\n",
    "loader1 = data.DataLoader(cifar_train, batch_size=1, shuffle=True, num_workers=0, collate_fn=preproc_fn)\n",
    "\n",
    "print(\"Datasetul contine {} de batch-uri\".format(len(loader1)))\n",
    "\n",
    "loader2 = data.DataLoader(cifar_train, batch_size=1, shuffle=True, num_workers=4, collate_fn=preproc_fn, persistent_workers=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for index, batch in enumerate(loader1):\n",
    "  if index == 0:\n",
    "    #TODO: Cerintele 1 si 2 - Iterati prin loader1 si printati doar primul element si shape-ul celor 2 tensori din exemplu\n",
    "    print(f\"primul element este : {batch}\")\n",
    "    print(f\"iar shape-ul celor 2 tensori sunt : {batch[0].shape} \\n & \\n {batch[1].shape}\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Iterarea prin dataset cu worker-ul pe procesul curent dureaza {} secunde\".format(end - start))\n",
    "\n",
    "# start = time.time()\n",
    "# \n",
    "# for index, batch in enumerate(loader2):\n",
    "#   #TODO: Cerintele 1 si 2 - Iterati prin loader2 si printati doar primul element si shape-ul celor 2 tensori din exemplu\n",
    "#   if index == 0:\n",
    "#     print(f\"primul element este : {batch}\")\n",
    "#     print(f\"iar shape-ul celor 2 tensori sunt : {batch[0].shape} \\n & \\n {batch[1].shape}\")\n",
    "# \n",
    "# end = time.time()\n",
    "# print(\"Iterarea prin dataset cu 2 worker-i pe procese diferinte dureaza {} secunde\".format(end - start))"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasetul contine 50000 de batch-uri\n",
      "primul element este : (tensor([[[[0.3373, 0.3176, 0.2784,  ..., 0.2353, 0.2118, 0.2510],\n",
      "          [0.3412, 0.3059, 0.3059,  ..., 0.2627, 0.2471, 0.2627],\n",
      "          [0.3255, 0.2941, 0.3333,  ..., 0.3059, 0.2902, 0.2549],\n",
      "          ...,\n",
      "          [0.2078, 0.2275, 0.2431,  ..., 0.2275, 0.2235, 0.2157],\n",
      "          [0.1804, 0.2157, 0.2000,  ..., 0.2784, 0.2078, 0.1961],\n",
      "          [0.1608, 0.2039, 0.2078,  ..., 0.3608, 0.2000, 0.1725]],\n",
      "\n",
      "         [[0.3373, 0.3176, 0.2784,  ..., 0.2353, 0.2039, 0.2431],\n",
      "          [0.3412, 0.3059, 0.3059,  ..., 0.2667, 0.2392, 0.2549],\n",
      "          [0.3255, 0.2941, 0.3333,  ..., 0.3059, 0.2824, 0.2471],\n",
      "          ...,\n",
      "          [0.2118, 0.2314, 0.2471,  ..., 0.1922, 0.1686, 0.1725],\n",
      "          [0.1843, 0.2196, 0.2039,  ..., 0.2549, 0.1569, 0.1490],\n",
      "          [0.1647, 0.2078, 0.2118,  ..., 0.3529, 0.1569, 0.1294]],\n",
      "\n",
      "         [[0.3373, 0.3176, 0.2784,  ..., 0.2157, 0.1843, 0.2235],\n",
      "          [0.3412, 0.3059, 0.3059,  ..., 0.2431, 0.2196, 0.2353],\n",
      "          [0.3255, 0.2941, 0.3333,  ..., 0.2863, 0.2627, 0.2275],\n",
      "          ...,\n",
      "          [0.1922, 0.2118, 0.2275,  ..., 0.1843, 0.1647, 0.1686],\n",
      "          [0.1647, 0.2000, 0.1843,  ..., 0.2588, 0.1529, 0.1451],\n",
      "          [0.1451, 0.1882, 0.1922,  ..., 0.3725, 0.1569, 0.1216]]]]), tensor([[5.]]))\n",
      "iar shape-ul celor 2 tensori sunt : torch.Size([1, 3, 32, 32]) \n",
      " & \n",
      " torch.Size([1, 1])\n",
      "Iterarea prin dataset cu worker-ul pe procesul curent dureaza 3.8553009033203125 secunde\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaJ0EJXOIAjR"
   },
   "source": [
    "## Definirea unei retele cu un singur layer\n",
    "\n",
    "Modulul *torch.nn* contine clase si functii utilitare pentru crearea retelelor neurale. Pentru a crea o retea neurala se va defini o clasa, *SingleLayerNet* ce mosteneste din clasa *torch.nn.Module*. Clasele din Pytorch ce definesc straturi, functii de activare, si functii cost mostenesc din clasa *torch.nn.Module*. Aceste clase implementeaza metoda **forward()** care este folosita pentru a defini ce se intampla la un forward pass. Aceasta metoda este apelata in metoda __\\_\\_call\\_\\_()__ a clasei.\n",
    "\n",
    "Clasa *Linear* din *torch.nn*, ce mosteneste din *nn.Module* defineste un strat 'fully-connected'. Contructorul primeste 3 parametrii:\n",
    " * Dimensiunea vectorului de intrare\n",
    " * Dimensiunea vectorului de iesire\n",
    " * Daca sa se foloseasca *bias* sau nu\n",
    "\n",
    "Clasa Simgoid din *torch.nn* , ce mosteneste din *nn.Module*, defineste o functie de activare sigmoid.\n",
    "\n",
    "### Cerinte\n",
    "  1. In constructorul clasei *SingleLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Linear*\n",
    "  2. In constructorul clase *SingleLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Sigmoid*\n",
    "  3. In metoda __forward()__ definiti o variabila care sa contina iesirea stratului linear aplicat pe intrarea 'x'.\n",
    "  4. In metoda __forward()__ definiti o variabila care sa contina iesirea functiei de activare sigmoid aplicata pe iesirea stratului linear si returnati aceasta variabila\n",
    "\n",
    "#### Hint\n",
    " * Imaginea de intrare are dimensiune 32x32x3 (inaltime x latime x canale). Vectorul de intrare in retea va avea dimensiune 3072.\n",
    "\n",
    "#### Atentie\n",
    " * Dimensiunea de iesire a stratului trebuie sa fie de aceeasi marime cu numarul de clase."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WrSKc8_Hplb2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d1c8f579-c151-44ae-e761-dee95a678285",
    "ExecuteTime": {
     "end_time": "2023-10-16T12:09:18.978545800Z",
     "start_time": "2023-10-16T12:09:18.920449500Z"
    }
   },
   "source": [
    "### Exemplu utilizare metoda __call__()\n",
    "class A(object):\n",
    "  def __init__(self):\n",
    "    self.a ='A'\n",
    "\n",
    "  def __call__(self, mesaj):\n",
    "    print(self.a, mesaj)\n",
    "\n",
    "obj = A()\n",
    "obj(\"OK!\")"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A OK!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2vL3FhX5Xi2j",
    "ExecuteTime": {
     "end_time": "2023-10-16T12:50:56.352805700Z",
     "start_time": "2023-10-16T12:50:56.343805400Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SingleLayerNet(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(SingleLayerNet, self).__init__()\n",
    "\n",
    "\n",
    "    self.linear1 = nn.Linear(3072, 10, bias=False)\n",
    "    self.activation1 = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    return self.activation1(self.linear1(x))\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB7O0mVkG231"
   },
   "source": [
    "# Definirea unei retele cu doua straturi (layere). (2p)\n",
    "\n",
    "### Cerinte\n",
    "  1. In constructorul clasei *TwoLayerNet* definiti doua atribute care sa contina doua obiecte de tip *nn.Linear*. (0.5p)\n",
    "  2. In constructorul clasei *TwoLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Sigmoid* (se poate folosi acelasi obiect de tip *nn.Sigmoid* pentru activarea ambelor straturi *nn.Linear*) (0.5p)\n",
    "  3. Implementati metoda **forward()** similar cu exercitul anterior (layer1->activation->layer2->activation). (1p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SVAxeNFXHURH",
    "ExecuteTime": {
     "end_time": "2023-10-16T13:57:14.096164500Z",
     "start_time": "2023-10-16T13:57:14.086149200Z"
    }
   },
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    # Cerinta 1 - completati codul aici\n",
    "    self.linear1 = nn.Linear(3072, 3072//4, bias=True)\n",
    "    self.linear2 = nn.Linear(3072//4, 10, bias=True)\n",
    "\n",
    "    # Cerinta 2 - completati codul aici\n",
    "    self.activation1 = nn.ReLU()\n",
    "    self.activation2 = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    # Cerinta 3 si 4- completati codul aici\n",
    "    x = self.linear1(x)\n",
    "    x = self.activation1(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.activation2(x)\n",
    "\n",
    "    return x\n",
    "    #return output\n"
   ],
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZNoELFZrRpC"
   },
   "source": [
    "## Preprocesare pentru retele cu straturi 'fully-connected' (2.5 p)\n",
    "\n",
    "Avand o retea cu straturi 'fully-connected' este necesar ca imaginea sa fie redimensionata intr-un vector. Clasa *torch.Tensor* defineste metoda __view()__ care returneaza un tensor redimensionat.\n",
    "\n",
    "### Cerinte\n",
    "  1. Completati functia de mai jos pentru a redimensiona imaginea intr-un vector. (1p)\n",
    "  2. Extrageti un batch din DataLoader-ul de antrenare si printati dimensiunile imaginii. (1.5p)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lxDsD6hSs8zL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eab8c30a-be8a-4728-816d-ac29327fd766",
    "ExecuteTime": {
     "end_time": "2023-10-16T12:09:18.989056800Z",
     "start_time": "2023-10-16T12:09:18.965493Z"
    }
   },
   "source": [
    "# Varianta numpy\n",
    "img1 = np.random.rand(32, 32, 3)\n",
    "img2 = np.random.rand(32, 32, 3)\n",
    "\n",
    "print(img1.shape, img2.shape)\n",
    "\n",
    "reshaped1 = img1.reshape(-1)\n",
    "reshaped2 = img2.reshape(32*32*3)\n",
    "\n",
    "print(reshaped1.shape, reshaped2.shape)"
   ],
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3) (32, 32, 3)\n",
      "(3072,) (3072,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:51:00.858321800Z",
     "start_time": "2023-10-16T12:51:00.846811700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ctqQtdWgW-7V",
    "ExecuteTime": {
     "end_time": "2023-10-16T13:57:17.406451300Z",
     "start_time": "2023-10-16T13:57:17.382928500Z"
    }
   },
   "source": [
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "def plot_weights(net: nn.Module):\n",
    "  net.to(torch.device(\"cpu\"))\n",
    "  named_params = net.named_parameters()\n",
    "  np_params = []\n",
    "  np_param_names = []\n",
    "  for name, param in named_params:\n",
    "    np_params.append(param.clone().detach().view(-1).numpy())\n",
    "    np_param_names.append(name)\n",
    "\n",
    "  fig = plt.figure(figsize=(20, 2.5))\n",
    "\n",
    "  count = len(np_param_names)\n",
    "  for i in range(count):\n",
    "    plt.subplot(1, count, i+1)\n",
    "    plt.hist(np_params[i], bins=25)\n",
    "    plt.title(np_param_names[i])\n",
    "  plt.show()\n",
    "  net.to(torch.device(\"cuda\"))\n",
    "\n",
    "def preproc_liniarized_fn(examples):\n",
    "  processed_images = []\n",
    "  processed_labels = []\n",
    "\n",
    "  for example in examples:\n",
    "    tensor_image = to_tensor(example[0])\n",
    "    # In linia de mai jos imaginea este normalizata astfel incat sa aiba toate valorile in \n",
    "    # [-1, 1] in loc de [0, 255]\n",
    "    normalized_tensor_image = normalize(tensor_image, [0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    # TODO: Cerinta 1 - completati codul aici \n",
    "    vector_image = normalized_tensor_image.reshape(32 * 32 * 3)\n",
    "    vector_image = vector_image.unsqueeze(0)\n",
    "    processed_images.append(vector_image)\n",
    "\n",
    "    label = np.array(example[1])\n",
    "    tensor_label = torch.tensor(label).to(torch.int64)\n",
    "    tensor_label = tensor_label.unsqueeze(0)\n",
    "    processed_labels.append(tensor_label)\n",
    "\n",
    "  torch_images = torch.cat(processed_images, dim=0)\n",
    "  torch_labels = torch.cat(processed_labels, dim=0)\n",
    "\n",
    "  return torch_images, torch_labels\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = data.DataLoader(cifar_train, batch_size=batch_size, shuffle=True, collate_fn=preproc_liniarized_fn,\n",
    "                               pin_memory=True) # Use pin_memory for faster GPU transfer if available\n",
    "test_loader = data.DataLoader(cifar_test, batch_size=1, shuffle=False, collate_fn=preproc_liniarized_fn,\n",
    "                              pin_memory=True) # Use pin_memory for faster GPU transfer if available\n",
    "\n",
    "# TODO: Cerinta 2 - completati codul aici\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "images, classes = batch\n",
    "print(f\"dimensiuena imaginilor: {images[0].shape}\")\n",
    "print(f\"type train data: {classes[0].dtype}\")\n"
   ],
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensiuena imaginilor: torch.Size([3072])\n",
      "type train data: torch.int64\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzQgrTWBvvPN"
   },
   "source": [
    "## Definirea antrenarii\n",
    "\n",
    "Pentru definirea antrenarii avem urmatorii pasi:\n",
    " * Definirea numarului de epoci (de cate ori parcurgem intregul dataset)\n",
    " * Definirea obiectului de tip *SingleLayerNet*\n",
    " * Definirea optimizatorului. Vom folosi Stochastic Gradient Descent (SGD) pentru optimizarea retelei, prin urmare definim un obiect de tip *optim.SGD*. Constructorul acestei clase primeste parametrii pe care trebuie sa-i optimizeze (single_layer_net.parameters()) si rata de invatare (lr=1e-2)\n",
    " * Definim functia cost de tip *nn.CrossEntropyLoss()*\n",
    " * Definim functa care parcurge datasetul si antreneaza reteaua."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eqAWKYNqZeuS",
    "ExecuteTime": {
     "end_time": "2023-10-16T13:57:20.239756600Z",
     "start_time": "2023-10-16T13:57:20.225243400Z"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Definim numarul de epoci\n",
    "epochs = 20\n",
    "\n",
    "# Definim reteaua\n",
    "single_layer_net = SingleLayerNet().to(device)\n",
    "two_layer_net = TwoLayerNet().to(device)\n",
    "\n",
    "# Definim optimizatorul\n",
    "optimizer = optim.SGD(single_layer_net.parameters(), lr=1e-2)\n",
    "# Dupa definirea optimizatorului si dupa fiecare iteratie trebuie apelata functia zero_grad().\n",
    "# Aceasta face toti gradientii zero.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Definim functia cost\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_fn(epochs: int, train_loader: data.DataLoader, test_loader: data.DataLoader,\n",
    "             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer):\n",
    "  # Iteram prin numarul de epoci\n",
    "  for e in range(epochs):\n",
    "    # Iteram prin fiecare exemplu din dataset\n",
    "    for images, labels in train_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "      # Aplicam reteaua neurala pe imaginile de intrare\n",
    "      out = net(images)\n",
    "      # Aplicam functia cost pe iesirea retelei neurale si pe adnotarile imaginilor \n",
    "      loss = loss_fn(out, labels)\n",
    "      # Aplicam algoritmul de back-propagation\n",
    "      loss.backward()\n",
    "      # Facem pasul de optimizare, pentru a aplica gradientii pe parametrii retelei\n",
    "      optimizer.step()\n",
    "      # Apelam functia zero_grad() pentru a uita gradientii de la iteratie curenta\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    print(\"Loss-ul la finalul epocii {} are valoarea {}\".format(e, loss.item()))\n",
    "\n",
    "    # Calculul acuratetii\n",
    "    count = len(test_loader)\n",
    "    correct = 0\n",
    "\n",
    "    for test_image, test_label in test_loader:\n",
    "      test_image, test_label = test_image.to(device), test_label.to(device)\n",
    "      out_class = torch.argmax(net(test_image))\n",
    "      if out_class == test_label:\n",
    "        correct += 1\n",
    "\n",
    "    print(\"Acuratetea la finalul epocii {} este {:.2f}%\".format(e, (correct / count) * batch_size))\n",
    "    # plot_weights(net)"
   ],
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE-xJ2b52Dw9"
   },
   "source": [
    "## Antrenam propria retea"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pwo4dQ9b_AxM",
    "ExecuteTime": {
     "end_time": "2023-10-16T13:57:23.238564100Z",
     "start_time": "2023-10-16T13:57:23.080712100Z"
    }
   },
   "source": [
    "train_fn(epochs, train_loader, test_loader, two_layer_net, loss_fn, optimizer)"
   ],
   "execution_count": 61,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [100, 3072]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtwo_layer_net\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[60], line 28\u001B[0m, in \u001B[0;36mtrain_fn\u001B[1;34m(epochs, train_loader, test_loader, net, loss_fn, optimizer)\u001B[0m\n\u001B[0;32m     25\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Aplicam reteaua neurala pe imaginile de intrare\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Aplicam functia cost pe iesirea retelei neurale si pe adnotarile imaginilor \u001B[39;00m\n\u001B[0;32m     30\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(out, labels)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PyTorchTest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PyTorchTest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[58], line 12\u001B[0m, in \u001B[0;36mTwoLayerNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 12\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(nn\u001B[38;5;241m.\u001B[39mReLU(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     13\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(nn\u001B[38;5;241m.\u001B[39mReLU(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n\u001B[0;32m     14\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m192\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PyTorchTest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PyTorchTest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PyTorchTest\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\PyTorchTest\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [100, 3072]"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAGFBjaD2Ji6"
   },
   "source": [
    "## Definirea unei retele cu 2 straturi si antrenarea ei (2.5 p)\n",
    "\n",
    "### Cerinte\n",
    " 1. Instantiati un obiect de tip *TwoLayerNet*. (0.5p)\n",
    " 2. Definiti un optimizator pentru antrenarea acestei retele (1p)\n",
    " 3. Folositi functia deifnita mai sus pentru a antrena aceasta retea (*train_fn*). (1p)\n",
    "\n",
    "#### Atentie\n",
    " * Dimensiunea de iesire a primului strat trebuie sa se potriveasca cu dimensiunea de intrare a celui de-al doilea."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nZbXpt9KI6yT"
   },
   "source": [
    "#TODO: Cerinta 1 - completati codul aici\n",
    "\n",
    "# Instantierea retelei\n",
    "# two_layer_net = ...\n",
    "\n",
    "#TODO: Cerinta 2 - completati codul aici\n",
    "#optimizer2 = ...\n",
    "\n",
    "# Dupa definirea optimizatorului si dupa fiecare iteratie trebuie apelata functia zero_grad().\n",
    "# Aceasta face toti gradientii zero.\n",
    "optimizer2.zero_grad()\n",
    "\n",
    "# Definim functia de cost\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "\n",
    "#TODO: Cerinta 3 - Antrenati reteaua\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICxUifGp3PY1"
   },
   "source": [
    "## Crearea dinamica a unei retele\n",
    "\n",
    "In  *torch.nn* exista clasa *Sequential* care primeste o lista de straturi si functii de activare in ordinea in care trebuie aplicate, e.g. [linear, sigmoid, linear, sigmoid]. Rezultatul este inlantuirea acestor straturi si functii de activare.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vekIJr0pjfcv"
   },
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, layer_sizes: list, activation: type):\n",
    "    \"\"\"\n",
    "      Constructor.\n",
    "\n",
    "      :param layer_sizes - Parametru de tip lista care contine dimensiunile fiecarui strat din retea\n",
    "      :param activation - Parametru de tip type. Poate fi nn.Sigmoid, nn.Tanh, nn.ReLU. Adica clasa pentru a instantia mai tarziu\n",
    "    \"\"\"\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    for i in range(0, len(layer_sizes)):\n",
    "      inl, out = layer_sizes[i]\n",
    "      layers.append(nn.Linear(inl, out))\n",
    "      layers.append(activation)\n",
    "\n",
    "    self.net = nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    return self.net(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RTE0XJg5ppw"
   },
   "source": [
    "## [BONUS] Antrenarea retelei cu N straturi (2p)\n",
    "\n",
    "Mai jos functia de antrenare a fost modificata pentru a afisa modificarile parametrilor retelei in timpul antrenarii. Aceasta se obtine prin implementarea functiei __plot_weights()__.\n",
    "\n",
    "### Cerinte \n",
    "\n",
    "  1. Creati un obiect de tipul *Net* (0.25)\n",
    "  2. Creati un optimizator pentru reteaua de tipul *Net* (0.25)\n",
    "  3. Antrenati reteaua folosind functia __plotting_train_fn()__ (0.25)\n",
    "  4. Experimentati cu retele straturi si functii de activare diferite (0.75)\n",
    "  5. Modificati celula de mai jos si scrieti o functie numita __plot_loss()__ pentru a afisa un grafic care arata evolutia rezultatului functiei cost in timp. Faceti acelasi lucru si pentru acuratete. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jZ8vr8fjoEs-"
   },
   "source": [
    "def plot_weights(net: nn.Module):\n",
    "  named_params = net.named_parameters()\n",
    "  np_params = []\n",
    "  np_param_names = []\n",
    "  for name, param in named_params:\n",
    "    np_params.append(param.clone().detach().view(-1).numpy())\n",
    "    np_param_names.append(name)\n",
    "\n",
    "  fig = plt.figure(figsize=(20, 2.5))\n",
    "\n",
    "  count = len(np_param_names)\n",
    "  for i in range(count):\n",
    "    plt.subplot(1, count, i+1)\n",
    "    plt.hist(np_params[i], bins=25)\n",
    "    plt.title(np_param_names[i])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plotting_train_fn(epochs: int, train_loader: data.DataLoader, test_loader: data.DataLoader,\n",
    "                      net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer):\n",
    "  for e in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      out = net(images)\n",
    "      loss = loss_fn(out, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "\n",
    "    print(\"Loss-ul la finalul epocii {} are valoarea {}\".format(e, loss.item()))\n",
    "\n",
    "    count = len(test_loader)\n",
    "    correct = 0\n",
    "\n",
    "    for test_image, test_label in test_loader:\n",
    "      out_class = torch.argmax(net(test_image))\n",
    "      if out_class == test_label:\n",
    "        correct += 1\n",
    "\n",
    "    print(\"Acuratetea la finalul epocii {} este {:.2f}%\".format(e, (correct / count) * 100))\n",
    "    plot_weights(net)\n",
    "\n",
    "# Cerinta 1 - completati codul aici\n",
    "\n",
    "# Cerinta 2 - completati codul aici\n",
    "\n",
    "# Cerinta 3 - completati codul aici\n",
    "\n",
    "# Cerinta 5 - completati codul aici\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
