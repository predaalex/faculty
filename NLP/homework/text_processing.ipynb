{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\allex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:05.983529700Z",
     "start_time": "2024-03-25T14:19:05.955518200Z"
    }
   },
   "id": "fcf48a8ddd9dea3d",
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Download it through python (inside the code, so you don't have to upload the file too when you send the solution for this exercise) with urlopen() from module urllib and read the entire text in one single string. If the download takes too much time at each running, download the file, but leave the former instructions in a comment (to show that you know how to access an online file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3225e63ceb63632"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:07.124306300Z",
     "start_time": "2024-03-25T14:19:05.986529300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Pride and prejudice, by Jane Austen\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere in the United States and\r\n",
      "most other parts of the world at no cost and with almost no restrictions\r\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
      "of the Project Gutenberg License included with this eBook or online at\r\n",
      "www.gutenberg.org. If you are not located in the United States, you\r\n",
      "will have to check the laws of the country where you are located before\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "def download_text(url):\n",
    "    try:\n",
    "        with urlopen(url) as response:\n",
    "            text = response.read().decode('utf-8')\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "\n",
    "# Download the text\n",
    "text = download_text(url)\n",
    "\n",
    "if text:\n",
    "    # Print the first 500 characters of the text\n",
    "    print(text[:500])\n",
    "else:\n",
    "    print(\"Failed to download the text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Remove the header (keep only the text starting from the title)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "112183e8521352df"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                            [Illustration:\r\n",
      "\r\n",
      "                             GEORGE ALLEN\r\n",
      "                               PUBLISHER\r\n",
      "\r\n",
      "                        156 CHARING CROSS ROAD\r\n",
      "                                LONDON\r\n",
      "\r\n",
      "                             RUSKIN HOUSE\r\n",
      "                                   ]\r\n",
      "\r\n",
      "                            [Illustration:\r\n",
      "\r\n",
      "               _Reading Jane’s Letters._      _Chap 34._\r\n",
      "                                   ]\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                                \n"
     ]
    }
   ],
   "source": [
    "def remove_header(text):\n",
    "    start_of_book = \"*** START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\"\n",
    "    start_index = text.find(start_of_book)\n",
    "    if start_index != -1:\n",
    "        return text[start_index + len(start_of_book):]\n",
    "    else:\n",
    "        print(\"Header pattern not found.\")\n",
    "        return text\n",
    "\n",
    "text_without_header = remove_header(text)\n",
    "\n",
    "print(text_without_header[:500])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:07.159185200Z",
     "start_time": "2024-03-25T14:19:07.126306400Z"
    }
   },
   "id": "6a8e615da8bb82da",
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Print the number of sentences in the text. Print the average length (number of words) of a sentence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef296d1c0fe2e77"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4910\n",
      "Average length of a sentence (in terms of number of words): 31.418126272912424\n"
     ]
    }
   ],
   "source": [
    "def count_sentences_and_avg_length(text):\n",
    "    sentences = nltk.sent_tokenize(text) # tokenize text into senteces\n",
    "\n",
    "    num_sentences = len(sentences) \n",
    "\n",
    "    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences) # tokenize each sentence into words and calculate total of words\n",
    "\n",
    "    avg_length = total_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return num_sentences, avg_length\n",
    "\n",
    "num_sentences, avg_length = count_sentences_and_avg_length(text_without_header)\n",
    "\n",
    "print(\"Number of sentences:\", num_sentences)\n",
    "print(\"Average length of a sentence (in terms of number of words):\", avg_length)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:07.658635200Z",
     "start_time": "2024-03-25T14:19:07.140135700Z"
    }
   },
   "id": "9e131fd58f4c2d29",
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Find the collocations in the text (bigram and trigram). Use the nltk.collocations module You will print them only once not each time they appear."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1256c764fe678b7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Collocations:\n",
      "'AS-IS '\n",
      "1500 West\n",
      "20 %\n",
      "24 Tailpiece\n",
      "809 North\n",
      "AGREEMENT WILL\n",
      "ALLEN PUBLISHER\n",
      "August 2_\n",
      "BE LIABLE\n",
      "CAROLINE BINGLEY.\n",
      "\n",
      "Trigram Collocations:\n",
      "809 North 1500\n",
      "CHARING CROSS ROAD\n",
      "CONTRACT EXCEPT THOSE\n",
      "EXCEPT THOSE PROVIDED\n",
      "Elizabeth._ _GEORGE SAINTSBURY._\n",
      "Frontispiece iv Title-page\n",
      "GEORGE ALLEN PUBLISHER\n",
      "H.T Feb 94\n",
      "Internal Revenue Service\n",
      "J. Comyns Carr\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "def find_collocations(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Initialize BigramCollocationFinder and TrigramCollocationFinder\n",
    "    bigram_finder = nltk.BigramCollocationFinder.from_words(words)\n",
    "    trigram_finder = nltk.TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "    # Find bigram collocations\n",
    "    bigram_collocations = bigram_finder.nbest(BigramAssocMeasures.pmi, 10)\n",
    "\n",
    "    # Find trigram collocations\n",
    "    trigram_collocations = trigram_finder.nbest(TrigramAssocMeasures.pmi, 10)\n",
    "\n",
    "    return bigram_collocations, trigram_collocations\n",
    "\n",
    "# Call the function with the text without the header\n",
    "bigram_collocations, trigram_collocations = find_collocations(text_without_header)\n",
    "\n",
    "# Print the bigram collocations\n",
    "print(\"Bigram Collocations:\")\n",
    "for bigram in bigram_collocations:\n",
    "    print(' '.join(bigram))\n",
    "\n",
    "# Print the trigram collocations\n",
    "print(\"\\nTrigram Collocations:\")\n",
    "for trigram in trigram_collocations:\n",
    "    print(' '.join(trigram))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:09.163345200Z",
     "start_time": "2024-03-25T14:19:07.734914100Z"
    }
   },
   "id": "5849924013b70bdb",
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Create a list of all the words (in lower case) from the text, without the punctuation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59fe9fb9c09a4392"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['illustration', 'george', 'allen', 'publisher', '156', 'charing', 'cross', 'road', 'london', 'ruskin', 'house', 'illustration', 'jane', 's', 'pride', 'and', 'prejudice', 'by', 'jane', 'austen', 'with', 'a', 'preface', 'by', 'george', 'saintsbury', 'and', 'illustrations', 'by', 'hugh', 'thomson', 'illustration', '1894', 'ruskin', '156', 'charing', 'house', 'cross', 'road', 'london', 'george', 'allen', 'chiswick', 'press', 'charles', 'whittingham', 'and', 'tooks', 'court', 'chancery']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(text_without_header)\n",
    "\n",
    "words_without_punctuation = [word.lower() for word in words if word.isalnum()] # alpha-numeric strings\n",
    "\n",
    "print(words_without_punctuation[:50])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:09.580421400Z",
     "start_time": "2024-03-25T14:19:09.160343300Z"
    }
   },
   "id": "84a9b8e3659debb7",
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Print the first N most frequent words (alphanumeric strings) together with their number of appearances."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f281759f7a253344"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent words:\n",
      "the: 4832\n",
      "to: 4377\n",
      "of: 3950\n",
      "and: 3816\n",
      "her: 2248\n",
      "i: 2097\n",
      "a: 2090\n",
      "in: 2033\n",
      "was: 1870\n",
      "she: 1732\n",
      "not: 1629\n",
      "that: 1619\n",
      "it: 1571\n",
      "you: 1392\n",
      "he: 1349\n",
      "his: 1288\n",
      "be: 1279\n",
      "as: 1238\n",
      "had: 1180\n",
      "with: 1145\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = nltk.word_tokenize(text_without_header)\n",
    "\n",
    "# Remove punctuation from the words and convert to lowercase\n",
    "words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Sort word frequencies by count in descending order\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Number of most frequent words to print\n",
    "N = 20  # Change N to print more or fewer words\n",
    "\n",
    "# Print the first N most frequent words and their counts\n",
    "print(f\"Top {N} most frequent words:\")\n",
    "for word, freq in sorted_word_freq[:N]:\n",
    "    print(f\"{word}: {freq}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:10.020443Z",
     "start_time": "2024-03-25T14:19:09.657082500Z"
    }
   },
   "id": "455c77d93b4874c9",
   "execution_count": 96
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Remove stopwords and assign the result to variable lws"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c84de8e591e4072"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\allex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['illustration', 'george', 'allen', 'publisher', '156', 'charing', 'cross', 'road', 'london', 'ruskin', 'house', 'illustration', 'jane', 'pride', 'prejudice', 'jane', 'austen', 'preface', 'george', 'saintsbury']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "words = nltk.word_tokenize(text_without_header)\n",
    "\n",
    "words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lws = [word for word in words if word not in stop_words]\n",
    "\n",
    "print(lws[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:10.437963Z",
     "start_time": "2024-03-25T14:19:10.017442200Z"
    }
   },
   "id": "caf9ad75e4fc35e3",
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Apply stemming (Porter) on the list of words (lws). Print the first 200 words. Do you see any words that don't appear in the dictionary?\n",
    "\n",
    "Stemming might produce non-words or words that are not present in a standard English dictionary. Stemming involves reducing words to their root or base form, so the stemmed words may not always correspond to actual words in the English language. Therefore, it's possible that some of the stemmed words won't appear in a dictionary.\n",
    "\n",
    "Another better approach is using Lemmatization, which aims to return the base or dictionary from of a words using a vocabulary and morphological analysis of words to ensure that the resulting lemma is a valid word in the language"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c8af944a4a08761"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['illustr', 'georg', 'allen', 'publish', '156', 'chare', 'cross', 'road', 'london', 'ruskin', 'hous', 'illustr', 'jane', 'pride', 'prejudic', 'jane', 'austen', 'prefac', 'georg', 'saintsburi', 'illustr', 'hugh', 'thomson', 'illustr', '1894', 'ruskin', '156', 'chare', 'hous', 'cross', 'road', 'london', 'georg', 'allen', 'chiswick', 'press', 'charl', 'whittingham', 'took', 'court', 'chanceri', 'lane', 'london', 'illustr', 'comyn', 'carr', 'acknowledg', 'owe', 'friendship', 'advic', 'illustr', 'grate', 'prefac', 'illustr', 'whitman', 'somewher', 'fine', 'distinct', 'love', 'allow', 'love', 'person', 'distinct', 'appli', 'book', 'well', 'men', 'women', 'case', 'numer', 'author', 'object', 'person', 'affect', 'bring', 'curiou', 'consequ', 'much', 'differ', 'best', 'work', 'case', 'other', 'love', 'allow', 'convent', 'felt', 'right', 'proper', 'thing', 'love', 'sect', 'fairli', 'larg', 'yet', 'unusu', 'choic', 'austenian', 'janit', 'would', 'probabl', 'found', 'partisan', 'claim', 'primaci', 'almost', 'everi', 'one', 'novel', 'delight', 'fresh', 'humour', 'northang', 'abbey', 'complet', 'finish', 'entrain', 'undoubt', 'critic', 'fact', 'scale', 'small', 'scheme', 'burlesqu', 'parodi', 'kind', 'first', 'rank', 'reach', 'persuas', 'faint', 'tone', 'enthral', 'interest', 'devote', 'exalt', 'other', 'exquisit', 'delicaci', 'keep', 'catastroph', 'mansfield', 'park', 'admittedli', 'theatric', 'hero', 'heroin', 'insipid', 'author', 'almost', 'wickedli', 'destroy', 'romant', 'interest', 'expressli', 'admit', 'edmund', 'took', 'fanni', 'mari', 'shock', 'fanni', 'might', 'like', 'taken', 'crawford', 'littl', 'assidu', 'yet', 'matchless', 'charact', 'norri', 'other', 'secur', 'believ', 'consider', 'parti', 'sens', 'sensibl', 'perhap', 'fewest', 'admir', 'want', 'suppos', 'howev', 'major', 'least', 'compet', 'vote', 'would', 'thing', 'consid', 'divid', 'emma', 'present', 'book', 'perhap', 'vulgar', 'verdict', 'inde']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize Porter stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to the list of words\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in lws]\n",
    "\n",
    "# Print the first 200 stemmed words\n",
    "print(stemmed_words[:200])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:11.037205700Z",
     "start_time": "2024-03-25T14:19:10.468155900Z"
    }
   },
   "id": "eff1b120b4e7e6d2",
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Print a table of three columns (of size N, where N is the maximum length for the words in the text). The columns will be separated with the character \"|\". The head of the table will be:\n",
    "## Porter    |Lancaster |Snowball\n",
    "# The table will contain only the words that give different stemming results for the three stemmers (for example, suppose that we have both \"runs\" and \"being\" inside the text. The word \"runs\" should not appear in the list, as all three results are \"run\"; however \"being\" should appear in the table). The stemming result for the word for each stemmer will appear in the table according to the head of the table. The table will contain the results for the first NW words from the text (the number of rows will obviously be less than NW, as not all words match the requirements). For example, NW=500. Try to print only distinct results inside the table (for example, if a word has two occurnces inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39a2f8cea31d62f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter          | Lancaster       | Snowball       \n",
      "illustr         | illust          | illustr        \n",
      "allen           | al              | allen          \n",
      "publish         | publ            | publish        \n",
      "chare           | char            | chare          \n",
      "jane            | jan             | jane           \n",
      "pride           | prid            | pride          \n",
      "prejudic        | prejud          | prejudic       \n",
      "austen          | aust            | austen         \n",
      "saintsburi      | saintsbury      | saintsburi     \n",
      "chanceri        | chancery        | chanceri       \n",
      "lane            | lan             | lane           \n",
      "carr            | car             | carr           \n",
      "all             | al              | all            \n",
      "owe             | ow              | owe            \n",
      "hi              | his             | his            \n",
      "friendship      | friend          | friendship     \n",
      "advic           | adv             | advic          \n",
      "these           | thes            | these          \n",
      "are             | ar              | are            \n",
      "grate           | grat            | grate          \n",
      "whitman         | whitm           | whitman        \n",
      "ha              | has             | has            \n",
      "somewher        | somewh          | somewher       \n",
      "fine            | fin             | fine           \n",
      "love            | lov             | love           \n",
      "thi             | thi             | this           \n",
      "appli           | apply           | appli          \n",
      "well            | wel             | well           \n",
      "women           | wom             | women          \n",
      "case            | cas             | case           \n",
      "veri            | very            | veri           \n",
      "numer           | num             | numer          \n",
      "author          | auth            | author         \n",
      "curiou          | cury            | curious        \n",
      "there           | ther            | there          \n",
      "more            | mor             | more           \n",
      "differ          | diff            | differ         \n",
      "those           | thos            | those          \n",
      "other           | oth             | other          \n",
      "convent         | conv            | convent        \n",
      "proper          | prop            | proper         \n",
      "fairli          | fair            | fair           \n",
      "unusu           | unus            | unusu          \n",
      "choic           | cho             | choic          \n",
      "austenian       | aust            | austenian      \n",
      "probabl         | prob            | probabl        \n",
      "partisan        | part            | partisan       \n",
      "primaci         | prim            | primaci        \n",
      "everi           | every           | everi          \n",
      "one             | on              | one            \n",
      "\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "NW = 50\n",
    "\n",
    "distinct_stemmed_results = set()\n",
    "\n",
    "table = f\"{'Porter':<15} | {'Lancaster':<15} | {'Snowball':<15}\\n\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "for word in words:\n",
    "    porter_stem = porter_stemmer.stem(word)\n",
    "    lancaster_stem = lancaster_stemmer.stem(word)\n",
    "    snowball_stem = snowball_stemmer.stem(word)\n",
    "    \n",
    "    # Check if stemming results are different\n",
    "    if porter_stem != lancaster_stem or porter_stem != snowball_stem:\n",
    "        # Check if the stemmed result is not already in the set\n",
    "        if (porter_stem, lancaster_stem, snowball_stem) not in distinct_stemmed_results:\n",
    "            # Add the stemmed result to the set\n",
    "            distinct_stemmed_results.add((porter_stem, lancaster_stem, snowball_stem))\n",
    "            \n",
    "            # Add the stemmed result to the table\n",
    "            table += f\"{porter_stem:<15} | {lancaster_stem:<15} | {snowball_stem:<15}\\n\"\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if count == NW:\n",
    "                break\n",
    "\n",
    "# Print the table\n",
    "print(table)\n",
    "print(len(distinct_stemmed_results))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:11.050257400Z",
     "start_time": "2024-03-25T14:19:11.041257900Z"
    }
   },
   "id": "aab46782bb3484f3",
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10. Print a table of two columns, simillar to the one above, that will compare the results of stemming and lemmatization. The head of the table will contain the values: \"Snowball\" and \"WordNetLemmatizer\". The table must contain only words that give different results in the process of stemming and lemmatization (for example, the word \"running\"). The table will contain the results for the first NW words from the text (the number of rows will obviously be less than NW, as not all words match the requirements). For example, NW=500. Try to print only distinct results inside the table (for example, if a word has two occurnces inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b032569c97df462"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowball        | WordNetLemmatizer\n",
      "illustr         | illustration   \n",
      "georg           | george         \n",
      "publish         | publisher      \n",
      "chare           | charing        \n",
      "hous            | house          \n",
      "prejudic        | prejudice      \n",
      "prefac          | preface        \n",
      "saintsburi      | saintsbury     \n",
      "charl           | charles        \n",
      "took            | tooks          \n",
      "chanceri        | chancery       \n",
      "comyn           | comyns         \n",
      "acknowledg      | acknowledgment \n",
      "advic           | advice         \n",
      "grate           | gratefully     \n",
      "has             | ha             \n",
      "somewher        | somewhere      \n",
      "distinct        | distinction    \n",
      "love            | loving         \n",
      "allow           | allowance      \n",
      "person          | personal       \n",
      "appli           | applies        \n",
      "as              | a              \n",
      "women           | woman          \n",
      "veri            | very           \n",
      "numer           | numerous       \n",
      "affect          | affection      \n",
      "bring           | brings         \n",
      "consequ         | consequence    \n",
      "differ          | difference     \n",
      "other           | others         \n",
      "love            | loved          \n",
      "convent         | convention     \n",
      "becaus          | because        \n",
      "fair            | fairly         \n",
      "larg            | large          \n",
      "unusu           | unusually      \n",
      "choic           | choice         \n",
      "austenian       | austenians     \n",
      "janit           | janites        \n",
      "probabl         | probably       \n",
      "primaci         | primacy        \n",
      "everi           | every          \n",
      "delight         | delightful     \n",
      "fresh           | freshness      \n",
      "northang        | northanger     \n",
      "complet         | completeness   \n",
      "undoubt         | undoubted      \n",
      "critic          | critical       \n",
      "burlesqu        | burlesque      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\allex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "distinct_results = set()\n",
    "\n",
    "table = f\"{'Snowball':<15} | {'WordNetLemmatizer':<15}\\n\"\n",
    "\n",
    "count = 0\n",
    "\n",
    "for word in words:\n",
    "    snowball_stem = snowball_stemmer.stem(word)\n",
    "    wordnet_lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "    \n",
    "    if snowball_stem != wordnet_lemma:\n",
    "        if (snowball_stem, wordnet_lemma) not in distinct_results:\n",
    "            distinct_results.add((snowball_stem, wordnet_lemma))\n",
    "            \n",
    "            table += f\"{snowball_stem:<15} | {wordnet_lemma:<15}\\n\"\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if count == NW:\n",
    "                break\n",
    "\n",
    "# Print the table\n",
    "print(table)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:11.092838900Z",
     "start_time": "2024-03-25T14:19:11.052297400Z"
    }
   },
   "id": "b83493c92c97037a",
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11. Print the first N most frequent lemmas (after the removal of stopwords) together with their number of appearances."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4410da1f5d9b5f7a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent lemmas (after removing stopwords):\n",
      "elizabeth: 643\n",
      "could: 530\n",
      "would: 482\n",
      "darcy: 424\n",
      "said: 406\n",
      "bennet: 346\n",
      "much: 333\n",
      "must: 321\n",
      "miss: 315\n",
      "bingley: 307\n",
      "jane: 302\n",
      "one: 293\n",
      "sister: 288\n",
      "lady: 279\n",
      "know: 248\n",
      "though: 238\n",
      "never: 228\n",
      "think: 222\n",
      "time: 219\n",
      "may: 216\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "words = nltk.word_tokenize(text_without_header)\n",
    "\n",
    "# Remove punctuation from the words and convert to lowercase\n",
    "words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words_without_stopwords = [word for word in words if word not in stop_words]\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = [wordnet_lemmatizer.lemmatize(word) for word in words_without_stopwords]\n",
    "\n",
    "lemma_freq = Counter(lemmas)\n",
    "\n",
    "sorted_lemma_freq = sorted(lemma_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "N = 20\n",
    "\n",
    "print(f\"Top {N} most frequent lemmas (after removing stopwords):\")\n",
    "for lemma, freq in sorted_lemma_freq[:N]:\n",
    "    print(f\"{lemma}: {freq}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:11.620236700Z",
     "start_time": "2024-03-25T14:19:11.067329700Z"
    }
   },
   "id": "2ff09b9f275d0e6b",
   "execution_count": 101
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 12. Change all the numbers from lws into words. Print the number of changes, and also the portion of list that contains first N changes (for example N=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da582f5bb4b66da"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of changes: 161\n",
      "\n",
      "First 10 changes:\n",
      "156 -> one hundred and fifty-six\n",
      "1894 -> one thousand, eight hundred and ninety-four\n",
      "156 -> one hundred and fifty-six\n",
      "1796 -> one thousand, seven hundred and ninety-six\n",
      "1813 -> one thousand, eight hundred and thirteen\n",
      "1 -> one\n",
      "2 -> two\n",
      "5 -> five\n",
      "6 -> six\n",
      "9 -> nine\n",
      "\n",
      "First 50 elements of lws after changes:\n",
      "['one hundred and fifty-six', 'charing', 'cross', 'road', 'london', 'ruskin', 'house', 'illustration', 'jane', 'pride', 'prejudice', 'jane', 'austen', 'preface', 'george', 'saintsbury', 'illustrations', 'hugh', 'thomson', 'illustration', 'one thousand, eight hundred and ninety-four', 'ruskin', 'one hundred and fifty-six', 'charing', 'house', 'cross', 'road', 'london', 'george', 'allen', 'chiswick', 'press', 'charles', 'whittingham', 'tooks', 'court', 'chancery', 'lane', 'london', 'illustration', 'comyns', 'carr', 'acknowledgment', 'owe', 'friendship', 'advice', 'illustrations', 'gratefully', 'preface', 'illustration']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "changes_count = 0\n",
    "\n",
    "N = 10\n",
    "first_N_changes = []\n",
    "\n",
    "min_index = sys.maxsize\n",
    "max_index = 0\n",
    "\n",
    "for i, word in enumerate(lws):\n",
    "    if word.isdigit(): \n",
    "        # Convert number to word representation\n",
    "        word_in_words = p.number_to_words(word)\n",
    "        # Replace number with word representation\n",
    "        lws[i] = word_in_words\n",
    "        \n",
    "        changes_count += 1\n",
    "        if len(first_N_changes) < N:\n",
    "            if i < min_index:\n",
    "                min_index = i\n",
    "            if i > max_index:\n",
    "                max_index = i\n",
    "            \n",
    "            first_N_changes.append((word, word_in_words))\n",
    "\n",
    "print(\"Number of changes:\", changes_count)\n",
    "\n",
    "print(f\"\\nFirst {N} changes:\")\n",
    "for number, word in first_N_changes:\n",
    "    print(f\"{number} -> {word}\")\n",
    "\n",
    "print(\"\\nFirst 50 elements of lws after changes:\")\n",
    "print(lws[min_index:max_index][:50])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:19:11.647750500Z",
     "start_time": "2024-03-25T14:19:11.631244700Z"
    }
   },
   "id": "4ccf76292a136bd7",
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13. Create a function that receives an integer N and a word W as parameter (it can also receive the list of words from the text). We want to print the concordance data for that word. This means printing the window of text (words on consecutive positions) of length N, that has the givend word W in the middle. For example, for the text \"\"I have two dogs and a cat. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat.\" and a window of length 3, the concordance data for the word \"cat\" would be [\"dogs\", \"cat\", \"pets\"] and [\"pets\",\"cat\", \"likes\"] (we consider the text without stopwords and punctuation). However, as you can see, the window of text may contain words from different sentences. Create a second function that prints windows of texts that contain words only from the phrase containing word W. We want to print concordance data for all the inflexions of word W."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68c4301a44e5649a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance Data (All Sentences):\n",
      "['a', 'cat', 'do']\n",
      "['my', 'cat', 'likes']\n",
      "['my', 'cat']\n",
      "\n",
      "Concordance Data (Same Sentence):\n",
      "['a', 'cat']\n",
      "['my', 'cat', 'likes']\n",
      "['my', 'cat']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def concordance_data_all_sentences(text, N, W):\n",
    "    words = [word.lower() for word in word_tokenize(text) if word.isalnum()]\n",
    "    for i, word in enumerate(words):\n",
    "        if word == W:\n",
    "            start_index = max(0, i - N)\n",
    "            end_index = min(len(words), i + N + 1)\n",
    "            window = words[start_index:end_index]\n",
    "            print(window)\n",
    "\n",
    "def concordance_data_same_sentence(text, N, W):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = [word.lower() for word in word_tokenize(sentence) if word.isalnum()]\n",
    "        for i, word in enumerate(words):\n",
    "            if word == W:\n",
    "                start_index = max(0, i - N)\n",
    "                end_index = min(len(words), i + N + 1)\n",
    "                window = words[start_index:end_index]\n",
    "                print(window)\n",
    "\n",
    "# Example usage:\n",
    "text = \"I have two dogs and a cat. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat.\"\n",
    "N = 1\n",
    "W = \"cat\"\n",
    "\n",
    "print(\"Concordance Data (All Sentences):\")\n",
    "concordance_data_all_sentences(text, N, W)\n",
    "\n",
    "print(\"\\nConcordance Data (Same Sentence):\")\n",
    "concordance_data_same_sentence(text, N, W)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:21:12.378447Z",
     "start_time": "2024-03-25T14:21:12.363436900Z"
    }
   },
   "id": "125f9813dc5d3351",
   "execution_count": 103
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
