{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:00.874108Z",
     "start_time": "2025-01-15T11:00:00.870580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "import unicodedata\n",
    "from sklearn import metrics\n",
    "import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch"
   ],
   "id": "39b2a9b7487b2cc",
   "outputs": [],
   "execution_count": 346
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:00.878643Z",
     "start_time": "2025-01-15T11:00:00.875619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "# from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "# from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import pandas as pd\n",
    "import os, random, copy, re\n",
    "from PIL import Image\n",
    "from PIL import ImageFile"
   ],
   "id": "ed63cce315648cd",
   "outputs": [],
   "execution_count": 347
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:00.892754Z",
     "start_time": "2025-01-15T11:00:00.887198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomDatasetFixed(Dataset):\n",
    "    def __init__(self, data_df, phase, img_transform, preprocess, tokenize, max_length):\n",
    "        self.data_df = data_df\n",
    "        self.phase = phase\n",
    "        self.img_transform = img_transform\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenize = tokenize\n",
    "        self.max_len = max_length\n",
    "        self.dloc = 'dataset/'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_df.iloc[idx]\n",
    "        \n",
    "        img_id = row['image_id']\n",
    "        caption = row['caption']\n",
    "        id = row['id']\n",
    "        label = -1 if self.phase == 'test' else row['label']\n",
    "        \n",
    "        # Validate image path\n",
    "        img_path = os.path.join(self.dloc, self.phase+'_images', str(img_id)+'.jpg')\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.img_transform(img)\n",
    "\n",
    "        # Validate caption preprocessing and tokenization\n",
    "        proc_caption = self.preprocess(caption)\n",
    "        if proc_caption is None:\n",
    "            raise ValueError(f\"Preprocessing failed for caption: {caption}\")\n",
    "        caption_tokens, masks = self.tokenize(proc_caption)\n",
    "        if caption_tokens is None or masks is None:\n",
    "            raise ValueError(f\"Tokenization failed for processed caption: {proc_caption}\")\n",
    "        \n",
    "        return {\n",
    "            'images': img,\n",
    "            'caption_tokens': caption_tokens,\n",
    "            'ids': id,\n",
    "            'labels': label,\n",
    "        }"
   ],
   "id": "59ad333d9baecba2",
   "outputs": [],
   "execution_count": 348
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:00.905340Z",
     "start_time": "2025-01-15T11:00:00.901308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return text\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    sample = re.sub(r\"\\S+\\.[(net)|(com)|(org)]\\S+\", \"\", sample)\n",
    "    sample = re.sub(r\"http\\S+\", \"\", sample)\n",
    "    sample = re.sub(r\"\\d+\", \" \", sample)\n",
    "    sample = re.sub(r\"\\s+\", \" \", sample)\n",
    "    sample = re.sub(r\"_\", \" \", sample)\n",
    "    return sample\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    return words\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample = remove_URL(sample)\n",
    "    # Tokenize\n",
    "    words = sample.split(' ')\n",
    "    words = normalize(words)\n",
    "\n",
    "    normalized_text = ''\n",
    "    for w in words:\n",
    "        normalized_text += w+' '\n",
    "\n",
    "    return normalized_text.strip()"
   ],
   "id": "6ffbe82bd6349728",
   "outputs": [],
   "execution_count": 349
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:00.914462Z",
     "start_time": "2025-01-15T11:00:00.906862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MMNetwork(nn.Module):\n",
    "    def __init__(self, vdim, tdim):\n",
    "        super(MMNetwork, self).__init__()\n",
    "\n",
    "        ## Linear layer for ResNet features\n",
    "        self.vfc = nn.Linear(vdim, 256)\n",
    "\n",
    "        ## Single Layer Bi-directional RNN with GRU cells. Projects 768 to 512\n",
    "        self.bigru = nn.LSTM(tdim, hidden_size=256, num_layers=1, bidirectional=False, batch_first=True, bias=False)\n",
    "\n",
    "        ## Concatenated Image and Text goes through this multi-layer network\n",
    "        self.mfc1 = nn.Linear(512, 256)\n",
    "        # self.mfc2 = nn.Linear(512, 256)\n",
    "        # self.mfc3 = nn.Linear(256, 128)\n",
    "        # self.mfc4 = nn.Linear(256, 128)\n",
    "\n",
    "        self.cf = nn.Linear(256, 1)\n",
    "\n",
    "        self.act = nn.ReLU()  ## ReLU\n",
    "        self.vdp = nn.Dropout(0.2)\n",
    "        self.tdp = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, vx, tx):\n",
    "        # vx = self.vdp(self.vfc(vx))\n",
    "        vx = self.vdp(self.act(self.vfc(vx)))\n",
    "\n",
    "        tx = tx.unsqueeze(1)  # (batch_size=64, seq_len=1, feature_dim=768)\n",
    "        _, (hidden_tx, _) = self.bigru(tx)  # (num_layers, batch_size, hidden_size=256)\n",
    "\n",
    "        hidden_tx = hidden_tx.squeeze(0)  # (batch_size, hidden_size=256)\n",
    "\n",
    "        # hidden = [n layers * n directions, batch size, emb dim]\n",
    "        # hidden_tx = self.tdp(torch.cat((hidden_tx[0][-2,:,:], hidden_tx[0][-1,:,:]), dim = 1))\n",
    "        ## Concatenate Visual and Textual output\n",
    "        # mx = torch.cat((vx, hidden_tx), dim=1)\n",
    "        mx = torch.cat((vx, self.tdp(hidden_tx)), dim=1)\n",
    "\n",
    "        mx = self.act(self.mfc1(mx))\n",
    "        # mx = self.act(self.mfc2(mx))\n",
    "        # mx = self.relu(self.mfc3(mx))\n",
    "        # mx = self.relu(self.mfc4(mx))\n",
    "\n",
    "        return torch.sigmoid(self.cf(mx))"
   ],
   "id": "19b07e4611909475",
   "outputs": [],
   "execution_count": 350
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:00.917972Z",
     "start_time": "2025-01-15T11:00:00.914462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(text, context_length: int = 77):\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    # all_tokens = _tokenizer.encode(text)\n",
    "    tokens = [sot_token] + _tokenizer.encode(text)[:context_length - 2] + [eot_token]\n",
    "    result = torch.zeros(context_length, dtype=torch.long)\n",
    "    mask = torch.zeros(context_length, dtype=torch.long)\n",
    "    result[:len(tokens)] = torch.tensor(tokens)\n",
    "    mask[:len(tokens)] = 1\n",
    "\n",
    "    return result, mask\n",
    "\n",
    "\n",
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()"
   ],
   "id": "527a1419b1b6adfc",
   "outputs": [],
   "execution_count": 351
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:00.990766Z",
     "start_time": "2025-01-15T11:00:00.918985Z"
    }
   },
   "cell_type": "code",
   "source": "_tokenizer = _Tokenizer()\n",
   "id": "35d9c1d345c1b39",
   "outputs": [],
   "execution_count": 352
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:00.998361Z",
     "start_time": "2025-01-15T11:00:00.991274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, optimizer, lr_scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    best_val_loss = 100\n",
    "    best_epoch = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since2 = time.time()\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        tot = 0.0\n",
    "        cnt = 0\n",
    "        # Iterate over data.\n",
    "        for batch in tr_loader:\n",
    "\n",
    "            images = batch['images'].to(device)      # Images from batch\n",
    "            caption_tokens = batch['caption_tokens'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            with torch.no_grad():\n",
    "                img_feats = clip_model.module.encode_image(images)\n",
    "                txt_feats = clip_model.module.encode_text(caption_tokens)\n",
    "\n",
    "            outputs = model(img_feats, txt_feats).squeeze(1)\n",
    "            preds = (outputs > 0.5).int()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward + optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            tot += len(labels)\n",
    "\n",
    "            if cnt % 40 == 0:\n",
    "                print('[%d, %5d] loss: %.4f, Acc: %.2f' %\n",
    "                      (epoch, cnt + 1, loss.item(), (100.0 * running_corrects) / tot))\n",
    "\n",
    "            cnt = cnt + 1\n",
    "\n",
    "        if scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        train_loss = running_loss / len(tr_loader)\n",
    "        train_acc = running_corrects * 1.0 / (len(tr_loader.dataset))\n",
    "\n",
    "        print('Training Loss: {:.6f} Acc: {:.2f}'.format(train_loss, 100.0 * train_acc))\n",
    "\n",
    "        test_loss, test_acc, test_f1 = evaluate(model, vl_loader)\n",
    "\n",
    "        print(\n",
    "            'Epoch: {:d}, Val Loss: {:.4f}, Acc: {:.2f}, F1: {:.2f}'.format(\n",
    "                epoch, test_loss, test_acc * 100, test_f1 * 100))\n",
    "\n",
    "        # deep copy the model\n",
    "        if test_f1 >= best_f1:\n",
    "            best_acc = test_acc\n",
    "            best_val_loss = test_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "            best_f1 = test_f1\n",
    "\n",
    "    time_elapsed2 = time.time() - since2\n",
    "    print('Epoch complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed2 // 60, time_elapsed2 % 60))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return best_model, best_epoch"
   ],
   "id": "9f033dff1f928592",
   "outputs": [],
   "execution_count": 353
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:01.002426Z",
     "start_time": "2025-01-15T11:00:00.998870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tr_loader:\n",
    "\n",
    "            images = batch['images'].to(device)      # Images from batch\n",
    "            caption_tokens = batch['caption_tokens'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "\n",
    "\n",
    "            img_feats = clip_model.module.encode_image(images)\n",
    "            txt_feats = clip_model.module.encode_text(caption_tokens)\n",
    "\n",
    "            outputs = model(img_feats, txt_feats).squeeze(1)\n",
    "\n",
    "            preds1 = (outputs > 0.5).int()\n",
    "\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            all_preds.extend(preds1.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "        acc = metrics.accuracy_score(all_labels, all_preds)\n",
    "        f1 = metrics.f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return test_loss / len(loader), acc, f1"
   ],
   "id": "33d7397efb4bba6f",
   "outputs": [],
   "execution_count": 354
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:01.009028Z",
     "start_time": "2025-01-15T11:00:01.002933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "15cda6db9a7ded65",
   "outputs": [],
   "execution_count": 355
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:02.484049Z",
     "start_time": "2025-01-15T11:00:01.009028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parser = argparse.ArgumentParser(description='Train Multimodal Multi-task model for Misogyny Detection')\n",
    "parser.add_argument('--bs', type=int, default=32, help='64,128')\n",
    "parser.add_argument('--epochs', type=int, default=20)\n",
    "parser.add_argument('--maxlen', type=int, default=60)\n",
    "parser.add_argument('--lr', type=str, default='1e-4', help='3e-5, 4e-5, 5e-5, 5e-4')\n",
    "parser.add_argument('--vmodel', type=str, default='rn50', help='resnet | vit32 | vit16 | vit14 | rn50 | rn101 | rn504 | rn5016 | rn5064')\n",
    "\n",
    "\n",
    "argv = sys.argv\n",
    "if '-f' in argv:\n",
    "    argv = argv[:argv.index('-f')]  # Exclude Jupyter's `-f` argument and everything after it\n",
    "args = parser.parse_args(argv[1:])\n",
    "## Arguments\n",
    "batch_size = args.bs\n",
    "init_lr = float(args.lr)\n",
    "epochs = args.epochs\n",
    "vmodel = args.vmodel\n",
    "\n",
    "## Pre-trained Stream Models\n",
    "clip_nms = {'vit32': 'ViT-B/32', 'vit16': 'ViT-B/16', 'rn50': 'RN50', 'rn504': 'RN50x4', 'rn101': 'RN101',\n",
    "            'rn5016': 'RN50x16', 'rn5064': 'RN50x64', 'vit14': 'ViT-L/14'}\n",
    "clip_dim = {'vit32': 512, 'vit16': 512, 'vit14': 768, 'rn50': 1024, 'rn504': 640, 'rn101': 512, 'rn5016': 768,\n",
    "            'rn5064': 1024}\n",
    "clip_model, _ = clip.load(clip_nms[vmodel], jit=False)\n",
    "input_resolution = clip_model.visual.input_resolution\n",
    "clip_model.float().eval()\n",
    "clip_model = nn.DataParallel(clip_model)\n",
    "\n",
    "dim = clip_dim[vmodel]\n",
    "\n",
    "## Transforms\n",
    "transform_config = {'train': transforms.Compose([\n",
    "    transforms.RandomResizedCrop(input_resolution, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.2),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    # transforms.RandomPerspective(),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                         std=[0.26862954, 0.26130258, 0.27577711])\n",
    "]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((input_resolution, input_resolution), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        # transforms.CenterCrop(clip_model.visual.input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                             std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ])\n",
    "}\n",
    "\n",
    "## Dataset\n",
    "train_df = pd.read_csv(\"./dataset/train.csv\")\n",
    "val_df = pd.read_csv(\"./dataset/val.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test.csv\")\n",
    "if args.maxlen != 0:\n",
    "    max_length = args.maxlen"
   ],
   "id": "dd08f392ec790f56",
   "outputs": [],
   "execution_count": 356
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:02.488648Z",
     "start_time": "2025-01-15T11:00:02.484049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tr_data = CustomDatasetFixed(train_df, 'train', transform_config['train'], preprocess, tokenize, max_length)\n",
    "vl_data = CustomDatasetFixed(val_df, 'val', transform_config['test'], preprocess, tokenize, max_length)\n",
    "ts_data = CustomDatasetFixed(test_df, 'test', transform_config['test'], preprocess, tokenize, max_length)\n",
    "tr_loader = DataLoader(tr_data, shuffle=True, num_workers=0, batch_size=batch_size)\n",
    "vl_loader = DataLoader(vl_data, num_workers=0, batch_size=batch_size)\n",
    "ts_loader = DataLoader(ts_data, num_workers=0, batch_size=batch_size)"
   ],
   "id": "80565b204531c3d5",
   "outputs": [],
   "execution_count": 357
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:00:02.491691Z",
     "start_time": "2025-01-15T11:00:02.489157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from torch.nn import BCELoss\n",
    "# \n",
    "# # Test model code\n",
    "# #  , num_heads, transformer_hidden_dim, num_transformer_layers\n",
    "# model = MMNetwork(dim, dim).to(device)\n",
    "# criterion = BCELoss()\n",
    "# # Dummy input data\n",
    "# images = torch.randn(16, 3, 224, 224).long().to(device)  # Batch of 16 RGB images of size 224x224\n",
    "# captions = torch.randint(0, 1000, (16, max_length)).long().to(device)  # Batch of 16 captions with max_len tokens each\n",
    "# labels = torch.rand(16).float().to(device)\n",
    "# with torch.no_grad():\n",
    "#     img_feats = clip_model.module.encode_image(images)\n",
    "#     txt_feats = clip_model.module.encode_text(captions)\n",
    "# \n",
    "# output = model(img_feats, txt_feats).squeeze(1)\n",
    "# output = (output > 0.5).float()\n",
    "# loss = criterion(output, labels)\n",
    "# print(output, loss)  # Should be (16) "
   ],
   "id": "dd7a0b6229d54a44",
   "outputs": [],
   "execution_count": 358
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:16:08.889378Z",
     "start_time": "2025-01-15T11:00:02.492203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Model\n",
    "model = MMNetwork(dim, dim)\n",
    "\n",
    "model.to(device)\n",
    "# print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), init_lr, betas=(0.99, 0.98), weight_decay=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "num_train_steps = int(len(tr_data) / batch_size) * epochs\n",
    "num_warmup_steps = int(0.1 * num_train_steps)\n",
    "# print(num_train_steps, num_warmup_steps)  ## Print Number of total and warmup steps\n",
    "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [5, 10, 15], gamma=0.5)\n",
    "# scheduler = None\n",
    "\n",
    "model_ft, best_epoch = train(model, optimizer, scheduler, num_epochs=epochs)\n",
    "\n",
    "torch.save(model_ft.state_dict(), 'best_models/trained_model.pt')\n",
    "\n",
    "vl_loss, vl_acc, vl_f1 = evaluate(model_ft, vl_loader)\n",
    "print('Validation best epoch: %d, Val Loss: %.4f, ACC: %.2f, F1: %.2f' % (\n",
    "best_epoch, np.round(vl_loss, 4), vl_acc * 100, vl_f1 * 100))\n",
    "\n",
    "# ts_loss, ts_acc, ts_f1= evaluate(model_ft, ts_loader)\n",
    "# print('Test results:, Test Loss: %.4f, ACC: %.2f, F1: %.2f' % (np.round(ts_loss, 4), ts_acc * 100, ts_f1 * 100))"
   ],
   "id": "f8a23ed69531e9f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n",
      "[1,     1] loss: 0.6947, Acc: 46.88\n",
      "[1,    41] loss: 0.6848, Acc: 57.55\n",
      "[1,    81] loss: 0.6975, Acc: 57.52\n",
      "[1,   121] loss: 0.6651, Acc: 58.73\n",
      "[1,   161] loss: 0.6935, Acc: 59.69\n",
      "[1,   201] loss: 0.6427, Acc: 59.65\n",
      "[1,   241] loss: 0.6671, Acc: 60.10\n",
      "[1,   281] loss: 0.5989, Acc: 60.39\n",
      "Training Loss: 0.669115 Acc: 60.70\n",
      "Epoch: 1, Val Loss: 2.1171, Acc: 64.66, F1_1: 64.63\n",
      "Epoch 2/20\n",
      "----------\n",
      "[2,     1] loss: 0.6811, Acc: 56.25\n",
      "[2,    41] loss: 0.6323, Acc: 65.62\n",
      "[2,    81] loss: 0.7334, Acc: 65.97\n",
      "[2,   121] loss: 0.6672, Acc: 65.99\n",
      "[2,   161] loss: 0.6859, Acc: 66.36\n",
      "[2,   201] loss: 0.6337, Acc: 66.60\n",
      "[2,   241] loss: 0.6432, Acc: 66.38\n",
      "[2,   281] loss: 0.6047, Acc: 66.27\n",
      "Training Loss: 0.618972 Acc: 66.28\n",
      "Epoch: 2, Val Loss: 1.9890, Acc: 68.56, F1_1: 68.56\n",
      "Epoch 3/20\n",
      "----------\n",
      "[3,     1] loss: 0.5910, Acc: 65.62\n",
      "[3,    41] loss: 0.5544, Acc: 68.90\n",
      "[3,    81] loss: 0.5681, Acc: 69.56\n",
      "[3,   121] loss: 0.5151, Acc: 69.37\n",
      "[3,   161] loss: 0.6236, Acc: 69.00\n",
      "[3,   201] loss: 0.6260, Acc: 69.05\n",
      "[3,   241] loss: 0.5329, Acc: 69.27\n",
      "[3,   281] loss: 0.6014, Acc: 68.94\n",
      "Training Loss: 0.591725 Acc: 68.83\n",
      "Epoch: 3, Val Loss: 1.9182, Acc: 70.14, F1_1: 70.14\n",
      "Epoch 4/20\n",
      "----------\n",
      "[4,     1] loss: 0.4686, Acc: 84.38\n",
      "[4,    41] loss: 0.5425, Acc: 70.66\n",
      "[4,    81] loss: 0.6205, Acc: 70.56\n",
      "[4,   121] loss: 0.5878, Acc: 69.86\n",
      "[4,   161] loss: 0.5750, Acc: 69.78\n",
      "[4,   201] loss: 0.6312, Acc: 69.54\n",
      "[4,   241] loss: 0.5534, Acc: 69.59\n",
      "[4,   281] loss: 0.6801, Acc: 69.52\n",
      "Training Loss: 0.577149 Acc: 69.76\n",
      "Epoch: 4, Val Loss: 1.8661, Acc: 71.11, F1_1: 71.11\n",
      "Epoch 5/20\n",
      "----------\n",
      "[5,     1] loss: 0.5691, Acc: 75.00\n",
      "[5,    41] loss: 0.5559, Acc: 68.75\n",
      "[5,    81] loss: 0.5592, Acc: 70.33\n",
      "[5,   121] loss: 0.4735, Acc: 70.38\n",
      "[5,   161] loss: 0.6422, Acc: 70.36\n",
      "[5,   201] loss: 0.6181, Acc: 70.18\n",
      "[5,   241] loss: 0.5210, Acc: 70.50\n",
      "[5,   281] loss: 0.4318, Acc: 70.77\n",
      "Training Loss: 0.558975 Acc: 70.88\n",
      "Epoch: 5, Val Loss: 1.8075, Acc: 72.22, F1_1: 72.17\n",
      "Epoch 6/20\n",
      "----------\n",
      "[6,     1] loss: 0.4772, Acc: 78.12\n",
      "[6,    41] loss: 0.5286, Acc: 74.62\n",
      "[6,    81] loss: 0.4584, Acc: 73.42\n",
      "[6,   121] loss: 0.6228, Acc: 72.91\n",
      "[6,   161] loss: 0.5213, Acc: 72.48\n",
      "[6,   201] loss: 0.5777, Acc: 72.79\n",
      "[6,   241] loss: 0.5482, Acc: 72.56\n",
      "[6,   281] loss: 0.5377, Acc: 72.52\n",
      "Training Loss: 0.544304 Acc: 72.46\n",
      "Epoch: 6, Val Loss: 1.7777, Acc: 73.39, F1_1: 73.39\n",
      "Epoch 7/20\n",
      "----------\n",
      "[7,     1] loss: 0.5117, Acc: 75.00\n",
      "[7,    41] loss: 0.5018, Acc: 73.55\n",
      "[7,    81] loss: 0.4755, Acc: 73.69\n",
      "[7,   121] loss: 0.4515, Acc: 73.84\n",
      "[7,   161] loss: 0.5119, Acc: 73.91\n",
      "[7,   201] loss: 0.6389, Acc: 73.63\n",
      "[7,   241] loss: 0.5758, Acc: 73.18\n",
      "[7,   281] loss: 0.5053, Acc: 73.19\n",
      "Training Loss: 0.534042 Acc: 73.36\n",
      "Epoch: 7, Val Loss: 1.7368, Acc: 74.72, F1_1: 74.72\n",
      "Epoch 8/20\n",
      "----------\n",
      "[8,     1] loss: 0.6063, Acc: 65.62\n",
      "[8,    41] loss: 0.6687, Acc: 73.55\n",
      "[8,    81] loss: 0.3961, Acc: 74.92\n",
      "[8,   121] loss: 0.3813, Acc: 74.66\n",
      "[8,   161] loss: 0.4076, Acc: 74.34\n",
      "[8,   201] loss: 0.5451, Acc: 74.42\n",
      "[8,   241] loss: 0.6398, Acc: 74.16\n",
      "[8,   281] loss: 0.5004, Acc: 74.44\n",
      "Training Loss: 0.519250 Acc: 74.59\n",
      "Epoch: 8, Val Loss: 1.6892, Acc: 76.01, F1_1: 75.95\n",
      "Epoch 9/20\n",
      "----------\n",
      "[9,     1] loss: 0.4794, Acc: 84.38\n",
      "[9,    41] loss: 0.4474, Acc: 77.59\n",
      "[9,    81] loss: 0.5186, Acc: 76.81\n",
      "[9,   121] loss: 0.5607, Acc: 76.29\n",
      "[9,   161] loss: 0.5519, Acc: 75.74\n",
      "[9,   201] loss: 0.4519, Acc: 75.79\n",
      "[9,   241] loss: 0.6269, Acc: 75.77\n",
      "[9,   281] loss: 0.5581, Acc: 76.12\n",
      "Training Loss: 0.500423 Acc: 76.34\n",
      "Epoch: 9, Val Loss: 1.6124, Acc: 77.35, F1_1: 77.35\n",
      "Epoch 10/20\n",
      "----------\n",
      "[10,     1] loss: 0.5208, Acc: 75.00\n",
      "[10,    41] loss: 0.4647, Acc: 76.68\n",
      "[10,    81] loss: 0.4615, Acc: 77.35\n",
      "[10,   121] loss: 0.5706, Acc: 76.78\n",
      "[10,   161] loss: 0.4656, Acc: 76.84\n",
      "[10,   201] loss: 0.4597, Acc: 77.16\n",
      "[10,   241] loss: 0.3702, Acc: 77.42\n",
      "[10,   281] loss: 0.5291, Acc: 77.39\n",
      "Training Loss: 0.483365 Acc: 77.39\n",
      "Epoch: 10, Val Loss: 1.5490, Acc: 79.01, F1_1: 79.00\n",
      "Epoch 11/20\n",
      "----------\n",
      "[11,     1] loss: 0.4406, Acc: 78.12\n",
      "[11,    41] loss: 0.4952, Acc: 77.90\n",
      "[11,    81] loss: 0.5047, Acc: 78.70\n",
      "[11,   121] loss: 0.4884, Acc: 78.51\n",
      "[11,   161] loss: 0.3821, Acc: 78.49\n",
      "[11,   201] loss: 0.5800, Acc: 78.58\n",
      "[11,   241] loss: 0.4103, Acc: 78.55\n",
      "[11,   281] loss: 0.4325, Acc: 78.87\n",
      "Training Loss: 0.466226 Acc: 78.91\n",
      "Epoch: 11, Val Loss: 1.5075, Acc: 79.79, F1_1: 79.78\n",
      "Epoch 12/20\n",
      "----------\n",
      "[12,     1] loss: 0.6527, Acc: 68.75\n",
      "[12,    41] loss: 0.4300, Acc: 78.35\n",
      "[12,    81] loss: 0.5425, Acc: 79.09\n",
      "[12,   121] loss: 0.3653, Acc: 79.88\n",
      "[12,   161] loss: 0.3314, Acc: 80.01\n",
      "[12,   201] loss: 0.4021, Acc: 79.77\n",
      "[12,   241] loss: 0.4648, Acc: 79.90\n",
      "[12,   281] loss: 0.3356, Acc: 79.87\n",
      "Training Loss: 0.454103 Acc: 79.84\n",
      "Epoch: 12, Val Loss: 1.4797, Acc: 80.21, F1_1: 80.20\n",
      "Epoch 13/20\n",
      "----------\n",
      "[13,     1] loss: 0.3444, Acc: 90.62\n",
      "[13,    41] loss: 0.4346, Acc: 81.40\n",
      "[13,    81] loss: 0.5009, Acc: 81.17\n",
      "[13,   121] loss: 0.5376, Acc: 80.76\n",
      "[13,   161] loss: 0.3944, Acc: 80.40\n",
      "[13,   201] loss: 0.5803, Acc: 80.01\n",
      "[13,   241] loss: 0.4563, Acc: 80.16\n",
      "[13,   281] loss: 0.3482, Acc: 80.13\n",
      "Training Loss: 0.446544 Acc: 80.17\n",
      "Epoch: 13, Val Loss: 1.4460, Acc: 81.05, F1_1: 81.04\n",
      "Epoch 14/20\n",
      "----------\n",
      "[14,     1] loss: 0.4257, Acc: 84.38\n",
      "[14,    41] loss: 0.3849, Acc: 81.55\n",
      "[14,    81] loss: 0.4510, Acc: 80.86\n",
      "[14,   121] loss: 0.4173, Acc: 81.46\n",
      "[14,   161] loss: 0.5527, Acc: 80.86\n",
      "[14,   201] loss: 0.5497, Acc: 80.75\n",
      "[14,   241] loss: 0.3752, Acc: 80.60\n",
      "[14,   281] loss: 0.5020, Acc: 80.54\n",
      "Training Loss: 0.441181 Acc: 80.55\n",
      "Epoch: 14, Val Loss: 1.4308, Acc: 81.05, F1_1: 81.02\n",
      "Epoch 15/20\n",
      "----------\n",
      "[15,     1] loss: 0.4834, Acc: 78.12\n",
      "[15,    41] loss: 0.3830, Acc: 82.09\n",
      "[15,    81] loss: 0.3281, Acc: 80.56\n",
      "[15,   121] loss: 0.5190, Acc: 79.86\n",
      "[15,   161] loss: 0.4604, Acc: 80.10\n",
      "[15,   201] loss: 0.3718, Acc: 80.35\n",
      "[15,   241] loss: 0.4440, Acc: 80.59\n",
      "[15,   281] loss: 0.5101, Acc: 80.68\n",
      "Training Loss: 0.434105 Acc: 80.88\n",
      "Epoch: 15, Val Loss: 1.4143, Acc: 81.36, F1_1: 81.35\n",
      "Epoch 16/20\n",
      "----------\n",
      "[16,     1] loss: 0.3404, Acc: 87.50\n",
      "[16,    41] loss: 0.5244, Acc: 79.80\n",
      "[16,    81] loss: 0.2884, Acc: 81.06\n",
      "[16,   121] loss: 0.3851, Acc: 81.56\n",
      "[16,   161] loss: 0.4119, Acc: 81.46\n",
      "[16,   201] loss: 0.4615, Acc: 81.08\n",
      "[16,   241] loss: 0.4197, Acc: 81.04\n",
      "[16,   281] loss: 0.3970, Acc: 80.97\n",
      "Training Loss: 0.423553 Acc: 81.03\n",
      "Epoch: 16, Val Loss: 1.4015, Acc: 81.47, F1_1: 81.46\n",
      "Epoch 17/20\n",
      "----------\n",
      "[17,     1] loss: 0.3487, Acc: 90.62\n",
      "[17,    41] loss: 0.3865, Acc: 83.16\n",
      "[17,    81] loss: 0.3265, Acc: 82.95\n",
      "[17,   121] loss: 0.3750, Acc: 81.71\n",
      "[17,   161] loss: 0.4269, Acc: 81.46\n",
      "[17,   201] loss: 0.5548, Acc: 81.33\n",
      "[17,   241] loss: 0.3419, Acc: 81.35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[359], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mMultiStepLR(optimizer, [\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m15\u001B[39m], gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# scheduler = None\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m model_ft, best_epoch \u001B[38;5;241m=\u001B[39m train(model, optimizer, scheduler, num_epochs\u001B[38;5;241m=\u001B[39mepochs)\n\u001B[0;32m     19\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model_ft\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_models/trained_model.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     21\u001B[0m vl_loss, vl_acc, vl_f1 \u001B[38;5;241m=\u001B[39m evaluate(model_ft, vl_loader)\n",
      "Cell \u001B[1;32mIn[353], line 24\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, optimizer, lr_scheduler, num_epochs)\u001B[0m\n\u001B[0;32m     22\u001B[0m cnt \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Iterate over data.\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tr_loader:\n\u001B[0;32m     26\u001B[0m     images \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)      \u001B[38;5;66;03m# Images from batch\u001B[39;00m\n\u001B[0;32m     27\u001B[0m     caption_tokens \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption_tokens\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Captions from batch\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[348], line 27\u001B[0m, in \u001B[0;36mCustomDatasetFixed.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage not found at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimg_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     26\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(img_path)\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 27\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_transform(img)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Validate caption preprocessing and tokenization\u001B[39;00m\n\u001B[0;32m     30\u001B[0m proc_caption \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(caption)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mto_tensor(pic)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    172\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mview(pic\u001B[38;5;241m.\u001B[39msize[\u001B[38;5;241m1\u001B[39m], pic\u001B[38;5;241m.\u001B[39msize[\u001B[38;5;241m0\u001B[39m], F_pil\u001B[38;5;241m.\u001B[39mget_image_num_channels(pic))\n\u001B[0;32m    173\u001B[0m \u001B[38;5;66;03m# put it from HWC to CHW format\u001B[39;00m\n\u001B[1;32m--> 174\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mByteTensor):\n\u001B[0;32m    176\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\u001B[38;5;241m.\u001B[39mto(dtype\u001B[38;5;241m=\u001B[39mdefault_float_dtype)\u001B[38;5;241m.\u001B[39mdiv(\u001B[38;5;241m255\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 359
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ids = []\n",
    "predictions = []\n",
    "\n",
    "def make_submission(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch['images'].to(device)      # Images from batch\n",
    "            caption_tokens = batch['caption_tokens'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "            id = batch['ids']\n",
    "\n",
    "            img_feats = clip_model.module.encode_image(images)\n",
    "            txt_feats = clip_model.module.encode_text(caption_tokens)\n",
    "\n",
    "            outputs = model(img_feats, txt_feats).squeeze(1)\n",
    "\n",
    "            preds = (outputs > 0.5).int()\n",
    "            \n",
    "            ids.extend(id)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            \n",
    "    df = pd.DataFrame({'id': ids, 'label': predictions})\n",
    "    df.to_csv('submission_final2.csv', index=False)"
   ],
   "id": "fe8b4354eb55b6ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "make_submission(model, ts_loader)",
   "id": "caaf256fdea4c2c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8d37929af625287d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
