{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.156523Z",
     "start_time": "2024-12-02T10:53:08.339385Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from torchvision import transforms\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Disable UserWarnings\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.182293Z",
     "start_time": "2024-12-02T10:53:14.157027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"./dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test.csv\")\n",
    "validation_df = pd.read_csv(\"./dataset/val.csv\")"
   ],
   "id": "68a1a9efa01eeb80",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.193978Z",
     "start_time": "2024-12-02T10:53:14.182293Z"
    }
   },
   "cell_type": "code",
   "source": "train_df",
   "id": "ea75fd0d701ae033",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        id  \\\n",
       "0     417812c5-0ce4-499d-b97d-4d28827239bc   \n",
       "1     5ac91fa3-55f2-4cb3-8c8f-ad84f78e6b36   \n",
       "2     d2705b90-8347-4cab-a7a6-654540d9a489   \n",
       "3     a3b33fe7-3085-4433-9c18-8814803891b4   \n",
       "4     1514b0e4-0665-45bc-ab32-52fce326cc29   \n",
       "...                                    ...   \n",
       "9995  1d1df243-485d-4b29-82c8-7e34c0de1f5c   \n",
       "9996  f7dfa883-e524-4974-b5ba-6b3c3db49087   \n",
       "9997  602e83dc-6539-4c1a-8d19-c1481b5c24bf   \n",
       "9998  d9ce2e8c-0831-466a-8756-4c40d772b1ce   \n",
       "9999  b22cdca0-79b3-4b37-a173-5176a32096f6   \n",
       "\n",
       "                                                caption  image_id  label  \n",
       "0     Wet elephants shake water onto people bathing ...    394330      0  \n",
       "1          Two men holding tennis racquets on the court    130849      0  \n",
       "2     A bird on a tree limb with mountains in the ba...    514790      0  \n",
       "3     A kitchen and dining room are featured along w...    182096      0  \n",
       "4        A fruit stand has various fruits on the table.     68788      1  \n",
       "...                                                 ...       ...    ...  \n",
       "9995     Several people stand in a field flying a kite.    522702      0  \n",
       "9996       A batter hitting a pitch at a baseball game.    441874      1  \n",
       "9997  A person on white surfboard next to group in a...    166716      0  \n",
       "9998  A baseball player getting ready to swing at th...    517601      0  \n",
       "9999                    a black cat is laying in a sink    394115      1  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417812c5-0ce4-499d-b97d-4d28827239bc</td>\n",
       "      <td>Wet elephants shake water onto people bathing ...</td>\n",
       "      <td>394330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ac91fa3-55f2-4cb3-8c8f-ad84f78e6b36</td>\n",
       "      <td>Two men holding tennis racquets on the court</td>\n",
       "      <td>130849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d2705b90-8347-4cab-a7a6-654540d9a489</td>\n",
       "      <td>A bird on a tree limb with mountains in the ba...</td>\n",
       "      <td>514790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a3b33fe7-3085-4433-9c18-8814803891b4</td>\n",
       "      <td>A kitchen and dining room are featured along w...</td>\n",
       "      <td>182096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514b0e4-0665-45bc-ab32-52fce326cc29</td>\n",
       "      <td>A fruit stand has various fruits on the table.</td>\n",
       "      <td>68788</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1d1df243-485d-4b29-82c8-7e34c0de1f5c</td>\n",
       "      <td>Several people stand in a field flying a kite.</td>\n",
       "      <td>522702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>f7dfa883-e524-4974-b5ba-6b3c3db49087</td>\n",
       "      <td>A batter hitting a pitch at a baseball game.</td>\n",
       "      <td>441874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>602e83dc-6539-4c1a-8d19-c1481b5c24bf</td>\n",
       "      <td>A person on white surfboard next to group in a...</td>\n",
       "      <td>166716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>d9ce2e8c-0831-466a-8756-4c40d772b1ce</td>\n",
       "      <td>A baseball player getting ready to swing at th...</td>\n",
       "      <td>517601</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>b22cdca0-79b3-4b37-a173-5176a32096f6</td>\n",
       "      <td>a black cat is laying in a sink</td>\n",
       "      <td>394115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.199076Z",
     "start_time": "2024-12-02T10:53:14.194991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Compile the regular expression pattern\n",
    "pattern = re.compile(r\"[^a-z ]\")\n",
    "\n",
    "\n",
    "def text_preparetion(sentence):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z.\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence1 = \"A World War II-era bomber flying out of formation\"\n",
    "text_preparetion(sentence1)"
   ],
   "id": "f47e808e14e16d34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a world war ii era bomber flying out of formation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.238693Z",
     "start_time": "2024-12-02T10:53:14.200618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df['preprocessed_text'] = train_df['caption'].progress_apply(text_preparetion)\n",
    "validation_df['preprocessed_text'] = validation_df['caption'].progress_apply(text_preparetion)\n",
    "test_df['preprocessed_text'] = test_df['caption'].progress_apply(text_preparetion)"
   ],
   "id": "decdaddece6dce3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 578500.75it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 587657.01it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 396100.10it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.346717Z",
     "start_time": "2024-12-02T10:53:14.239706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_simple_vocab(sentences, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Manually create a vocabulary from a list of tokenized sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): List of sentences to build vocabulary from.\n",
    "        special_tokens (list of str): Special tokens like <pad>, <unk>.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A vocabulary mapping tokens to indices.\n",
    "        dict: An inverse vocabulary mapping indices to tokens.\n",
    "    \"\"\"\n",
    "    special_tokens = special_tokens or ['<pad>', '<unk>']\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Default index is the current vocab size\n",
    "    for token in special_tokens:\n",
    "        vocab[token]  # Add special tokens first\n",
    "    \n",
    "    # Add tokens from sentences\n",
    "    for sentence in sentences:\n",
    "        for token in sentence.split():\n",
    "            if token.strip():  # Exclude empty tokens\n",
    "                vocab[token]\n",
    "    \n",
    "    # Convert to a normal dict (no longer dynamic)\n",
    "    vocab = dict(vocab)\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    return vocab, inverse_vocab\n",
    "\n",
    "\n",
    "# Vectorize a sentence\n",
    "def vectorize_sentence(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a tensor of token indices using a given vocabulary,\n",
    "    ignoring empty tokens.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "        vocab (Vocab): Vocabulary to map tokens to indices.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Vectorized sentence as a tensor.\n",
    "    \"\"\"\n",
    "    # Ensure '<unk>' exists in the vocabulary\n",
    "    unk_idx = vocab.get('<unk>', -1)\n",
    "    if unk_idx == -1:\n",
    "        raise ValueError(\"The vocabulary must include '<unk>' for unknown tokens.\")\n",
    "\n",
    "    # Split sentence into tokens and map them to indices\n",
    "    tokens = [token for token in sentence.split() if token.strip()]\n",
    "    return torch.tensor([vocab.get(token, unk_idx) for token in tokens], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, images_path, train=True, max_len=None):\n",
    "        \"\"\"\n",
    "        Dataset for preprocessing image-text pairs.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing 'image_id', 'sentence', and optionally 'label'.\n",
    "            vocab (Vocab): Vocabulary for text vectorization.\n",
    "            train (bool): Whether this is a training dataset.\n",
    "            max_len (int): Maximum length for sentences. If None, no truncation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.train = train\n",
    "        self.max_len = max_len\n",
    "        self.images_path = images_path\n",
    "\n",
    "        # Define image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(100, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Process sentence\n",
    "        sentence = row['caption']\n",
    "        vectorized_sentence = vectorize_sentence(sentence, self.vocab)\n",
    "\n",
    "        # Pad or truncate the sentence\n",
    "        if len(vectorized_sentence) < self.max_len:\n",
    "            padding_length = self.max_len - len(vectorized_sentence)\n",
    "            pad_tensor = torch.full((padding_length,), self.vocab['<pad>'], dtype=torch.long)\n",
    "            vectorized_sentence = torch.cat((vectorized_sentence, pad_tensor), dim=0)\n",
    "        else:\n",
    "            vectorized_sentence = vectorized_sentence[:self.max_len]\n",
    "\n",
    "        # Process image\n",
    "        image_path = f\"{self.images_path}{row['image_id']}.jpg\"\n",
    "            \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
    "            image = self.image_transform(image)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "        # Handle labels (for training)\n",
    "        if self.train:\n",
    "            label = row['label']\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'labels': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'id': row['id']\n",
    "            }\n"
   ],
   "id": "b120e5be81dc41aa",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.378586Z",
     "start_time": "2024-12-02T10:53:14.347226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_sentence = [sentence.split(\" \") for sentence in train_df['preprocessed_text']]\n",
    "max_len = max(len([token for token in sentence.split(\" \")]) for sentence in train_df['preprocessed_text'])\n",
    "print(max_len)\n",
    "vocab, inverse_vocab = build_simple_vocab(train_df['preprocessed_text'])"
   ],
   "id": "434eac424e669b66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.394091Z",
     "start_time": "2024-12-02T10:53:14.380109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = PreprocessingDataset(train_df, vocab, train=True, max_len=max_len, images_path = \"./dataset/train_images/\")\n",
    "test_dataset = PreprocessingDataset(test_df, vocab, train=False, max_len=max_len, images_path = \"./dataset/test_images/\")\n",
    "val_dataset = PreprocessingDataset(validation_df, vocab, train=True, max_len=max_len, images_path = \"./dataset/val_images/\")"
   ],
   "id": "f6a9c5515a874c53",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.398630Z",
     "start_time": "2024-12-02T10:53:14.394603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "ef7e8ef05ced1727",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.466743Z",
     "start_time": "2024-12-02T10:53:14.400137Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "51c0d07bf84fb724",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.471830Z",
     "start_time": "2024-12-02T10:53:14.467252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a matrix of shape (max_len, embedding_dim) for positional encodings\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(float(max_len)) / embedding_dim))\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)  # Shape: (max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Sin for even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Cos for odd indices\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension: (1, max_len, embedding_dim)\n",
    "        self.register_buffer('pe', pe)  # Register as non-learnable buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to the input embeddings.\n",
    "        x: (Batch, SeqLen, EmbeddingDim)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :].to(x.device)"
   ],
   "id": "4ffdcc898e8cd83c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.512891Z",
     "start_time": "2024-12-02T10:53:14.472343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the parameters\n",
    "embedding_dim = 256  # Dimensionality of the embeddings\n",
    "max_len2 = 50        # Maximum sequence length\n",
    "batch_size = 16      # Batch size\n",
    "\n",
    "# Initialize the positional encoding module\n",
    "positional_encoding = PositionalEncoding(embedding_dim=embedding_dim, max_len=max_len2)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "# Shape: (Batch, SeqLen, EmbeddingDim)\n",
    "input_tensor = torch.randn(batch_size, max_len2, embedding_dim)\n",
    "\n",
    "# Apply positional encoding\n",
    "output_tensor = positional_encoding(input_tensor)\n",
    "\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Output Tensor Shape:\", output_tensor.shape)"
   ],
   "id": "619a5f494022c5bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([16, 50, 256])\n",
      "Output Tensor Shape: torch.Size([16, 50, 256])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.520500Z",
     "start_time": "2024-12-02T10:53:14.513399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImageTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, transformer_hidden_dim, num_transformer_layers, seq_len, cnn_dropout_value, transformer_dropout_value, num_classes=1):\n",
    "        super(ImageTextClassifier, self).__init__()\n",
    "        \n",
    "        # CNN for image feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # Flatten image features to (Batch, 128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Embedding for text\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim=embedding_dim, max_len=seq_len)\n",
    "        # self.rotary_positional_embeddings = torchtune.modules.RotaryPositionalEmbeddings(dim=embedding_dim // num_heads, max_seq_len=max_len)\n",
    "        \n",
    "        # Transformer for text feature extraction\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=transformer_hidden_dim,\n",
    "                dropout=transformer_dropout_value,\n",
    "                activation='relu'\n",
    "            ),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(128 + embedding_dim, 256)  # Combine image (128) + text (embedding_dim)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.dropout_fc = nn.Dropout(transformer_dropout_value)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        # Image feature extraction\n",
    "        img_features = self.cnn(images)  # (Batch, 128, 1, 1)\n",
    "        img_features = self.flatten(img_features)  # (Batch, 128)\n",
    "        \n",
    "        # Text feature extraction using Transformer\n",
    "        embedded_captions = self.embedding(captions)  # (Batch, SeqLen, EmbeddingDim)\n",
    "        # pos_encoded_captions = self.rotary_positional_embeddings(embedded_captions)  # Add positional encodings\n",
    "        pos_encoded_captions = self.positional_encoding(embedded_captions)\n",
    "        \n",
    "        # Transformer expects input shape (SeqLen, Batch, EmbeddingDim)\n",
    "        transformer_input = embedded_captions.permute(1, 0, 2)  # (SeqLen, Batch, EmbeddingDim)\n",
    "        transformer_output = self.transformer_encoder(transformer_input)  # (SeqLen, Batch, EmbeddingDim)\n",
    "        text_features = transformer_output.mean(dim=0)  # Mean pooling over SeqLen: (Batch, EmbeddingDim)\n",
    "        \n",
    "        # Combine image and text features\n",
    "        combined_features = torch.cat((img_features, text_features), dim=1)  # (Batch, 128 + EmbeddingDim)\n",
    "        \n",
    "        # Classification\n",
    "        x = F.relu(self.fc1(combined_features))  # (Batch, 256)\n",
    "        x = self.dropout_fc(x)  # Apply dropout\n",
    "        x = self.fc2(x)  # (Batch, 1 for binary classification)\n",
    "        return x.squeeze(1)  # Logits (not probabilities)\n",
    "    \n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight)"
   ],
   "id": "7c2f52a2b797846b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.524136Z",
     "start_time": "2024-12-02T10:53:14.521590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Test model code\n",
    "# model = ImageTextClassifier(\n",
    "#     vocab_size=len(vocab),\n",
    "#     embedding_dim=256,\n",
    "#     num_heads=8,\n",
    "#     transformer_hidden_dim=1024,\n",
    "#     num_transformer_layers=4,\n",
    "#     num_classes=1\n",
    "# )\n",
    "# \n",
    "# # Dummy input data\n",
    "# images = torch.randn(16, 3, 100, 100)  # Batch of 16 RGB images of size 224x224\n",
    "# captions = torch.randint(0, len(vocab), (16, max_len))  # Batch of 16 captions with max_len tokens each\n",
    "# \n",
    "# output = model(images, captions)\n",
    "# print(output.shape)  # Should be (16,) "
   ],
   "id": "61d242071b2bf57d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.732181Z",
     "start_time": "2024-12-02T10:53:14.524647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model_config = {\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"embedding_dim\": 128,\n",
    "    \"num_heads\": 2,\n",
    "    \"transformer_hidden_dim\": 256,\n",
    "    \"num_transformer_layers\": 2,\n",
    "    \"seq_len\": max_len,\n",
    "    \"cnn_dropout_value\": 0.4,\n",
    "    \"transformer_dropout_value\": 0.5,\n",
    "}\n",
    "model = ImageTextClassifier(**model_config)\n",
    "model.to(device)\n",
    "model.apply(initialize_weights)"
   ],
   "id": "b8e49cebdfb0724c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageTextClassifier(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Dropout(p=0.4, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): Dropout(p=0.4, inplace=False)\n",
       "    (14): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (embedding): Embedding(3733, 128)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.736239Z",
     "start_time": "2024-12-02T10:53:14.733201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCEWithLogitsLoss()  # Use logits directly\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    " # T_0 -> Number of iterations until the first restart\n",
    " # T_mult -> Double the cycle length after every restart\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, \n",
    "    T_0=5,  # Number of epochs before the first restart\n",
    "    T_mult=2,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=1e-7  # Minimum learning rate\n",
    ")"
   ],
   "id": "2fd4fe8235fc725d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.746915Z",
     "start_time": "2024-12-02T10:53:14.737257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "def training_method(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, patience=5, delta = 0.01, loss_procentage_improvement=10):\n",
    "    train_losses = []  # List to store training losses\n",
    "    val_losses = []  # List to store validation losses\n",
    "    val_accuracies = []  # List to store validation accuracies\n",
    "    val_precisions = []  # List to store validation precisions\n",
    "    val_recalls = []  # List to store validation recalls\n",
    "    val_f1s = []  # List to store validation F1-scores\n",
    "    learning_rates = [] # List to store learning rate progression\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    initial_loss = float('inf')\n",
    "    best_model = None  # Store the best model\n",
    "    epochs_without_improvement = 0  # Track epochs without improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        ### TRAINING\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images = batch['images'].to(device)      # Images from batch\n",
    "            captions = batch['captions'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            output = model(images, captions)  # Forward pass (logits)\n",
    "            loss = criterion(output, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            \n",
    "            optimizer.step()  # Update weights\n",
    "            training_loss += loss.item()  # Accumulate loss\n",
    "            \n",
    "        train_loss = training_loss / len(train_loader)  # Average training loss\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        ### VALIDATING\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        all_labels = []  # Ground truth labels for validation\n",
    "        all_preds = []   # Predictions for validation\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['images'].to(device)\n",
    "                captions = batch['captions'].to(device)\n",
    "                labels = batch['labels'].to(device).float()\n",
    "                \n",
    "                output = model(images, captions)  # Forward pass (logits)\n",
    "                loss = criterion(output, labels)  # Compute validation loss\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "                # Convert logits to probabilities and apply threshold\n",
    "                preds = (torch.sigmoid(output) > 0.5).float()\n",
    "                \n",
    "                # Store for statistics\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                \n",
    "        val_loss = validation_loss / len(val_loader)  # Average validation loss\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Compute validation statistics\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        val_recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        val_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        \n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        if epoch == 1:\n",
    "            initial_loss = val_loss\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)  # Save the best model\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            print(f\"New best model with Loss: {val_loss:.4f} at epoch {epoch + 1}\")\n",
    "        elif val_loss < best_val_loss - delta:\n",
    "            print(f\"Validation loss did not improve significantly\")            \n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Validation loss did not improve for {epochs_without_improvement} epoch(s).\")\n",
    "            # Stop training if validation loss does not improve for 'patience' epochs\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}. Best Loss: {best_val_loss:.4f}\")\n",
    "                break  # Exit training loop\n",
    "\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']  # Get the current learning rate\n",
    "        learning_rates.append(current_lr)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Training Loss: {train_loss:.4f} - \"\n",
    "              f\"Validation Loss: {val_loss:.4f} - \"\n",
    "              f\"Accuracy: {val_accuracy:.4f} - \"\n",
    "              f\"Precision: {val_precision:.4f} - \"\n",
    "              f\"Recall: {val_recall:.4f} - \"\n",
    "              f\"F1 Score: {val_f1:.4f} - \"\n",
    "              f\"Time: {end_time - start_time:.2f} - \"\n",
    "              f\"Lr: {current_lr:.2e}\")\n",
    "\n",
    "    print('Training finished!')\n",
    "    \n",
    "    # save the model only if the best loss is lower than the first initial loss ( to see that the model actually improved with 10% loss )\n",
    "    if best_val_loss < (100 - loss_procentage_improvement) * initial_loss:\n",
    "        # Init plot&model save path\n",
    "        plt_save_path = \"models/\"\n",
    "        model_config['eval_loss'] = best_val_loss\n",
    "        for key, value in model_config.items():\n",
    "            plt_save_path += key + \"=\" + str(value) + \"+\"\n",
    "        plt_save_path = plt_save_path[:-1] + \".png\"\n",
    "        model_path = plt_save_path[:-4] + \".pt\"\n",
    "        \n",
    "        torch.save(best_model.state_dict(), model_path)\n",
    "        print(f\"Best model with Loss: {best_val_loss:.4f} saved.\")\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        # Plotting the losses and validation metrics over epochs\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(val_accuracies, label='Accuracy')\n",
    "        plt.plot(val_precisions, label='Precision')\n",
    "        plt.plot(val_recalls, label='Recall')\n",
    "        plt.plot(val_f1s, label='F1 Score')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Metric')\n",
    "        plt.title('Validation Metrics')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(learning_rates, label='Learning Rate')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.title(\"Learning Rate Progression\")\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plt_save_path)\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(f\"Model wasn't saved because it didn't improve: {loss_procentage_improvement}%\")"
   ],
   "id": "6280acf86aa228a2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.749446Z",
     "start_time": "2024-12-02T10:53:14.747424Z"
    }
   },
   "cell_type": "code",
   "source": "# training_method(model, criterion, optimizer, scheduler, num_epochs=100, train_loader=train_dataloader, val_loader=val_dataloader)",
   "id": "ee2f8b5c78457e56",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.753003Z",
     "start_time": "2024-12-02T10:53:14.750469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = []\n",
    "predictions = []\n",
    "\n",
    "def make_submission(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            id = batch['id']\n",
    "            \n",
    "            output = model(images, captions)\n",
    "            preds = (torch.sigmoid(output) > 0.5).int()\n",
    "            \n",
    "            ids.extend(id)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "    df = pd.DataFrame({'id': ids, 'label': predictions})\n",
    "    df.to_csv('submission2.csv', index=False)"
   ],
   "id": "95402ff5c7357c82",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.756584Z",
     "start_time": "2024-12-02T10:53:14.753547Z"
    }
   },
   "cell_type": "code",
   "source": "# make_submission(model, test_dataloader)",
   "id": "6cc9a81e05125a1d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:14.762172Z",
     "start_time": "2024-12-02T10:53:14.757094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "\n",
    "def hyperparameter_tuning(vocab_size, max_len, train_loader, val_loader, param_grid, training_method, num_epochs=200):\n",
    "\n",
    "    # Create all combinations of hyperparameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    for params in tqdm(param_combinations):\n",
    "        print(f\"Testing configuration: {params}\")\n",
    "        \n",
    "        try:\n",
    "            # Update model configuration\n",
    "            model_config = {\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"embedding_dim\": params[\"embedding_dim\"],\n",
    "                \"num_heads\": params[\"num_heads\"],\n",
    "                \"transformer_hidden_dim\": params[\"transformer_hidden_dim\"],\n",
    "                \"num_transformer_layers\": params[\"num_transformer_layers\"],\n",
    "                \"seq_len\": max_len,\n",
    "                \"cnn_dropout_value\": params[\"cnn_dropout_value\"],\n",
    "                \"transformer_dropout_value\": params[\"transformer_dropout_value\"],\n",
    "            }\n",
    "            \n",
    "            # Initialize model\n",
    "            model = ImageTextClassifier(**model_config)\n",
    "            model.to(device)\n",
    "            model.apply(initialize_weights)\n",
    "            \n",
    "            # Define criterion, optimizer, and scheduler\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(), \n",
    "                lr=params[\"lr\"], \n",
    "                weight_decay=params[\"weight_decay\"]\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, \n",
    "                T_0=params[\"T_0\"]\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            training_method(\n",
    "                model, criterion, optimizer, scheduler,\n",
    "                num_epochs=num_epochs,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader\n",
    "            )\n",
    "            print(f\"Completed configuration: {params}\")\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error with configuration: {params}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "        \n",
    "        finally:\n",
    "            # Reset GPU memory\n",
    "            print(\"Resetting GPU memory...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()"
   ],
   "id": "cf6ef0fe03c73deb",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T10:53:26.598372Z",
     "start_time": "2024-12-02T10:53:26.594287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    \"embedding_dim\": [128, 256],\n",
    "    \"num_heads\": [2, 4],\n",
    "    \"transformer_hidden_dim\": [256, 512],\n",
    "    \"num_transformer_layers\": [2, 4, 6],\n",
    "    \"cnn_dropout_value\": [0.3, 0.4, 0.5],\n",
    "    \"transformer_dropout_value\": [0.3, 0.5, 0.7],\n",
    "    \"lr\": [1e-5],\n",
    "    \"weight_decay\": [1e-4, 1e-3],\n",
    "    \"T_0\": [10],\n",
    "    \"T_mult\": [1],\n",
    "    \"eta_min\": [5e-5],\n",
    "}\n",
    "\n",
    "total_combinations = math.prod(len(values) for values in param_grid.values())\n",
    "print(f\"Total combinations: {total_combinations}\")\n",
    "\n",
    "time_per_epoch = 10  # seconds\n",
    "num_epochs = 200  # epochs per configuration\n",
    "total_time_seconds = total_combinations * time_per_epoch * num_epochs\n",
    "\n",
    "# Convert to hours\n",
    "total_time_hours = total_time_seconds / 3600\n",
    "print(f\"Total time to hyper tune: {total_time_hours} hours\")"
   ],
   "id": "b9f5a5e907d2c63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 432\n",
      "Total time to hyper tune: 240.0 hours\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-02T10:53:39.257939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = hyperparameter_tuning(\n",
    "    vocab_size=len(vocab),\n",
    "    max_len=max_len,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    param_grid=param_grid,\n",
    "    training_method=training_method,\n",
    "    num_epochs=100\n",
    ")"
   ],
   "id": "2b980f756d58665b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/432 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration: {'embedding_dim': 128, 'num_heads': 2, 'transformer_hidden_dim': 256, 'num_transformer_layers': 2, 'cnn_dropout_value': 0.3, 'transformer_dropout_value': 0.3, 'lr': 1e-05, 'weight_decay': 0.0001, 'T_0': 10, 'T_mult': 1, 'eta_min': 5e-05}\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:45:00.917936Z",
     "start_time": "2024-11-30T08:45:00.914273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "dir_models = os.listdir(\"./models\")"
   ],
   "id": "b13f77f45721bd1d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:47:22.630384Z",
     "start_time": "2024-11-30T08:47:22.628377Z"
    }
   },
   "cell_type": "code",
   "source": "best_models = [path[:-3] for path in dir_models if path.endswith(\".pt\")]",
   "id": "d4641d6ec9100cd2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:28.781131Z",
     "start_time": "2024-11-30T08:50:28.778820Z"
    }
   },
   "cell_type": "code",
   "source": "best_val_loss = [float(str(best_model.split(\"+\")[-1:]).split(\"=\")[1][:8]) for best_model in best_models]",
   "id": "b66b0b18ddd45eaf",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:46.897115Z",
     "start_time": "2024-11-30T08:50:46.893840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_loss.sort()\n",
    "best_val_loss[:10]"
   ],
   "id": "f8072a414588fe28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.645867,\n",
       " 0.64889,\n",
       " 0.64901,\n",
       " 0.649261,\n",
       " 0.6493,\n",
       " 0.649739,\n",
       " 0.649992,\n",
       " 0.650589,\n",
       " 0.65069,\n",
       " 0.651973]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db3c79d743aaea6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
