{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.632901Z",
     "start_time": "2024-11-28T07:08:09.719822Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from torchvision import transforms\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Disable UserWarnings\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.653344Z",
     "start_time": "2024-11-28T07:08:12.632901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"./dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test.csv\")\n",
    "validation_df = pd.read_csv(\"./dataset/val.csv\")"
   ],
   "id": "68a1a9efa01eeb80",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.661149Z",
     "start_time": "2024-11-28T07:08:12.653852Z"
    }
   },
   "cell_type": "code",
   "source": "train_df",
   "id": "ea75fd0d701ae033",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        id  \\\n",
       "0     417812c5-0ce4-499d-b97d-4d28827239bc   \n",
       "1     5ac91fa3-55f2-4cb3-8c8f-ad84f78e6b36   \n",
       "2     d2705b90-8347-4cab-a7a6-654540d9a489   \n",
       "3     a3b33fe7-3085-4433-9c18-8814803891b4   \n",
       "4     1514b0e4-0665-45bc-ab32-52fce326cc29   \n",
       "...                                    ...   \n",
       "9995  1d1df243-485d-4b29-82c8-7e34c0de1f5c   \n",
       "9996  f7dfa883-e524-4974-b5ba-6b3c3db49087   \n",
       "9997  602e83dc-6539-4c1a-8d19-c1481b5c24bf   \n",
       "9998  d9ce2e8c-0831-466a-8756-4c40d772b1ce   \n",
       "9999  b22cdca0-79b3-4b37-a173-5176a32096f6   \n",
       "\n",
       "                                                caption  image_id  label  \n",
       "0     Wet elephants shake water onto people bathing ...    394330      0  \n",
       "1          Two men holding tennis racquets on the court    130849      0  \n",
       "2     A bird on a tree limb with mountains in the ba...    514790      0  \n",
       "3     A kitchen and dining room are featured along w...    182096      0  \n",
       "4        A fruit stand has various fruits on the table.     68788      1  \n",
       "...                                                 ...       ...    ...  \n",
       "9995     Several people stand in a field flying a kite.    522702      0  \n",
       "9996       A batter hitting a pitch at a baseball game.    441874      1  \n",
       "9997  A person on white surfboard next to group in a...    166716      0  \n",
       "9998  A baseball player getting ready to swing at th...    517601      0  \n",
       "9999                    a black cat is laying in a sink    394115      1  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417812c5-0ce4-499d-b97d-4d28827239bc</td>\n",
       "      <td>Wet elephants shake water onto people bathing ...</td>\n",
       "      <td>394330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ac91fa3-55f2-4cb3-8c8f-ad84f78e6b36</td>\n",
       "      <td>Two men holding tennis racquets on the court</td>\n",
       "      <td>130849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d2705b90-8347-4cab-a7a6-654540d9a489</td>\n",
       "      <td>A bird on a tree limb with mountains in the ba...</td>\n",
       "      <td>514790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a3b33fe7-3085-4433-9c18-8814803891b4</td>\n",
       "      <td>A kitchen and dining room are featured along w...</td>\n",
       "      <td>182096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514b0e4-0665-45bc-ab32-52fce326cc29</td>\n",
       "      <td>A fruit stand has various fruits on the table.</td>\n",
       "      <td>68788</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1d1df243-485d-4b29-82c8-7e34c0de1f5c</td>\n",
       "      <td>Several people stand in a field flying a kite.</td>\n",
       "      <td>522702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>f7dfa883-e524-4974-b5ba-6b3c3db49087</td>\n",
       "      <td>A batter hitting a pitch at a baseball game.</td>\n",
       "      <td>441874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>602e83dc-6539-4c1a-8d19-c1481b5c24bf</td>\n",
       "      <td>A person on white surfboard next to group in a...</td>\n",
       "      <td>166716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>d9ce2e8c-0831-466a-8756-4c40d772b1ce</td>\n",
       "      <td>A baseball player getting ready to swing at th...</td>\n",
       "      <td>517601</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>b22cdca0-79b3-4b37-a173-5176a32096f6</td>\n",
       "      <td>a black cat is laying in a sink</td>\n",
       "      <td>394115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.664530Z",
     "start_time": "2024-11-28T07:08:12.661149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Compile the regular expression pattern\n",
    "pattern = re.compile(r\"[^a-z ]\")\n",
    "\n",
    "\n",
    "def text_preparetion(sentence):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z.\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence1 = \"A World War II-era bomber flying out of formation\"\n",
    "text_preparetion(sentence1)"
   ],
   "id": "f47e808e14e16d34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a world war ii era bomber flying out of formation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.698310Z",
     "start_time": "2024-11-28T07:08:12.665528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df['preprocessed_text'] = train_df['caption'].progress_apply(text_preparetion)\n",
    "validation_df['preprocessed_text'] = validation_df['caption'].progress_apply(text_preparetion)\n",
    "test_df['preprocessed_text'] = test_df['caption'].progress_apply(text_preparetion)"
   ],
   "id": "decdaddece6dce3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 668926.67it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 601909.21it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 401426.42it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.705450Z",
     "start_time": "2024-11-28T07:08:12.698310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def build_simple_vocab(sentences, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Manually create a vocabulary from a list of tokenized sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): List of sentences to build vocabulary from.\n",
    "        special_tokens (list of str): Special tokens like <pad>, <unk>.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A vocabulary mapping tokens to indices.\n",
    "        dict: An inverse vocabulary mapping indices to tokens.\n",
    "    \"\"\"\n",
    "    special_tokens = special_tokens or ['<pad>', '<unk>']\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Default index is the current vocab size\n",
    "    for token in special_tokens:\n",
    "        vocab[token]  # Add special tokens first\n",
    "    \n",
    "    # Add tokens from sentences\n",
    "    for sentence in sentences:\n",
    "        for token in sentence.split():\n",
    "            if token.strip():  # Exclude empty tokens\n",
    "                vocab[token]\n",
    "    \n",
    "    # Convert to a normal dict (no longer dynamic)\n",
    "    vocab = dict(vocab)\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    return vocab, inverse_vocab\n",
    "\n",
    "\n",
    "# Vectorize a sentence\n",
    "def vectorize_sentence(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a tensor of token indices using a given vocabulary,\n",
    "    ignoring empty tokens.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "        vocab (Vocab): Vocabulary to map tokens to indices.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Vectorized sentence as a tensor.\n",
    "    \"\"\"\n",
    "    # Ensure '<unk>' exists in the vocabulary\n",
    "    unk_idx = vocab.get('<unk>', -1)\n",
    "    if unk_idx == -1:\n",
    "        raise ValueError(\"The vocabulary must include '<unk>' for unknown tokens.\")\n",
    "\n",
    "    # Split sentence into tokens and map them to indices\n",
    "    tokens = [token for token in sentence.split() if token.strip()]\n",
    "    return torch.tensor([vocab.get(token, unk_idx) for token in tokens], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, images_path, train=True, max_len=None):\n",
    "        \"\"\"\n",
    "        Dataset for preprocessing image-text pairs.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing 'image_id', 'sentence', and optionally 'label'.\n",
    "            vocab (Vocab): Vocabulary for text vectorization.\n",
    "            train (bool): Whether this is a training dataset.\n",
    "            max_len (int): Maximum length for sentences. If None, no truncation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.train = train\n",
    "        self.max_len = max_len\n",
    "        self.images_path = images_path\n",
    "\n",
    "        # Define image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(100, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Process sentence\n",
    "        sentence = row['caption']\n",
    "        vectorized_sentence = vectorize_sentence(sentence, self.vocab)\n",
    "\n",
    "        # Pad or truncate the sentence\n",
    "        if len(vectorized_sentence) < self.max_len:\n",
    "            padding_length = self.max_len - len(vectorized_sentence)\n",
    "            pad_tensor = torch.full((padding_length,), self.vocab['<pad>'], dtype=torch.long)\n",
    "            vectorized_sentence = torch.cat((vectorized_sentence, pad_tensor), dim=0)\n",
    "        else:\n",
    "            vectorized_sentence = vectorized_sentence[:self.max_len]\n",
    "\n",
    "        # Process image\n",
    "        image_path = f\"{self.images_path}{row['image_id']}.jpg\"\n",
    "            \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
    "            image = self.image_transform(image)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "        # Handle labels (for training)\n",
    "        if self.train:\n",
    "            label = row['label']\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'labels': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'id': row['id']\n",
    "            }\n"
   ],
   "id": "b120e5be81dc41aa",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.807585Z",
     "start_time": "2024-11-28T07:08:12.705450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_sentence = [sentence.split(\" \") for sentence in train_df['preprocessed_text']]\n",
    "max_len = max(len([token.strip() for token in sentence.split(\" \")]) for sentence in train_df['preprocessed_text'])\n",
    "vocab, inverse_vocab = build_simple_vocab(train_df['preprocessed_text'])"
   ],
   "id": "434eac424e669b66",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.810464Z",
     "start_time": "2024-11-28T07:08:12.807585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = PreprocessingDataset(train_df, vocab, train=True, max_len=max_len, images_path = \"./dataset/train_images/\")\n",
    "test_dataset = PreprocessingDataset(test_df, vocab, train=False, max_len=max_len, images_path = \"./dataset/test_images/\")\n",
    "val_dataset = PreprocessingDataset(validation_df, vocab, train=True, max_len=max_len, images_path = \"./dataset/val_images/\")"
   ],
   "id": "f6a9c5515a874c53",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.812979Z",
     "start_time": "2024-11-28T07:08:12.810464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "ef7e8ef05ced1727",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.827302Z",
     "start_time": "2024-11-28T07:08:12.812979Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "51c0d07bf84fb724",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.833072Z",
     "start_time": "2024-11-28T07:08:12.827302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, num_classes=1, lstm_dropout=0.3):\n",
    "        super(ImageTextClassifier, self).__init__()\n",
    "        \n",
    "        # CNN for image feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),  # (Batch, 32, H, W)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),  # Normalize\n",
    "            nn.Dropout(0.2),     # Dropout\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (Batch, 32, H/2, W/2)\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # (Batch, 64, H/2, W/2)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),  # Normalize\n",
    "            nn.Dropout(0.2),     # Dropout\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (Batch, 64, H/4, W/4)\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),  # (Batch, 128, H/4, W/4)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),  # Normalize\n",
    "            nn.Dropout(0.2),      # Dropout\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Output size (Batch, 128, 1, 1)\n",
    "        )\n",
    "        \n",
    "        # Flatten image features to (Batch, 128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Embedding + LSTM for text feature extraction\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_hidden_dim, batch_first=True,\n",
    "                            dropout=lstm_dropout,  # Dropout between LSTM layers\n",
    "                            bidirectional=False)   # Unidirectional LSTM\n",
    "        \n",
    "        # Regularization for LSTM (weight clipping)\n",
    "        self.lstm_clipping = 1.0  # Clipping threshold for gradient values\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(128 + lstm_hidden_dim, 256)\n",
    "        self.dropout_fc = nn.Dropout(0.7)  # Dropout in the fully connected layer\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        # Image feature extraction\n",
    "        img_features = self.cnn(images)  # (Batch, 128, 1, 1)\n",
    "        img_features = self.flatten(img_features)  # (Batch, 128)\n",
    "        \n",
    "        # Text feature extraction\n",
    "        embedded_captions = self.embedding(captions)  # (Batch, SeqLen, EmbeddingDim)\n",
    "        lstm_out, (lstm_hidden, _) = self.lstm(embedded_captions)  # lstm_hidden: (1, Batch, HiddenDim)\n",
    "        text_features = lstm_hidden.squeeze(0)  # (Batch, HiddenDim)\n",
    "        \n",
    "        # Combine image and text features\n",
    "        combined_features = torch.cat((img_features, text_features), dim=1)  # (Batch, 128 + HiddenDim)\n",
    "        \n",
    "        # Classification\n",
    "        x = F.relu(self.fc1(combined_features))  # (Batch, 256)\n",
    "        # x = self.dropout_fc(x)  # Apply dropout\n",
    "        x = self.fc2(x)  # (Batch, 1 for binary classification)\n",
    "        return x.squeeze(1)  # Logits (not probabilities)\n",
    "    \n",
    "    def clip_lstm_weights(self):\n",
    "        # Clip LSTM weights to prevent exploding gradients\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                param.data = torch.clamp(param.data, -self.lstm_clipping, self.lstm_clipping)\n"
   ],
   "id": "7c2f52a2b797846b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.839198Z",
     "start_time": "2024-11-28T07:08:12.833072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "vocab_size = len(vocab)   # Total number of words in vocabulary\n",
    "embedding_dim = 100       # Dimension of word embeddings\n",
    "lstm_hidden_dim = 128     # Hidden state size for LSTM\n",
    "num_classes = 1           # Binary classification (matching or non-matching)\n",
    "\n",
    "# Initialize the model\n",
    "model = ImageTextClassifier(vocab_size, embedding_dim, lstm_hidden_dim, num_classes)"
   ],
   "id": "b8e49cebdfb0724c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.947472Z",
     "start_time": "2024-11-28T07:08:12.839198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use logits directly\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)  # T_0 -> Number of iterations until the first restart"
   ],
   "id": "2fd4fe8235fc725d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:08:12.954956Z",
     "start_time": "2024-11-28T07:08:12.948474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def training_method(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader):\n",
    "    train_losses = []  # List to store training losses\n",
    "    val_losses = []  # List to store validation losses\n",
    "    val_accuracies = []  # List to store validation accuracies\n",
    "    val_precisions = []  # List to store validation precisions\n",
    "    val_recalls = []  # List to store validation recalls\n",
    "    val_f1s = []  # List to store validation F1-scores\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        ### TRAINING\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "        \n",
    "        # Add tqdm for training progress bar\n",
    "        # train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\")\n",
    "        for batch in train_dataloader:\n",
    "            images = batch['images'].to(device)      # Images from batch\n",
    "            captions = batch['captions'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            output = model(images, captions)  # Forward pass (logits)\n",
    "            loss = criterion(output, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            \n",
    "            # Clip LSTM weights\n",
    "            model.clip_lstm_weights()\n",
    "            \n",
    "            optimizer.step()  # Update weights\n",
    "            training_loss += loss.item()  # Accumulate loss\n",
    "            \n",
    "            # Update tqdm with current batch loss\n",
    "            # train_loader_tqdm.set_postfix({'Batch Loss': loss.item()})\n",
    "            \n",
    "        train_loss = training_loss / len(train_loader)  # Average training loss\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        ### VALIDATING\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        all_labels = []  # Ground truth labels for validation\n",
    "        all_preds = []   # Predictions for validation\n",
    "        \n",
    "        # Add tqdm for validation progress bar\n",
    "        # val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validating\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                images = batch['images'].to(device)\n",
    "                captions = batch['captions'].to(device)\n",
    "                labels = batch['labels'].to(device).float()\n",
    "                \n",
    "                output = model(images, captions)  # Forward pass (logits)\n",
    "                loss = criterion(output, labels)  # Compute validation loss\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "                # Convert logits to probabilities and apply threshold\n",
    "                preds = (torch.sigmoid(output) > 0.5).float()\n",
    "                \n",
    "                # Store for statistics\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                \n",
    "        val_loss = validation_loss / len(val_loader)  # Average validation loss\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Compute validation statistics\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        val_recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        val_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        \n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Training Loss: {train_loss:.4f} - \"\n",
    "              f\"Validation Loss: {val_loss:.4f} - \"\n",
    "              f\"Accuracy: {val_accuracy:.4f} - \"\n",
    "              f\"Precision: {val_precision:.4f} - \"\n",
    "              f\"Recall: {val_recall:.4f} - \"\n",
    "              f\"F1 Score: {val_f1:.4f} - \")\n",
    "\n",
    "    print('Training finished!')\n",
    "\n",
    "    # Plotting the losses and validation metrics over epochs\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(val_accuracies, label='Accuracy')\n",
    "    plt.plot(val_precisions, label='Precision')\n",
    "    plt.plot(val_recalls, label='Recall')\n",
    "    plt.plot(val_f1s, label='F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.title('Validation Metrics')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "6280acf86aa228a2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:21:59.597963Z",
     "start_time": "2024-11-28T07:08:12.954956Z"
    }
   },
   "cell_type": "code",
   "source": "training_method(model, criterion, optimizer, scheduler, num_epochs=200, train_loader=train_dataloader, val_loader=val_dataloader)",
   "id": "ee2f8b5c78457e56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200 - Training Loss: 0.6921 - Validation Loss: 0.6937 - Accuracy: 0.5197 - Precision: 0.5187 - Recall: 0.5460 - F1 Score: 0.5320 - \n",
      "\n",
      "Epoch 2/200 - Training Loss: 0.6875 - Validation Loss: 0.7075 - Accuracy: 0.5127 - Precision: 0.5068 - Recall: 0.9400 - F1 Score: 0.6586 - \n",
      "\n",
      "Epoch 3/200 - Training Loss: 0.6882 - Validation Loss: 0.6955 - Accuracy: 0.5043 - Precision: 0.5023 - Recall: 0.9280 - F1 Score: 0.6518 - \n",
      "\n",
      "Epoch 4/200 - Training Loss: 0.6859 - Validation Loss: 0.6990 - Accuracy: 0.5157 - Precision: 0.5086 - Recall: 0.9293 - F1 Score: 0.6574 - \n",
      "\n",
      "Epoch 5/200 - Training Loss: 0.6845 - Validation Loss: 0.6974 - Accuracy: 0.5077 - Precision: 0.5042 - Recall: 0.9180 - F1 Score: 0.6509 - \n",
      "\n",
      "Epoch 6/200 - Training Loss: 0.6835 - Validation Loss: 0.7021 - Accuracy: 0.5200 - Precision: 0.5114 - Recall: 0.8987 - F1 Score: 0.6518 - \n",
      "\n",
      "Epoch 7/200 - Training Loss: 0.6811 - Validation Loss: 0.6951 - Accuracy: 0.5207 - Precision: 0.5119 - Recall: 0.8893 - F1 Score: 0.6498 - \n",
      "\n",
      "Epoch 8/200 - Training Loss: 0.6790 - Validation Loss: 0.6995 - Accuracy: 0.5230 - Precision: 0.5139 - Recall: 0.8520 - F1 Score: 0.6411 - \n",
      "\n",
      "Epoch 9/200 - Training Loss: 0.6791 - Validation Loss: 0.6960 - Accuracy: 0.5287 - Precision: 0.5170 - Recall: 0.8720 - F1 Score: 0.6491 - \n",
      "\n",
      "Epoch 10/200 - Training Loss: 0.6777 - Validation Loss: 0.6937 - Accuracy: 0.5293 - Precision: 0.5178 - Recall: 0.8533 - F1 Score: 0.6445 - \n",
      "\n",
      "Epoch 11/200 - Training Loss: 0.6840 - Validation Loss: 0.7105 - Accuracy: 0.5087 - Precision: 0.5047 - Recall: 0.9353 - F1 Score: 0.6556 - \n",
      "\n",
      "Epoch 12/200 - Training Loss: 0.6822 - Validation Loss: 0.6969 - Accuracy: 0.5477 - Precision: 0.5350 - Recall: 0.7280 - F1 Score: 0.6168 - \n",
      "\n",
      "Epoch 13/200 - Training Loss: 0.6829 - Validation Loss: 0.6904 - Accuracy: 0.5330 - Precision: 0.5200 - Recall: 0.8573 - F1 Score: 0.6474 - \n",
      "\n",
      "Epoch 14/200 - Training Loss: 0.6810 - Validation Loss: 0.6894 - Accuracy: 0.5503 - Precision: 0.5364 - Recall: 0.7413 - F1 Score: 0.6224 - \n",
      "\n",
      "Epoch 15/200 - Training Loss: 0.6810 - Validation Loss: 0.6849 - Accuracy: 0.5563 - Precision: 0.5404 - Recall: 0.7540 - F1 Score: 0.6296 - \n",
      "\n",
      "Epoch 16/200 - Training Loss: 0.6789 - Validation Loss: 0.6933 - Accuracy: 0.5393 - Precision: 0.5255 - Recall: 0.8100 - F1 Score: 0.6375 - \n",
      "\n",
      "Epoch 17/200 - Training Loss: 0.6770 - Validation Loss: 0.6912 - Accuracy: 0.5363 - Precision: 0.5244 - Recall: 0.7807 - F1 Score: 0.6274 - \n",
      "\n",
      "Epoch 18/200 - Training Loss: 0.6762 - Validation Loss: 0.6932 - Accuracy: 0.5410 - Precision: 0.5270 - Recall: 0.7993 - F1 Score: 0.6352 - \n",
      "\n",
      "Epoch 19/200 - Training Loss: 0.6744 - Validation Loss: 0.6888 - Accuracy: 0.5460 - Precision: 0.5308 - Recall: 0.7927 - F1 Score: 0.6358 - \n",
      "\n",
      "Epoch 20/200 - Training Loss: 0.6748 - Validation Loss: 0.6911 - Accuracy: 0.5427 - Precision: 0.5288 - Recall: 0.7840 - F1 Score: 0.6316 - \n",
      "\n",
      "Epoch 21/200 - Training Loss: 0.6807 - Validation Loss: 0.7161 - Accuracy: 0.5247 - Precision: 0.5149 - Recall: 0.8507 - F1 Score: 0.6415 - \n",
      "\n",
      "Epoch 22/200 - Training Loss: 0.6817 - Validation Loss: 0.6839 - Accuracy: 0.5577 - Precision: 0.5421 - Recall: 0.7427 - F1 Score: 0.6267 - \n",
      "\n",
      "Epoch 23/200 - Training Loss: 0.6787 - Validation Loss: 0.6858 - Accuracy: 0.5523 - Precision: 0.5383 - Recall: 0.7360 - F1 Score: 0.6218 - \n",
      "\n",
      "Epoch 24/200 - Training Loss: 0.6777 - Validation Loss: 0.6899 - Accuracy: 0.5557 - Precision: 0.5457 - Recall: 0.6647 - F1 Score: 0.5993 - \n",
      "\n",
      "Epoch 25/200 - Training Loss: 0.6768 - Validation Loss: 0.6875 - Accuracy: 0.5513 - Precision: 0.5367 - Recall: 0.7513 - F1 Score: 0.6261 - \n",
      "\n",
      "Epoch 26/200 - Training Loss: 0.6755 - Validation Loss: 0.6829 - Accuracy: 0.5600 - Precision: 0.5430 - Recall: 0.7580 - F1 Score: 0.6327 - \n",
      "\n",
      "Epoch 27/200 - Training Loss: 0.6746 - Validation Loss: 0.6831 - Accuracy: 0.5643 - Precision: 0.5517 - Recall: 0.6860 - F1 Score: 0.6116 - \n",
      "\n",
      "Epoch 28/200 - Training Loss: 0.6723 - Validation Loss: 0.6842 - Accuracy: 0.5607 - Precision: 0.5515 - Recall: 0.6493 - F1 Score: 0.5964 - \n",
      "\n",
      "Epoch 29/200 - Training Loss: 0.6710 - Validation Loss: 0.6835 - Accuracy: 0.5617 - Precision: 0.5502 - Recall: 0.6760 - F1 Score: 0.6066 - \n",
      "\n",
      "Epoch 30/200 - Training Loss: 0.6703 - Validation Loss: 0.6870 - Accuracy: 0.5590 - Precision: 0.5461 - Recall: 0.6993 - F1 Score: 0.6133 - \n",
      "\n",
      "Epoch 31/200 - Training Loss: 0.6775 - Validation Loss: 0.6822 - Accuracy: 0.5690 - Precision: 0.5584 - Recall: 0.6593 - F1 Score: 0.6047 - \n",
      "\n",
      "Epoch 32/200 - Training Loss: 0.6776 - Validation Loss: 0.6824 - Accuracy: 0.5643 - Precision: 0.5544 - Recall: 0.6560 - F1 Score: 0.6009 - \n",
      "\n",
      "Epoch 33/200 - Training Loss: 0.6751 - Validation Loss: 0.6833 - Accuracy: 0.5603 - Precision: 0.5519 - Recall: 0.6413 - F1 Score: 0.5933 - \n",
      "\n",
      "Epoch 34/200 - Training Loss: 0.6741 - Validation Loss: 0.6836 - Accuracy: 0.5643 - Precision: 0.5576 - Recall: 0.6227 - F1 Score: 0.5883 - \n",
      "\n",
      "Epoch 35/200 - Training Loss: 0.6726 - Validation Loss: 0.6906 - Accuracy: 0.5377 - Precision: 0.5280 - Recall: 0.7093 - F1 Score: 0.6054 - \n",
      "\n",
      "Epoch 36/200 - Training Loss: 0.6712 - Validation Loss: 0.6891 - Accuracy: 0.5543 - Precision: 0.5481 - Recall: 0.6193 - F1 Score: 0.5815 - \n",
      "\n",
      "Epoch 37/200 - Training Loss: 0.6695 - Validation Loss: 0.6848 - Accuracy: 0.5713 - Precision: 0.5656 - Recall: 0.6153 - F1 Score: 0.5894 - \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m training_method(model, criterion, optimizer, scheduler, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, train_loader\u001B[38;5;241m=\u001B[39mtrain_dataloader, val_loader\u001B[38;5;241m=\u001B[39mval_dataloader)\n",
      "Cell \u001B[1;32mIn[14], line 17\u001B[0m, in \u001B[0;36mtraining_method\u001B[1;34m(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader)\u001B[0m\n\u001B[0;32m     13\u001B[0m training_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Add tqdm for training progress bar\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\")\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[0;32m     18\u001B[0m     images \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)      \u001B[38;5;66;03m# Images from batch\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     captions \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaptions\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Captions from batch\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[6], line 107\u001B[0m, in \u001B[0;36mPreprocessingDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    106\u001B[0m     image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(image_path)\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Convert to RGB\u001B[39;00m\n\u001B[1;32m--> 107\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_transform(image)\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage not found at path: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:1280\u001B[0m, in \u001B[0;36mColorJitter.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m   1278\u001B[0m         img \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39madjust_saturation(img, saturation_factor)\n\u001B[0;32m   1279\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m hue_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1280\u001B[0m         img \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39madjust_hue(img, hue_factor)\n\u001B[0;32m   1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torchvision\\transforms\\functional.py:968\u001B[0m, in \u001B[0;36madjust_hue\u001B[1;34m(img, hue_factor)\u001B[0m\n\u001B[0;32m    966\u001B[0m     _log_api_usage_once(adjust_hue)\n\u001B[0;32m    967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F_pil\u001B[38;5;241m.\u001B[39madjust_hue(img, hue_factor)\n\u001B[0;32m    970\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39madjust_hue(img, hue_factor)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:117\u001B[0m, in \u001B[0;36madjust_hue\u001B[1;34m(img, hue_factor)\u001B[0m\n\u001B[0;32m    113\u001B[0m np_h \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(hue_factor \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m255\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8)\n\u001B[0;32m    115\u001B[0m h \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(np_h, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 117\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mmerge(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHSV\u001B[39m\u001B[38;5;124m\"\u001B[39m, (h, s, v))\u001B[38;5;241m.\u001B[39mconvert(input_mode)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\PIL\\Image.py:1152\u001B[0m, in \u001B[0;36mImage.convert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m   1149\u001B[0m     dither \u001B[38;5;241m=\u001B[39m Dither\u001B[38;5;241m.\u001B[39mFLOYDSTEINBERG\n\u001B[0;32m   1151\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1152\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim\u001B[38;5;241m.\u001B[39mconvert(mode, dither)\n\u001B[0;32m   1153\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m   1154\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1155\u001B[0m         \u001B[38;5;66;03m# normalize source image and try again\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T07:21:59.598966Z",
     "start_time": "2024-11-28T07:21:59.597963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = []\n",
    "predictions = []\n",
    "\n",
    "def make_submission(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            id = batch['id']\n",
    "            \n",
    "            output = model(images, captions)\n",
    "            preds = (torch.sigmoid(output) > 0.5).int()\n",
    "            \n",
    "            ids.extend(id)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "    df = pd.DataFrame({'id': ids, 'label': predictions})\n",
    "    df.to_csv('submission2.csv', index=False)"
   ],
   "id": "95402ff5c7357c82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "make_submission(model, test_dataloader)",
   "id": "6cc9a81e05125a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "48961b76059f0f48",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
