{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:05.342622Z",
     "start_time": "2025-01-11T20:38:05.338536Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from torchvision import transforms\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from textblob import TextBlob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Disable UserWarnings\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:05.393323Z",
     "start_time": "2025-01-11T20:38:05.374032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"./dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test.csv\")\n",
    "validation_df = pd.read_csv(\"./dataset/val.csv\")"
   ],
   "id": "68a1a9efa01eeb80",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:05.428863Z",
     "start_time": "2025-01-11T20:38:05.393830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import text_preprocessing\n",
    "\n",
    "train_df['preprocessed_text'] = train_df['caption'].progress_apply(text_preprocessing.text_preparetion_simple)\n",
    "validation_df['preprocessed_text'] = validation_df['caption'].progress_apply(text_preprocessing.text_preparetion_simple)\n",
    "test_df['preprocessed_text'] = test_df['caption'].progress_apply(text_preprocessing.text_preparetion_simple)"
   ],
   "id": "decdaddece6dce3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 580141.08it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 532227.05it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 394962.47it/s]\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:05.434462Z",
     "start_time": "2025-01-11T20:38:05.429369Z"
    }
   },
   "cell_type": "code",
   "source": "validation_df",
   "id": "348bb7544ec995b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        id  \\\n",
       "0     a087830f-54e9-4ce2-9161-616b7438d390   \n",
       "1     bd017842-68ed-481b-9686-88e37e1a0b87   \n",
       "2     0d14e4b2-0d92-49c1-8363-83c8f224f56b   \n",
       "3     a148bd15-7910-4c4b-9f3b-64f22370b7cc   \n",
       "4     07ba0a04-75e4-44fc-8d5b-a89bf80b740c   \n",
       "...                                    ...   \n",
       "2995  7f59af29-6d75-44d3-9697-3525cc342dae   \n",
       "2996  d6d197e0-1ffc-420e-a9f5-b6d3971adfae   \n",
       "2997  f7197a85-e395-4b92-9a0e-705d858a7bb7   \n",
       "2998  bf904d37-c479-4bfc-b461-b94ab4ca0d45   \n",
       "2999  56ef63ca-9151-4961-b281-28003e78f8a5   \n",
       "\n",
       "                                                caption  image_id  label  \\\n",
       "0     The under carriage of a jet plane on take off ...    149202      0   \n",
       "1     A female professional tennis player preparing ...     49933      1   \n",
       "2         An outdoor market with fruit and vegetables.     449510      1   \n",
       "3            A soccer player prepares to kick the ball.    383866      0   \n",
       "4          A group of elephants walking in muddy water.    219792      1   \n",
       "...                                                 ...       ...    ...   \n",
       "2995  Panorama photograph of people in an indoor mar...     94589      0   \n",
       "2996       Two men playing a game of frisbee at a park.    509471      1   \n",
       "2997  Men's doubles tennis players shaking hands on ...    231675      0   \n",
       "2998  Small computer desk with electronic equipment ...    450856      0   \n",
       "2999  Boy wearing safety gear is airborne while doin...    477654      0   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0     the under carriage of a jet plane on take off ...  \n",
       "1     a female professional tennis player preparing ...  \n",
       "2          an outdoor market with fruit and vegetables   \n",
       "3             a soccer player prepares to kick the ball  \n",
       "4           a group of elephants walking in muddy water  \n",
       "...                                                 ...  \n",
       "2995  panorama photograph of people in an indoor mar...  \n",
       "2996        two men playing a game of frisbee at a park  \n",
       "2997  mens doubles tennis players shaking hands on t...  \n",
       "2998  small computer desk with electronic equipment ...  \n",
       "2999  boy wearing safety gear is airborne while doin...  \n",
       "\n",
       "[3000 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a087830f-54e9-4ce2-9161-616b7438d390</td>\n",
       "      <td>The under carriage of a jet plane on take off ...</td>\n",
       "      <td>149202</td>\n",
       "      <td>0</td>\n",
       "      <td>the under carriage of a jet plane on take off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bd017842-68ed-481b-9686-88e37e1a0b87</td>\n",
       "      <td>A female professional tennis player preparing ...</td>\n",
       "      <td>49933</td>\n",
       "      <td>1</td>\n",
       "      <td>a female professional tennis player preparing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0d14e4b2-0d92-49c1-8363-83c8f224f56b</td>\n",
       "      <td>An outdoor market with fruit and vegetables.</td>\n",
       "      <td>449510</td>\n",
       "      <td>1</td>\n",
       "      <td>an outdoor market with fruit and vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a148bd15-7910-4c4b-9f3b-64f22370b7cc</td>\n",
       "      <td>A soccer player prepares to kick the ball.</td>\n",
       "      <td>383866</td>\n",
       "      <td>0</td>\n",
       "      <td>a soccer player prepares to kick the ball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07ba0a04-75e4-44fc-8d5b-a89bf80b740c</td>\n",
       "      <td>A group of elephants walking in muddy water.</td>\n",
       "      <td>219792</td>\n",
       "      <td>1</td>\n",
       "      <td>a group of elephants walking in muddy water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>7f59af29-6d75-44d3-9697-3525cc342dae</td>\n",
       "      <td>Panorama photograph of people in an indoor mar...</td>\n",
       "      <td>94589</td>\n",
       "      <td>0</td>\n",
       "      <td>panorama photograph of people in an indoor mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>d6d197e0-1ffc-420e-a9f5-b6d3971adfae</td>\n",
       "      <td>Two men playing a game of frisbee at a park.</td>\n",
       "      <td>509471</td>\n",
       "      <td>1</td>\n",
       "      <td>two men playing a game of frisbee at a park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>f7197a85-e395-4b92-9a0e-705d858a7bb7</td>\n",
       "      <td>Men's doubles tennis players shaking hands on ...</td>\n",
       "      <td>231675</td>\n",
       "      <td>0</td>\n",
       "      <td>mens doubles tennis players shaking hands on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>bf904d37-c479-4bfc-b461-b94ab4ca0d45</td>\n",
       "      <td>Small computer desk with electronic equipment ...</td>\n",
       "      <td>450856</td>\n",
       "      <td>0</td>\n",
       "      <td>small computer desk with electronic equipment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>56ef63ca-9151-4961-b281-28003e78f8a5</td>\n",
       "      <td>Boy wearing safety gear is airborne while doin...</td>\n",
       "      <td>477654</td>\n",
       "      <td>0</td>\n",
       "      <td>boy wearing safety gear is airborne while doin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:05.443135Z",
     "start_time": "2025-01-11T20:38:05.434462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_simple_vocab(sentences, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Manually create a vocabulary from a list of tokenized sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): List of sentences to build vocabulary from.\n",
    "        special_tokens (list of str): Special tokens like <pad>, <unk>.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A vocabulary mapping tokens to indices.\n",
    "        dict: An inverse vocabulary mapping indices to tokens.\n",
    "    \"\"\"\n",
    "    special_tokens = special_tokens or ['<pad>', '<unk>']\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Default index is the current vocab size\n",
    "    for token in special_tokens:\n",
    "        vocab[token]  # Add special tokens first\n",
    "\n",
    "    # Add tokens from sentences\n",
    "    for sentence in sentences:\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            if token.strip():  # Exclude empty tokens\n",
    "                vocab[token]\n",
    "\n",
    "    # Convert to a normal dict (no longer dynamic)\n",
    "    vocab = dict(vocab)\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    return vocab, inverse_vocab\n",
    "\n",
    "\n",
    "# Vectorize a sentence\n",
    "def vectorize_sentence(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a tensor of token indices using a given vocabulary,\n",
    "    ignoring empty tokens.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "        vocab (Vocab): Vocabulary to map tokens to indices.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Vectorized sentence as a tensor.\n",
    "    \"\"\"\n",
    "    # Ensure '<unk>' exists in the vocabulary\n",
    "    unk_idx = vocab.get('<unk>', -1)\n",
    "    if unk_idx == -1:\n",
    "        raise ValueError(\"The vocabulary must include '<unk>' for unknown tokens.\")\n",
    "\n",
    "    # Split sentence into tokens and map them to indices\n",
    "    tokens = [token for token in sentence.split() if token.strip()]\n",
    "    return torch.tensor([vocab.get(token, unk_idx) for token in tokens], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, images_path, train=True, max_len=None, augmentation_prob=0.3):\n",
    "        \"\"\"\n",
    "        Dataset for preprocessing image-text pairs with TF-IDF vectorization.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing 'image_id', 'sentence', and optionally 'label'.\n",
    "            vectorizer (TfidfVectorizer): TF-IDF vectorizer for text.\n",
    "            images_path (str): Base path to the images.\n",
    "            train (bool): Whether this is a training dataset.\n",
    "            max_len (int): Maximum length for sentences in terms of features. Truncation isn't typical with TF-IDF.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.train = train\n",
    "        self.max_len = max_len\n",
    "        self.images_path = images_path\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        # Define image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(100, scale=(0.8, 1.0)),\n",
    "            # transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Process sentence\n",
    "        sentence = row['caption']\n",
    "        if random.random() < self.augmentation_prob:\n",
    "            # sentence = self.augment_text(sentence)\n",
    "            sentence = self.synonym_replacement(sentence)\n",
    "        vectorized_sentence = vectorize_sentence(sentence, self.vocab)\n",
    "\n",
    "        # Pad or truncate the sentence\n",
    "        if len(vectorized_sentence) < self.max_len:\n",
    "            padding_length = self.max_len - len(vectorized_sentence)\n",
    "            pad_tensor = torch.full((padding_length,), self.vocab['<pad>'], dtype=torch.long)\n",
    "            vectorized_sentence = torch.cat((vectorized_sentence, pad_tensor), dim=0)\n",
    "        else:\n",
    "            vectorized_sentence = vectorized_sentence[:self.max_len]\n",
    "\n",
    "        # Process image\n",
    "        image_path = f\"{self.images_path}{row['image_id']}.jpg\"\n",
    "            \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
    "            image = self.image_transform(image)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "        # Handle labels (for training)\n",
    "        if self.train:\n",
    "            label = row['label']\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'labels': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'id': row['id']\n",
    "            }\n",
    "\n",
    "    def augment_text(self, text):\n",
    "        \"\"\" Augment text using synonym replacement and rephrasing. \"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            return self.synonym_replacement(text)\n",
    "        else:\n",
    "            return self.rephrase_text(text)\n",
    "\n",
    "\n",
    "    def synonym_replacement(self, text):\n",
    "        # Tokenize the sentence\n",
    "        words = nltk.word_tokenize(text)\n",
    "        new_words = words.copy()\n",
    "    \n",
    "        # Find indices of words that can be replaced\n",
    "        replaceable = [i for i, word in enumerate(words) if wordnet.synsets(word)]\n",
    "        \n",
    "        # Randomly choose half of these words to replace\n",
    "        num_to_replace = len(replaceable) // 2\n",
    "        chosen_indices = random.sample(replaceable, num_to_replace)\n",
    "    \n",
    "        # Replace chosen words with synonyms\n",
    "        for i in chosen_indices:\n",
    "            synsets = wordnet.synsets(words[i])\n",
    "            if synsets:\n",
    "                # Choose a random synonym from the first synset\n",
    "                synonyms = list(set([lemma.name() for lemma in synsets[0].lemmas() if lemma.name() != words[i]]))\n",
    "                if synonyms:\n",
    "                    new_words[i] = random.choice(synonyms).replace('_', ' ')\n",
    "\n",
    "        # Reconstruct the sentence\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "\n",
    "    def rephrase_text(self, text):\n",
    "        blob = TextBlob(text)\n",
    "        return str(blob.correct())"
   ],
   "id": "b120e5be81dc41aa",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:05.934197Z",
     "start_time": "2025-01-11T20:38:05.443643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_sentence = [nltk.word_tokenize(sentence) for sentence in train_df['preprocessed_text']]\n",
    "max_len = max(len([token for token in sentence.split(\" \")]) for sentence in train_df['preprocessed_text'])\n",
    "print(max_len)\n",
    "vocab, inverse_vocab = build_simple_vocab(train_df['preprocessed_text'])"
   ],
   "id": "434eac424e669b66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:05.937763Z",
     "start_time": "2025-01-11T20:38:05.934705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = PreprocessingDataset(train_df, vocab, images_path = \"./dataset/train_images/\", train=True, max_len=max_len)\n",
    "val_dataset = PreprocessingDataset(validation_df, vocab, images_path = \"./dataset/val_images/\", train=True, max_len=max_len)\n",
    "test_dataset = PreprocessingDataset(test_df, vocab, images_path = \"./dataset/test_images/\", train=False, max_len=max_len)"
   ],
   "id": "f6a9c5515a874c53",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:05.941296Z",
     "start_time": "2025-01-11T20:38:05.938764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "ef7e8ef05ced1727",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.003614Z",
     "start_time": "2025-01-11T20:38:05.941296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "images = batch['images']\n",
    "captions = batch['captions']\n",
    "print(images.shape)\n",
    "print(captions.shape)\n",
    "print(captions[0])"
   ],
   "id": "c15ef9418428fccb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 100, 100])\n",
      "torch.Size([32, 46])\n",
      "tensor([   1,  164,   15,   18,  120,  300,   24,   16, 1149,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.006660Z",
     "start_time": "2025-01-11T20:38:06.004121Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "51c0d07bf84fb724",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.010201Z",
     "start_time": "2025-01-11T20:38:06.007167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ],
   "id": "2d2cc743de7dbeb2",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.015308Z",
     "start_time": "2025-01-11T20:38:06.010711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_dropout_value):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "\n",
    "        self.CNN_block = nn.Sequential(\n",
    "            self.CNN2d_block(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN2d_block(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN2d_block(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN2d_block(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            \n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            # SEBlock(128),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, images):\n",
    "        img_features = self.CNN_block(images)  \n",
    "        img_features = self.flatten(img_features)  \n",
    "        return img_features\n",
    "    \n",
    "    def CNN2d_block(self, in_channels, out_channels, kernel_size, stride, padding, cnn_dropout_value):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            # SEBlock(out_channels),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        "
   ],
   "id": "f0cf44b7b652262c",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.019372Z",
     "start_time": "2025-01-11T20:38:06.016323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SEBlock1D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock1D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)"
   ],
   "id": "d3b9361117eb34",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.023442Z",
     "start_time": "2025-01-11T20:38:06.019372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a matrix of shape (max_len, embedding_dim) for positional encodings\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(float(max_len)) / embedding_dim))\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)  # Shape: (max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Sin for even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Cos for odd indices\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension: (1, max_len, embedding_dim)\n",
    "        self.register_buffer('pe', pe)  # Register as non-learnable buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to the input embeddings.\n",
    "        x: (Batch, SeqLen, EmbeddingDim)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :].to(x.device)"
   ],
   "id": "d0fe1b54daf51be8",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.028Z",
     "start_time": "2025-01-11T20:38:06.023442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextModule(nn.Module):\n",
    "    def __init__(self, cnn_dropout_value, vocab_size, embedding_dim=32):\n",
    "        super(TextModule, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        # self.positional_encoding = PositionalEncoding(embedding_dim=embedding_dim, max_len=max_len)\n",
    "        \n",
    "\n",
    "        self.CNN1d = nn.Sequential(\n",
    "            self.CNN1d_block(in_channels=embedding_dim, out_channels=32, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN1d_block(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN1d_block(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            # SEBlock1D(128),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, captions):\n",
    "        text_features = self.embedding(captions).permute(0, 2, 1)\n",
    "        # text_features = self.positional_encoding(text_features)\n",
    "        text_features = self.CNN1d(text_features)  # \n",
    "        text_features = self.flatten(text_features)  # \n",
    "        return text_features\n",
    "    \n",
    "    def CNN1d_block(self, in_channels, out_channels, kernel_size, stride, padding, cnn_dropout_value):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            # SEBlock1D(out_channels),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )"
   ],
   "id": "e6ea631b4226a0d8",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.031063Z",
     "start_time": "2025-01-11T20:38:06.028507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ],
   "id": "d4ae90c24e535ca5",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.034602Z",
     "start_time": "2025-01-11T20:38:06.031570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=1.0,\n",
    "        image_embedding=128,\n",
    "        text_embedding=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = CNN(0.25)\n",
    "        self.text_encoder = TextModule(0.25, len(vocab))\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, images, texts):\n",
    "        image_features = self.image_encoder(images)\n",
    "        text_features = self.text_encoder(texts)\n",
    "        \n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "        \n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        similarity_scores = torch.diag(logits)\n",
    "        \n",
    "        return torch.sigmoid(similarity_scores)"
   ],
   "id": "5366b540a2446756",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.038162Z",
     "start_time": "2025-01-11T20:38:06.034602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.TransformerEncoderLayer):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.ones_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)"
   ],
   "id": "340278a77ea962ef",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.054345Z",
     "start_time": "2025-01-11T20:38:06.038670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model_config = {\n",
    "    \"cnn_dropout_value\": 0.25,\n",
    "    \"vocab_size\":len(vocab),\n",
    "}\n",
    "\n",
    "model = CLIPModel()\n",
    "# model.apply(initialize_weights)\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# test modules\n",
    "dummy_images = torch.randn(2, 3, 100, 100).to(device)\n",
    "dummy_texts = torch.randint(0, len(vocab), (2, max_len)).to(device)\n",
    "dummy_labels = torch.randint(0, 2, (2,), dtype=torch.float).to(device)\n",
    "\n",
    "output = model(dummy_images, dummy_texts)\n",
    "loss = criterion(output, dummy_labels)\n",
    "output, loss"
   ],
   "id": "b8e49cebdfb0724c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.8656, 0.8852], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " tensor(0.1331, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.057392Z",
     "start_time": "2025-01-11T20:38:06.054852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),  lr=1e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Number of epochs before the first restart\n",
    "    T_mult=1,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=1e-5  # Minimum learning rate\n",
    ")"
   ],
   "id": "cbcdaae8ca3a8e4f",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T20:38:06.066470Z",
     "start_time": "2025-01-11T20:38:06.057899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def training_method(criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, patience=5, delta=0.05, loss_procentage_improvement=10):\n",
    "    train_losses = []  # List to store training losses\n",
    "    val_losses = []  # List to store validation losses\n",
    "    val_accuracies = []  # List to store validation accuracies\n",
    "    val_precisions = []  # List to store validation precisions\n",
    "    val_recalls = []  # List to store validation recalls\n",
    "    val_f1s = []  # List to store validation F1-scores\n",
    "    learning_rates = []  # List to store learning rate progression\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    initial_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_without_improvement = 0  # Track epochs without improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        all_distances = []\n",
    "\n",
    "        ### TRAINING\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            images = batch['images'].to(device)  # Images from batch\n",
    "            captions = batch['captions'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "            \n",
    "            # Forward pass           \n",
    "            outputs = model(images, captions)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "\n",
    "        train_loss = training_loss / len(train_loader.dataset)  # Average training loss\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        ### VALIDATING\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        all_labels = []  # Ground truth labels for validation\n",
    "        all_preds = []  # Predictions for validation\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['images'].to(device)\n",
    "                captions = batch['captions'].to(device)\n",
    "                labels = batch['labels'].to(device).float()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images, captions)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                validation_loss += loss.item() * images.size(0)\n",
    "                # Compute predictions\n",
    "                preds = (outputs > 0.5).float() # hypertune threshold\n",
    "\n",
    "                # Store for statistics\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                \n",
    "        print(f\"{sum(all_preds)=}\")\n",
    "        \n",
    "        val_loss = validation_loss / len(val_loader.dataset)  # Average validation loss\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Compute validation statistics\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        val_recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        val_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']  # Get the current learning rate\n",
    "        learning_rates.append(current_lr)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Training Loss: {train_loss:.4f} - \"\n",
    "              f\"Validation Loss: {val_loss:.4f} - \"\n",
    "              f\"Accuracy: {val_accuracy:.4f} - \"\n",
    "              f\"Precision: {val_precision:.4f} - \"\n",
    "              f\"Recall: {val_recall:.4f} - \"\n",
    "              f\"F1 Score: {val_f1:.4f} - \"\n",
    "              f\"Time: {end_time - start_time:.2f} - \"\n",
    "              f\"Lr: {current_lr:.2e}\")\n",
    "\n",
    "        if epoch == 1:\n",
    "            initial_loss = val_loss\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            print(f\"New best model with Loss: {val_loss:.4f} at epoch {epoch + 1}\")\n",
    "        elif val_loss < best_val_loss + delta:\n",
    "            print(f\"Validation loss did not improve significantly\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Validation loss did not improve for {epochs_without_improvement} epoch(s).\")\n",
    "            # Stop training if validation loss does not improve for 'patience' epochs\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}. Best Loss: {best_val_loss:.4f}\")\n",
    "                break  # Exit training loop\n",
    "\n",
    "    print('Training finished!')\n",
    "\n",
    "    # save the model only if the best loss is lower than the first initial loss ( to see that the model actually improved with 10% loss )\n",
    "    if best_val_loss < (100 - loss_procentage_improvement) * initial_loss:\n",
    "        # Init plot&model save path\n",
    "        # plt_save_path = \"models/\"\n",
    "        # model_config['eval_loss'] = best_val_loss\n",
    "        # for key, value in model_config.items():\n",
    "        #     plt_save_path += key + \"=\" + str(value) + \"+\"\n",
    "        # plt_save_path = plt_save_path[:-1] + \".png\"\n",
    "        # model_path = plt_save_path[:-4] + \".pt\"\n",
    "        # torch.save(best_model.state_dict(), model_path)\n",
    "        # print(f\"Best model with Loss: {best_val_loss:.4f} saved.\")\n",
    "        # print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        # Plotting the losses and validation metrics over epochs\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(val_accuracies, label='Accuracy')\n",
    "        plt.plot(val_precisions, label='Precision')\n",
    "        plt.plot(val_recalls, label='Recall')\n",
    "        plt.plot(val_f1s, label='F1 Score')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Metric')\n",
    "        plt.title('Validation Metrics')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(learning_rates, label='Learning Rate')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.title(\"Learning Rate Progression\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig(plt_save_path)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Model wasn't saved because it didn't improve: {loss_procentage_improvement}%\")\n"
   ],
   "id": "6280acf86aa228a2",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-11T20:38:06.066978Z"
    }
   },
   "cell_type": "code",
   "source": "training_method(criterion, optimizer, scheduler, num_epochs=50, train_loader=train_dataloader, val_loader=val_dataloader)",
   "id": "ee2f8b5c78457e56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(all_preds)=1813.0\n",
      "\n",
      "Epoch 1/50 - Training Loss: 3.5432 - Validation Loss: 1.8958 - Accuracy: 0.5177 - Precision: 0.5146 - Recall: 0.6220 - F1 Score: 0.5632 - Time: 24.62 - Lr: 1.00e-05\n",
      "New best model with Loss: 1.8958 at epoch 1\n",
      "sum(all_preds)=2507.0\n",
      "\n",
      "Epoch 2/50 - Training Loss: 2.9691 - Validation Loss: 2.2161 - Accuracy: 0.5323 - Precision: 0.5193 - Recall: 0.8680 - F1 Score: 0.6499 - Time: 24.18 - Lr: 1.00e-05\n",
      "Validation loss did not improve for 1 epoch(s).\n",
      "sum(all_preds)=2296.0\n",
      "\n",
      "Epoch 3/50 - Training Loss: 2.5689 - Validation Loss: 1.6931 - Accuracy: 0.5380 - Precision: 0.5248 - Recall: 0.8033 - F1 Score: 0.6349 - Time: 24.02 - Lr: 1.00e-05\n",
      "New best model with Loss: 1.6931 at epoch 3\n",
      "sum(all_preds)=2602.0\n",
      "\n",
      "Epoch 4/50 - Training Loss: 2.2583 - Validation Loss: 1.9112 - Accuracy: 0.5187 - Precision: 0.5108 - Recall: 0.8860 - F1 Score: 0.6480 - Time: 23.78 - Lr: 1.00e-05\n",
      "Validation loss did not improve for 1 epoch(s).\n",
      "sum(all_preds)=2652.0\n",
      "\n",
      "Epoch 5/50 - Training Loss: 2.1871 - Validation Loss: 1.8987 - Accuracy: 0.5193 - Precision: 0.5109 - Recall: 0.9033 - F1 Score: 0.6527 - Time: 24.22 - Lr: 1.00e-05\n",
      "Validation loss did not improve for 2 epoch(s).\n",
      "sum(all_preds)=2596.0\n",
      "\n",
      "Epoch 6/50 - Training Loss: 2.0774 - Validation Loss: 1.6844 - Accuracy: 0.5187 - Precision: 0.5108 - Recall: 0.8840 - F1 Score: 0.6475 - Time: 25.53 - Lr: 1.00e-05\n",
      "New best model with Loss: 1.6844 at epoch 6\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def make_submission(test_loader, text_model, image_model):\n",
    "    image_model.eval()\n",
    "    text_model.eval()\n",
    "    ids = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            id = batch['id']\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, captions)\n",
    "            # Compute predictions\n",
    "            preds = (outputs > 0.5).float() # hypertune threshold\n",
    "\n",
    "            ids.extend(id)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "    df = pd.DataFrame({'id': ids, 'label': predictions}) \n",
    "    df.to_csv('submission5.csv', index=False) "
   ],
   "id": "95402ff5c7357c82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# # LOAD MODEL FROM PATH\n",
    "# model_config = {\n",
    "# ADD\n",
    "# }\n",
    "# model = ImageTextClassifier(**model_config)\n",
    "# model_path = \"vocab_size=3733+embedding_dim=128+num_filters=128+filter_sizes=[3, 4, 5, 6, 7, 8]+seq_len=53+cnn_text_drop_value=0.5+cnn_dropout_value=0.4+head_dropout_value=0.5+num_classes=1+eval_loss=0.6202353974606128.pt\"\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "# model.to(device)"
   ],
   "id": "595baf65e18e78cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "make_submission(test_dataloader, best_text_model, best_image_model)",
   "id": "6cc9a81e05125a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, temperature=0.07, image_embedding=256, text_embedding=256):\n",
    "        super().__init__()\n",
    "        self.image_encoder = CNN(0.25)\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * temperature)  # Learnable temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])  # [batch_size, image_embedding]\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )  # [batch_size, text_embedding]\n",
    "        \n",
    "        print(image_features.shape)\n",
    "        print(text_features.shape)\n",
    "        # Projecting Features to Latent Space\n",
    "        image_embeddings = self.image_projection(image_features)  # [batch_size, projection_dim]\n",
    "        text_embeddings = self.text_projection(text_features)    # [batch_size, projection_dim]\n",
    "        \n",
    "        # Normalize Embeddings\n",
    "        image_embeddings = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "        \n",
    "        # Similarity Scores\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature  # [batch_size, batch_size]\n",
    "        images_similarity = image_embeddings @ image_embeddings.T  # [batch_size, batch_size]\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T    # [batch_size, batch_size]\n",
    "        \n",
    "        # Targets\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature.detach(), dim=-1\n",
    "        )  # Detach temperature to prevent gradients from flowing\n",
    "        \n",
    "        # Loss Calculation\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')  # [batch_size]\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')  # [batch_size]\n",
    "        loss = (images_loss + texts_loss) / 2.0  # Average the losses\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid reduction mode: {reduction}\")\n",
    "\n",
    "\n",
    "# Dummy Components for Testing\n",
    "class ImageEncoder(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.randn(x.size(0), 128)  # Example dummy output\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return torch.randn(input_ids.size(0), 256)  # Example dummy output\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    images = torch.randn(8, 3, 100, 100)\n",
    "    input_ids = torch.randint(5, 300, size=(8, 25))\n",
    "    attention_mask = torch.ones(8, 25)\n",
    "    batch = {\n",
    "        'image': images,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "    CLIP = CLIPModel()\n",
    "    loss = CLIP(batch)\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n"
   ],
   "id": "7afcca15e0298353",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def hyperparameter_tuning(vocab_size, max_len, train_loader, val_loader, param_grid, training_method, num_epochs=200):\n",
    "    # Create all combinations of hyperparameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    for params in tqdm(param_combinations):\n",
    "        print(f\"Testing configuration: {params}\")\n",
    "\n",
    "        try:\n",
    "            # Update model configuration\n",
    "            model_config = {\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"embedding_dim\": params[\"embedding_dim\"],\n",
    "                \"num_filters\": params[\"num_filters\"],\n",
    "                \"filter_sizes\": params[\"filter_sizes\"],\n",
    "                \"seq_len\": max_len,\n",
    "                \"cnn_text_drop_value\": params[\"cnn_text_drop_value\"],\n",
    "                \"cnn_dropout_value\": params[\"cnn_dropout_value\"],\n",
    "                \"head_dropout_value\": params[\"head_dropout_value\"],\n",
    "            }\n",
    "\n",
    "            # Initialize model\n",
    "            model = ImageTextClassifier(**model_config)\n",
    "            model.to(device)\n",
    "            model.apply(initialize_weights)\n",
    "\n",
    "            # Define criterion, optimizer, and scheduler\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=params[\"lr\"],\n",
    "                weight_decay=params[\"weight_decay\"]\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer,\n",
    "                T_0=params[\"T_0\"],\n",
    "                eta_min=params[\"eta_min\"],\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            training_method(\n",
    "                model, criterion, optimizer, scheduler,\n",
    "                num_epochs=num_epochs,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader\n",
    "            )\n",
    "            print(f\"Completed configuration: {params}\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error with configuration: {params}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "\n",
    "        finally:\n",
    "            # Reset GPU memory\n",
    "            print(\"Resetting GPU memory...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()"
   ],
   "id": "cf6ef0fe03c73deb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    \"embedding_dim\": [128],\n",
    "    \"num_filters\": [16],\n",
    "    \"filter_sizes\": [[3, 4, 5], [3, 4, 5, 6, 7, 8, 9]],\n",
    "    \"head_dropout_value\": [0.5],\n",
    "    \"cnn_text_drop_value\": [0.5],\n",
    "    \"cnn_dropout_value\": [0.5],\n",
    "    \"lr\": [1e-5, 1e-4, 5e-4, 1e-3, 5e-3],  # Learning rate candidates\n",
    "    \"weight_decay\": [1e-6, 1e-5, 1e-4, 1e-3],  # Weight decay candidates\n",
    "    \"T_0\": [10],  # Number of epochs for the first cycle\n",
    "    \"T_mult\": [1],  # Cycle multiplier\n",
    "    \"eta_min\": [1e-6, 1e-5, 1e-4],  # Minimum learning rate\n",
    "}\n",
    "\n",
    "total_combinations = math.prod(len(values) for values in param_grid.values())\n",
    "print(f\"Total combinations: {total_combinations}\")\n",
    "\n",
    "time_per_epoch = 23  # seconds\n",
    "num_epochs = 100  # epochs per configuration\n",
    "total_time_seconds = total_combinations * time_per_epoch * num_epochs\n",
    "\n",
    "# Convert to hours\n",
    "total_time_hours = total_time_seconds / 3600\n",
    "print(f\"Total time to hyper tune: {total_time_hours} hours\")"
   ],
   "id": "b9f5a5e907d2c63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "results = hyperparameter_tuning(\n",
    "    vocab_size=len(vocab),\n",
    "    max_len=max_len,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    param_grid=param_grid,\n",
    "    training_method=training_method,\n",
    "    num_epochs=100\n",
    ")"
   ],
   "id": "2b980f756d58665b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "dir_models = os.listdir(\"./models\")",
   "id": "b13f77f45721bd1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "best_models = [path[:-3] for path in dir_models if path.endswith(\".pt\")]",
   "id": "d4641d6ec9100cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "best_val_loss = [float(str(best_model.split(\"+\")[-1:]).split(\"=\")[1][:8]) for best_model in best_models]",
   "id": "b66b0b18ddd45eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_loss.sort()\n",
    "best_val_loss[:100]"
   ],
   "id": "f8072a414588fe28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "db3c79d743aaea6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
