{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T16:03:55.535905Z",
     "start_time": "2025-01-01T16:03:55.531849Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from torchvision import transforms, models\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Disable UserWarnings\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T15:45:57.613243Z",
     "start_time": "2025-01-01T15:45:57.584185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"./dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test.csv\")\n",
    "validation_df = pd.read_csv(\"./dataset/val.csv\")"
   ],
   "id": "68a1a9efa01eeb80",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T15:45:59.795809Z",
     "start_time": "2025-01-01T15:45:57.613243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "def text_preparation_with_spell_correction(sentence):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z.\n",
    "    pattern = re.compile(r\"[^a-z ]\")\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    # 3. Correct spelling\n",
    "    words = sentence.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is None:\n",
    "            corrected_word = '<unk>'  # Use '<unk>' if no correction found\n",
    "        corrected_words.append(corrected_word)\n",
    "    corrected_sentence = ' '.join(corrected_words)\n",
    "    \n",
    "    return corrected_sentence\n",
    "\n",
    "sentence_example = '\"A domesticated carnivvorous mzammal that typicbally hfaas a lons sfnout, an acxujte sense off osmell, noneetractaaln crlaws, anid xbarkring,y howlingu, or whining rvoiche.\"'\n",
    "print(text_preparation_with_spell_correction(sentence_example))"
   ],
   "id": "963056941ecf496",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a domesticated carnivorous mammal that typically haas a long snout an acute sense off smell <unk> claws and barking y howling or whining voice\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T15:45:59.843955Z",
     "start_time": "2025-01-01T15:45:59.797344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# Assume 'vocabulary' is a predefined list of words expected in your domain\n",
    "vocabulary = ['example', 'list', 'of', 'words', 'in', 'your', 'vocabulary']\n",
    "\n",
    "\n",
    "def find_closest_word(word, vocabulary):\n",
    "    return min(vocabulary, key=lambda v: levenshtein_distance(word, v))\n",
    "\n",
    "\n",
    "def text_preparation_with_levenshtein(sentence):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z.\n",
    "    pattern = re.compile(r\"[^a-z ]\")\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    # 3. Replace words with the closest in vocabulary\n",
    "    words = sentence.split()\n",
    "    closest_words = [find_closest_word(word, vocabulary) for word in words]\n",
    "    corrected_sentence = ' '.join(closest_words)\n",
    "    \n",
    "    return corrected_sentence"
   ],
   "id": "36c7625d793a331b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T15:45:59.851097Z",
     "start_time": "2025-01-01T15:45:59.844976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compile the regular expression pattern\n",
    "pattern = re.compile(r\"[^a-z ]\")\n",
    "\n",
    "\n",
    "def text_preparetion_simple(sentence):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z.\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence1 = \"A World War II-era bomber flying out of formation\"\n",
    "text_preparetion_simple(sentence1)"
   ],
   "id": "f47e808e14e16d34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a world war ii era bomber flying out of formation'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:03:55.346802Z",
     "start_time": "2025-01-01T15:45:59.851607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df['preprocessed_text'] = train_df['caption'].progress_apply(text_preparation_with_spell_correction)\n",
    "validation_df['preprocessed_text'] = validation_df['caption'].progress_apply(text_preparation_with_spell_correction)\n",
    "test_df['preprocessed_text'] = test_df['caption'].progress_apply(text_preparation_with_spell_correction)"
   ],
   "id": "decdaddece6dce3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:20<00:00, 476.81it/s]\n",
      "100%|██████████| 3000/3000 [00:06<00:00, 461.88it/s]\n",
      "100%|██████████| 2000/2000 [17:28<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:03:55.358551Z",
     "start_time": "2025-01-01T16:03:55.347316Z"
    }
   },
   "cell_type": "code",
   "source": "test_df",
   "id": "348bb7544ec995b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        id  \\\n",
       "0     c1d1b26f-9874-4d8d-96c9-46fea3b04bcc   \n",
       "1     7e4e47d2-79c2-4f9f-a111-bafc565b41ef   \n",
       "2     55325480-747f-4ed3-bd66-7eafaa7f5129   \n",
       "3     dc45f0c0-5b01-4103-bd04-6c7f1454686b   \n",
       "4     e49c3ed9-1f52-4e2b-b734-4ceb47c1d27f   \n",
       "...                                    ...   \n",
       "1995  bc29ffb9-20ac-4c5f-bafd-32b2f62dd5d8   \n",
       "1996  ff21696c-1221-45b7-bd48-99b698405940   \n",
       "1997  37de10e4-c292-43cc-a45a-828c2b071f4a   \n",
       "1998  efe646a3-8bec-45bf-9d17-8d84798f2cdc   \n",
       "1999  9e818afc-60d6-47bc-9b3e-5e672115b1d9   \n",
       "\n",
       "                                                caption  image_id  \\\n",
       "0                               Two cars on the street.    139551   \n",
       "1                               Two cars on the street.     81593   \n",
       "2                                One car on the street.    496031   \n",
       "3                                    A red colored car.    535526   \n",
       "4     A large motor vehicle carrying passengers by r...    251922   \n",
       "...                                                 ...       ...   \n",
       "1995                             One car on the street.    234197   \n",
       "1996                            A green colored banana.     11041   \n",
       "1997                            A green colored banana.    523818   \n",
       "1998                                 A red colored car.    170393   \n",
       "1999                            Two cars on the street.    524249   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0                                two cars on the street  \n",
       "1                                two cars on the street  \n",
       "2                                 one car on the street  \n",
       "3                                     a red colored car  \n",
       "4     a large motor vehicle carrying passengers by r...  \n",
       "...                                                 ...  \n",
       "1995                              one car on the street  \n",
       "1996                             a green colored banana  \n",
       "1997                             a green colored banana  \n",
       "1998                                  a red colored car  \n",
       "1999                             two cars on the street  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>image_id</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c1d1b26f-9874-4d8d-96c9-46fea3b04bcc</td>\n",
       "      <td>Two cars on the street.</td>\n",
       "      <td>139551</td>\n",
       "      <td>two cars on the street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7e4e47d2-79c2-4f9f-a111-bafc565b41ef</td>\n",
       "      <td>Two cars on the street.</td>\n",
       "      <td>81593</td>\n",
       "      <td>two cars on the street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55325480-747f-4ed3-bd66-7eafaa7f5129</td>\n",
       "      <td>One car on the street.</td>\n",
       "      <td>496031</td>\n",
       "      <td>one car on the street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dc45f0c0-5b01-4103-bd04-6c7f1454686b</td>\n",
       "      <td>A red colored car.</td>\n",
       "      <td>535526</td>\n",
       "      <td>a red colored car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e49c3ed9-1f52-4e2b-b734-4ceb47c1d27f</td>\n",
       "      <td>A large motor vehicle carrying passengers by r...</td>\n",
       "      <td>251922</td>\n",
       "      <td>a large motor vehicle carrying passengers by r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>bc29ffb9-20ac-4c5f-bafd-32b2f62dd5d8</td>\n",
       "      <td>One car on the street.</td>\n",
       "      <td>234197</td>\n",
       "      <td>one car on the street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>ff21696c-1221-45b7-bd48-99b698405940</td>\n",
       "      <td>A green colored banana.</td>\n",
       "      <td>11041</td>\n",
       "      <td>a green colored banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>37de10e4-c292-43cc-a45a-828c2b071f4a</td>\n",
       "      <td>A green colored banana.</td>\n",
       "      <td>523818</td>\n",
       "      <td>a green colored banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>efe646a3-8bec-45bf-9d17-8d84798f2cdc</td>\n",
       "      <td>A red colored car.</td>\n",
       "      <td>170393</td>\n",
       "      <td>a red colored car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>9e818afc-60d6-47bc-9b3e-5e672115b1d9</td>\n",
       "      <td>Two cars on the street.</td>\n",
       "      <td>524249</td>\n",
       "      <td>two cars on the street</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:03:55.476562Z",
     "start_time": "2025-01-01T16:03:55.359066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_simple_vocab(sentences, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Manually create a vocabulary from a list of tokenized sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): List of sentences to build vocabulary from.\n",
    "        special_tokens (list of str): Special tokens like <pad>, <unk>.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A vocabulary mapping tokens to indices.\n",
    "        dict: An inverse vocabulary mapping indices to tokens.\n",
    "    \"\"\"\n",
    "    special_tokens = special_tokens or ['<pad>', '<unk>']\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Default index is the current vocab size\n",
    "    for token in special_tokens:\n",
    "        vocab[token]  # Add special tokens first\n",
    "    \n",
    "    # Add tokens from sentences\n",
    "    for sentence in sentences:\n",
    "        for token in sentence.split():\n",
    "            if token.strip():  # Exclude empty tokens\n",
    "                vocab[token]\n",
    "    \n",
    "    # Convert to a normal dict (no longer dynamic)\n",
    "    vocab = dict(vocab)\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    return vocab, inverse_vocab\n",
    "\n",
    "\n",
    "# Vectorize a sentence\n",
    "def vectorize_sentence(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a tensor of token indices using a given vocabulary,\n",
    "    ignoring empty tokens.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "        vocab (Vocab): Vocabulary to map tokens to indices.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Vectorized sentence as a tensor.\n",
    "    \"\"\"\n",
    "    # Ensure '<unk>' exists in the vocabulary\n",
    "    unk_idx = vocab.get('<unk>', -1)\n",
    "    if unk_idx == -1:\n",
    "        raise ValueError(\"The vocabulary must include '<unk>' for unknown tokens.\")\n",
    "    \n",
    "\n",
    "    # Split sentence into tokens and map them to indices\n",
    "    tokens = [token for token in sentence.split() if token.strip()]\n",
    "    return torch.tensor([vocab.get(token, unk_idx) for token in tokens], dtype=torch.long)\n",
    "\n",
    "\n",
    "class GaussianBlurTransform:\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = cv2.GaussianBlur(img, (self.kernel_size, self.kernel_size), 0)\n",
    "        return Image.fromarray(img)\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, images_path, train=True, max_len=None):\n",
    "        \"\"\"\n",
    "        Dataset for preprocessing image-text pairs.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing 'image_id', 'sentence', and optionally 'label'.\n",
    "            vocab (Vocab): Vocabulary for text vectorization.\n",
    "            train (bool): Whether this is a training dataset.\n",
    "            max_len (int): Maximum length for sentences. If None, no truncation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.train = train\n",
    "        self.max_len = max_len\n",
    "        self.images_path = images_path\n",
    "\n",
    "        # Define image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(100, scale=(0.8, 1.0)),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            GaussianBlurTransform(kernel_size=3),\n",
    "            # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Process sentence\n",
    "        sentence = row['caption']\n",
    "        vectorized_sentence = vectorize_sentence(sentence, self.vocab)\n",
    "\n",
    "        # Pad or truncate the sentence\n",
    "        if len(vectorized_sentence) < self.max_len:\n",
    "            padding_length = self.max_len - len(vectorized_sentence)\n",
    "            pad_tensor = torch.full((padding_length,), self.vocab['<pad>'], dtype=torch.long)\n",
    "            vectorized_sentence = torch.cat((vectorized_sentence, pad_tensor), dim=0)\n",
    "        else:\n",
    "            vectorized_sentence = vectorized_sentence[:self.max_len]\n",
    "\n",
    "        # Process image\n",
    "        image_path = f\"{self.images_path}{row['image_id']}.jpg\"\n",
    "            \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
    "            image = self.image_transform(image)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "        # Handle labels (for training)\n",
    "        if self.train:\n",
    "            label = row['label']\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'labels': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'id': row['id']\n",
    "            }"
   ],
   "id": "b120e5be81dc41aa",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:03:55.502787Z",
     "start_time": "2025-01-01T16:03:55.477565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_sentence = [sentence.split(\" \") for sentence in train_df['preprocessed_text']]\n",
    "max_len = max(len([token for token in sentence.split(\" \")]) for sentence in train_df['preprocessed_text'])\n",
    "print(max_len)\n",
    "vocab, inverse_vocab = build_simple_vocab(train_df['preprocessed_text'])"
   ],
   "id": "434eac424e669b66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:03:55.505999Z",
     "start_time": "2025-01-01T16:03:55.503331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = PreprocessingDataset(train_df, vocab, train=True, max_len=max_len, images_path = \"./dataset/train_images/\")\n",
    "test_dataset = PreprocessingDataset(test_df, vocab, train=False, max_len=max_len, images_path = \"./dataset/test_images/\")\n",
    "val_dataset = PreprocessingDataset(validation_df, vocab, train=True, max_len=max_len, images_path = \"./dataset/val_images/\")"
   ],
   "id": "f6a9c5515a874c53",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:03:55.509191Z",
     "start_time": "2025-01-01T16:03:55.506551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "ef7e8ef05ced1727",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:03:55.531340Z",
     "start_time": "2025-01-01T16:03:55.509738Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "51c0d07bf84fb724",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:06:12.495290Z",
     "start_time": "2025-01-01T16:06:12.491626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove the final fully connected layer\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, images):\n",
    "        x = self.resnet(images)\n",
    "        img_features = self.flatten(x)\n",
    "        return img_features\n"
   ],
   "id": "f0cf44b7b652262c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:06:12.852654Z",
     "start_time": "2025-01-01T16:06:12.848635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a matrix of shape (max_len, embedding_dim) for positional encodings\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(float(max_len)) / embedding_dim))\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)  # Shape: (max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Sin for even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Cos for odd indices\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension: (1, max_len, embedding_dim)\n",
    "        self.register_buffer('pe', pe)  # Register as non-learnable buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to the input embeddings.\n",
    "        x: (Batch, SeqLen, EmbeddingDim)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :].to(x.device)"
   ],
   "id": "94a9c21800d85e33",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:06:15.192795Z",
     "start_time": "2025-01-01T16:06:15.188729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextModule(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, transformer_hidden_dim, num_transformer_layers, seq_len, transformer_dropout_value):\n",
    "        super(TextModule, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim=embedding_dim, max_len=seq_len)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=transformer_hidden_dim,\n",
    "                dropout=transformer_dropout_value,\n",
    "                activation='relu'\n",
    "            ),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, captions):\n",
    "        embedded_captions = self.embedding(captions)  # (Batch, SeqLen, EmbeddingDim)\n",
    "        pos_encoded_captions = self.positional_encoding(embedded_captions)  # Apply positional encoding\n",
    "        \n",
    "        # Transformer expects input shape (SeqLen, Batch, EmbeddingDim)\n",
    "        transformer_input = pos_encoded_captions.permute(1, 0, 2)  # (SeqLen, Batch, EmbeddingDim)\n",
    "        transformer_output = self.transformer_encoder(transformer_input)  # (SeqLen, Batch, EmbeddingDim)\n",
    "        text_features = transformer_output.mean(dim=0)  # Mean pooling over SeqLen: (Batch, EmbeddingDim)\n",
    "        return text_features"
   ],
   "id": "e6ea631b4226a0d8",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:06:16.392517Z",
     "start_time": "2025-01-01T16:06:16.388465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, image_feature_dim, text_feature_dim, transformer_dropout_value, num_classes):\n",
    "        super(Head, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_feature_dim + text_feature_dim, 256)  # Combine image + text features\n",
    "        self.dropout = nn.Dropout(transformer_dropout_value)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, img_features, text_features):\n",
    "        combined_features = torch.cat((img_features, text_features), dim=1)  # (Batch, 128 + EmbeddingDim)\n",
    "        x = F.relu(self.fc1(combined_features))  # (Batch, 256)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)  # (Batch, num_classes)\n",
    "        return x.squeeze(1)  # Logits (not probabilities)"
   ],
   "id": "5366b540a2446756",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:07:24.348914Z",
     "start_time": "2025-01-01T16:07:24.345364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImageTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, transformer_hidden_dim, num_transformer_layers, seq_len, cnn_dropout_value, transformer_dropout_value, num_classes=1):\n",
    "        super(ImageTextClassifier, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.text_module = TextModule(\n",
    "            vocab_size, embedding_dim, num_heads,\n",
    "            transformer_hidden_dim, num_transformer_layers,\n",
    "            seq_len, transformer_dropout_value\n",
    "        )\n",
    "        self.head = Head(image_feature_dim=2048, text_feature_dim=embedding_dim, transformer_dropout_value=transformer_dropout_value, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_features = self.cnn(images)  # Image feature extraction\n",
    "        text_features = self.text_module(captions)  # Text feature extraction\n",
    "        return self.head(img_features, text_features)  # Classification"
   ],
   "id": "7c2f52a2b797846b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:07:31.613844Z",
     "start_time": "2025-01-01T16:07:31.609764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.TransformerEncoderLayer):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.ones_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)"
   ],
   "id": "340278a77ea962ef",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:07:33.432259Z",
     "start_time": "2025-01-01T16:07:32.900620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Test model code\n",
    "model = ImageTextClassifier(\n",
    "    vocab_size=10000,  # Example parameters\n",
    "    embedding_dim=128,\n",
    "    num_heads=8,\n",
    "    transformer_hidden_dim=512,\n",
    "    num_transformer_layers=6,\n",
    "    seq_len=max_len,\n",
    "    cnn_dropout_value=0.3,\n",
    "    transformer_dropout_value=0.3,\n",
    "    num_classes=1\n",
    ")\n",
    "\n",
    "# Apply weight initialization recursively\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Dummy input data\n",
    "images = torch.randn(16, 3, 100, 100)  # Batch of 16 RGB images of size 224x224\n",
    "captions = torch.randint(0, len(vocab), (16, max_len))  # Batch of 16 captions with max_len tokens each\n",
    "\n",
    "output = model(images, captions)\n",
    "print(output.shape)  # Should be (16) "
   ],
   "id": "61d242071b2bf57d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:18:33.527931Z",
     "start_time": "2025-01-01T16:18:33.275214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model_config = {\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"embedding_dim\": 512,\n",
    "    \"num_heads\": 8,\n",
    "    \"transformer_hidden_dim\": 512,\n",
    "    \"num_transformer_layers\": 8,\n",
    "    \"seq_len\": max_len,\n",
    "    \"cnn_dropout_value\": 0.5,\n",
    "    \"transformer_dropout_value\": 0.5,\n",
    "}\n",
    "model = ImageTextClassifier(**model_config)\n",
    "model.to(device)\n",
    "# model.apply(initialize_weights)"
   ],
   "id": "b8e49cebdfb0724c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageTextClassifier(\n",
       "  (cnn): CNN(\n",
       "    (resnet): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (text_module): TextModule(\n",
       "    (embedding): Embedding(3662, 512, padding_idx=0)\n",
       "    (positional_encoding): PositionalEncoding()\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Head(\n",
       "    (fc1): Linear(in_features=2560, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:18:33.743217Z",
     "start_time": "2025-01-01T16:18:33.740110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn_params = model.cnn.parameters()  # Parameters of the CNN module\n",
    "text_params = model.text_module.parameters()  # Parameters of the text module\n",
    "head_params = model.head.parameters()\n",
    "\n",
    "cnn_optimizer = optim.AdamW(cnn_params, lr=5e-6, weight_decay=1e-5)\n",
    "text_optimizer = optim.AdamW(text_params, lr=1e-4, weight_decay=1e-3)\n",
    "head_optimizer = optim.AdamW(head_params, lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use logits directly\n"
   ],
   "id": "13ef31cc2a253c3c",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:18:40.063307Z",
     "start_time": "2025-01-01T16:18:40.059716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    cnn_optimizer, \n",
    "    T_0=10,  # Number of epochs before the first restart\n",
    "    T_mult=2,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=5e-7  # Minimum learning rate\n",
    ")\n",
    "\n",
    "text_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    text_optimizer,\n",
    "    T_0=10,  # Number of epochs before the first restart\n",
    "    T_mult=2,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=1e-5  # Minimum learning rate\n",
    ")\n",
    "\n",
    "head_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    head_optimizer,\n",
    "    T_0=10,  # Number of epochs before the first restart\n",
    "    T_mult=2,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=1e-5  # Minimum learning rate\n",
    ")"
   ],
   "id": "2fd4fe8235fc725d",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:18:41.056033Z",
     "start_time": "2025-01-01T16:18:41.046344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "def training_method(model, criterion, cnn_optimizer, text_optimizer, head_optimizer, cnn_scheduler, text_scheduler, head_scheduler, num_epochs, train_loader, val_loader, patience=5, delta = 0.2, loss_procentage_improvement=10):\n",
    "    train_losses = []  # List to store training losses\n",
    "    val_losses = []  # List to store validation losses\n",
    "    val_accuracies = []  # List to store validation accuracies\n",
    "    val_precisions = []  # List to store validation precisions\n",
    "    val_recalls = []  # List to store validation recalls\n",
    "    val_f1s = []  # List to store validation F1-scores\n",
    "    learning_rates = [] # List to store learning rate progression\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    initial_loss = float('inf')\n",
    "    best_model = None  # Store the best model\n",
    "    epochs_without_improvement = 0  # Track epochs without improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        ### TRAINING\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images = batch['images'].to(device)      # Images from batch\n",
    "            captions = batch['captions'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "            \n",
    "            cnn_optimizer.zero_grad()  # Reset gradients\n",
    "            text_optimizer.zero_grad()\n",
    "            head_optimizer.zero_grad()\n",
    "            \n",
    "            output = model(images, captions)  # Forward pass (logits)\n",
    "            loss = criterion(output, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            \n",
    "            cnn_optimizer.step()  # Update weights\n",
    "            text_optimizer.step()\n",
    "            head_optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item()  # Accumulate loss\n",
    "            \n",
    "        train_loss = training_loss / len(train_loader)  # Average training loss\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        ### VALIDATING\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        all_labels = []  # Ground truth labels for validation\n",
    "        all_preds = []   # Predictions for validation\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['images'].to(device)\n",
    "                captions = batch['captions'].to(device)\n",
    "                labels = batch['labels'].to(device).float()\n",
    "                \n",
    "                output = model(images, captions)  # Forward pass (logits)\n",
    "                loss = criterion(output, labels)  # Compute validation loss\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "                # Convert logits to probabilities and apply threshold\n",
    "                preds = (torch.sigmoid(output) > 0.5).float()\n",
    "                \n",
    "                # Store for statistics\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                \n",
    "        val_loss = validation_loss / len(val_loader)  # Average validation loss\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Compute validation statistics\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        val_recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        val_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        \n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        if epoch == 1:\n",
    "            initial_loss = val_loss\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)  # Save the best model\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            print(f\"New best model with Loss: {val_loss:.4f} at epoch {epoch + 1}\")\n",
    "        elif val_loss < best_val_loss + delta:\n",
    "            print(f\"Validation loss did not improve significantly\")            \n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Validation loss did not improve for {epochs_without_improvement} epoch(s).\")\n",
    "            # Stop training if validation loss does not improve for 'patience' epochs\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}. Best Loss: {best_val_loss:.4f}\")\n",
    "                break  # Exit training loop\n",
    "\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        cnn_scheduler.step()\n",
    "        text_scheduler.step()\n",
    "        head_scheduler.step()\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Training Loss: {train_loss:.4f} - \"\n",
    "              f\"Validation Loss: {val_loss:.4f} - \"\n",
    "              f\"Accuracy: {val_accuracy:.4f} - \"\n",
    "              f\"Precision: {val_precision:.4f} - \"\n",
    "              f\"Recall: {val_recall:.4f} - \"\n",
    "              f\"F1 Score: {val_f1:.4f} - \"\n",
    "              f\"Time: {end_time - start_time:.2f}\")\n",
    "\n",
    "    print('Training finished!')\n",
    "    \n",
    "    # save the model only if the best loss is lower than the first initial loss ( to see that the model actually improved with 10% loss )\n",
    "    if best_val_loss < (100 - loss_procentage_improvement) * initial_loss:\n",
    "        # Init plot&model save path\n",
    "        plt_save_path = \"models/\"\n",
    "        model_config['eval_loss'] = best_val_loss\n",
    "        for key, value in model_config.items():\n",
    "            plt_save_path += key + \"=\" + str(value) + \"+\"\n",
    "        plt_save_path = plt_save_path[:-1] + \".png\"\n",
    "        model_path = plt_save_path[:-4] + \".pt\"\n",
    "        \n",
    "        torch.save(best_model.state_dict(), model_path)\n",
    "        print(f\"Best model with Loss: {best_val_loss:.4f} saved.\")\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        # Plotting the losses and validation metrics over epochs\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(val_accuracies, label='Accuracy')\n",
    "        plt.plot(val_precisions, label='Precision')\n",
    "        plt.plot(val_recalls, label='Recall')\n",
    "        plt.plot(val_f1s, label='F1 Score')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Metric')\n",
    "        plt.title('Validation Metrics')\n",
    "        plt.legend()\n",
    "\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plt_save_path)\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(f\"Model wasn't saved because it didn't improve: {loss_procentage_improvement}%\")"
   ],
   "id": "6280acf86aa228a2",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T16:20:44.301430Z",
     "start_time": "2025-01-01T16:18:41.408436Z"
    }
   },
   "cell_type": "code",
   "source": "training_method(model, criterion, cnn_optimizer, text_optimizer, head_optimizer, cnn_scheduler, text_scheduler, head_scheduler, num_epochs=100, train_loader=train_dataloader, val_loader=val_dataloader)",
   "id": "ee2f8b5c78457e56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model with Loss: 0.6845 at epoch 1\n",
      "\n",
      "Epoch 1/100 - Training Loss: 0.6806 - Validation Loss: 0.6845 - Accuracy: 0.5623 - Precision: 0.7367 - Recall: 0.1940 - F1 Score: 0.3071 - Time: 22.96\n",
      "New best model with Loss: 0.6362 at epoch 2\n",
      "\n",
      "Epoch 2/100 - Training Loss: 0.6474 - Validation Loss: 0.6362 - Accuracy: 0.6323 - Precision: 0.6431 - Recall: 0.5947 - F1 Score: 0.6179 - Time: 22.07\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 3/100 - Training Loss: 0.6075 - Validation Loss: 0.6474 - Accuracy: 0.6350 - Precision: 0.6150 - Recall: 0.7220 - F1 Score: 0.6642 - Time: 21.87\n",
      "New best model with Loss: 0.6345 at epoch 4\n",
      "\n",
      "Epoch 4/100 - Training Loss: 0.5529 - Validation Loss: 0.6345 - Accuracy: 0.6577 - Precision: 0.6698 - Recall: 0.6220 - F1 Score: 0.6450 - Time: 21.42\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 5/100 - Training Loss: 0.5019 - Validation Loss: 0.6399 - Accuracy: 0.6437 - Precision: 0.6224 - Recall: 0.7307 - F1 Score: 0.6722 - Time: 23.18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[60], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m training_method(model, criterion, cnn_optimizer, text_optimizer, head_optimizer, cnn_scheduler, text_scheduler, head_scheduler, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, train_loader\u001B[38;5;241m=\u001B[39mtrain_dataloader, val_loader\u001B[38;5;241m=\u001B[39mval_dataloader)\n",
      "Cell \u001B[1;32mIn[59], line 23\u001B[0m, in \u001B[0;36mtraining_method\u001B[1;34m(model, criterion, cnn_optimizer, text_optimizer, head_optimizer, cnn_scheduler, text_scheduler, head_scheduler, num_epochs, train_loader, val_loader, patience, delta, loss_procentage_improvement)\u001B[0m\n\u001B[0;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     21\u001B[0m training_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     24\u001B[0m     images \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)      \u001B[38;5;66;03m# Images from batch\u001B[39;00m\n\u001B[0;32m     25\u001B[0m     captions \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaptions\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Captions from batch\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[8], line 116\u001B[0m, in \u001B[0;36mPreprocessingDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    115\u001B[0m     image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(image_path)\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Convert to RGB\u001B[39;00m\n\u001B[1;32m--> 116\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_transform(image)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage not found at path: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "Cell \u001B[1;32mIn[8], line 61\u001B[0m, in \u001B[0;36mGaussianBlurTransform.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     59\u001B[0m img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(img)\n\u001B[0;32m     60\u001B[0m img \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mGaussianBlur(img, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel_size), \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Image\u001B[38;5;241m.\u001B[39mfromarray(img)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\PIL\\Image.py:3342\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   3339\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrides\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m requires either tobytes() or tostring()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3340\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m-> 3342\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frombuffer(mode, size, obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m, rawmode, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\PIL\\Image.py:3244\u001B[0m, in \u001B[0;36mfrombuffer\u001B[1;34m(mode, size, data, decoder_name, *args)\u001B[0m\n\u001B[0;32m   3241\u001B[0m         im\u001B[38;5;241m.\u001B[39mreadonly \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   3242\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m im\n\u001B[1;32m-> 3244\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frombytes(mode, size, data, decoder_name, args)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\PIL\\Image.py:3171\u001B[0m, in \u001B[0;36mfrombytes\u001B[1;34m(mode, size, data, decoder_name, *args)\u001B[0m\n\u001B[0;32m   3146\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3147\u001B[0m \u001B[38;5;124;03mCreates a copy of an image memory from pixel data in a buffer.\u001B[39;00m\n\u001B[0;32m   3148\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3166\u001B[0m \u001B[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[0;32m   3167\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3169\u001B[0m _check_size(size)\n\u001B[1;32m-> 3171\u001B[0m im \u001B[38;5;241m=\u001B[39m new(mode, size)\n\u001B[0;32m   3172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m im\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   3173\u001B[0m     decoder_args: Any \u001B[38;5;241m=\u001B[39m args\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\PIL\\Image.py:3136\u001B[0m, in \u001B[0;36mnew\u001B[1;34m(mode, size, color)\u001B[0m\n\u001B[0;32m   3134\u001B[0m         im\u001B[38;5;241m.\u001B[39mpalette \u001B[38;5;241m=\u001B[39m ImagePalette\u001B[38;5;241m.\u001B[39mImagePalette()\n\u001B[0;32m   3135\u001B[0m         color \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39mpalette\u001B[38;5;241m.\u001B[39mgetcolor(color_ints)\n\u001B[1;32m-> 3136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m im\u001B[38;5;241m.\u001B[39m_new(core\u001B[38;5;241m.\u001B[39mfill(mode, size, color))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:17:08.325416Z",
     "start_time": "2024-12-23T18:17:08.321854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = []\n",
    "predictions = []\n",
    "\n",
    "def make_submission(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            id = batch['id']\n",
    "            \n",
    "            output = model(images, captions)\n",
    "            preds = (torch.sigmoid(output) > 0.5).int()\n",
    "            \n",
    "            ids.extend(id)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "    df = pd.DataFrame({'id': ids, 'label': predictions})\n",
    "    df.to_csv('submission2.csv', index=False)"
   ],
   "id": "95402ff5c7357c82",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:17:08.741801Z",
     "start_time": "2024-12-23T18:17:08.325925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model_config = {\n",
    "#     \"vocab_size\": len(vocab),\n",
    "#     \"embedding_dim\": 128,\n",
    "#     \"num_heads\": 2,\n",
    "#     \"transformer_hidden_dim\": 256,\n",
    "#     \"num_transformer_layers\": 2,\n",
    "#     \"seq_len\": max_len,\n",
    "#     \"cnn_dropout_value\": 0.4,\n",
    "#     \"transformer_dropout_value\": 0.5,\n",
    "# }\n",
    "# model = ImageTextClassifier(**model_config)\n",
    "# model_path = \"models/vocab_size=3733+embedding_dim=128+num_filters=128+filter_sizes=[3, 4, 5, 6, 7, 8]+seq_len=53+cnn_text_drop_value=0.5+cnn_dropout_value=0.4+transformer_dropout_value=0.5+num_classes=1+eval_loss=0.6212541232717798.pt\"\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=True))"
   ],
   "id": "dd19aec7fc0cdddc",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ImageTextClassifier:\n\tMissing key(s) in state_dict: \"text_module.transformer_encoder.layers.0.self_attn.in_proj_weight\", \"text_module.transformer_encoder.layers.0.self_attn.in_proj_bias\", \"text_module.transformer_encoder.layers.0.self_attn.out_proj.weight\", \"text_module.transformer_encoder.layers.0.self_attn.out_proj.bias\", \"text_module.transformer_encoder.layers.0.linear1.weight\", \"text_module.transformer_encoder.layers.0.linear1.bias\", \"text_module.transformer_encoder.layers.0.linear2.weight\", \"text_module.transformer_encoder.layers.0.linear2.bias\", \"text_module.transformer_encoder.layers.0.norm1.weight\", \"text_module.transformer_encoder.layers.0.norm1.bias\", \"text_module.transformer_encoder.layers.0.norm2.weight\", \"text_module.transformer_encoder.layers.0.norm2.bias\", \"text_module.transformer_encoder.layers.1.self_attn.in_proj_weight\", \"text_module.transformer_encoder.layers.1.self_attn.in_proj_bias\", \"text_module.transformer_encoder.layers.1.self_attn.out_proj.weight\", \"text_module.transformer_encoder.layers.1.self_attn.out_proj.bias\", \"text_module.transformer_encoder.layers.1.linear1.weight\", \"text_module.transformer_encoder.layers.1.linear1.bias\", \"text_module.transformer_encoder.layers.1.linear2.weight\", \"text_module.transformer_encoder.layers.1.linear2.bias\", \"text_module.transformer_encoder.layers.1.norm1.weight\", \"text_module.transformer_encoder.layers.1.norm1.bias\", \"text_module.transformer_encoder.layers.1.norm2.weight\", \"text_module.transformer_encoder.layers.1.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"text_module.conv_layers.0.weight\", \"text_module.conv_layers.0.bias\", \"text_module.conv_layers.1.weight\", \"text_module.conv_layers.1.bias\", \"text_module.conv_layers.2.weight\", \"text_module.conv_layers.2.bias\", \"text_module.conv_layers.3.weight\", \"text_module.conv_layers.3.bias\", \"text_module.conv_layers.4.weight\", \"text_module.conv_layers.4.bias\", \"text_module.conv_layers.5.weight\", \"text_module.conv_layers.5.bias\", \"text_module.conv_layers.6.weight\", \"text_module.conv_layers.6.bias\", \"text_module.fc.weight\", \"text_module.fc.bias\". \n\tsize mismatch for text_module.embedding.weight: copying a param with shape torch.Size([3733, 128]) from checkpoint, the shape in current model is torch.Size([3662, 128]).\n\tsize mismatch for text_module.positional_encoding.pe: copying a param with shape torch.Size([1, 53, 128]) from checkpoint, the shape in current model is torch.Size([1, 45, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m ImageTextClassifier(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_config)\n\u001B[0;32m     12\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/vocab_size=3733+embedding_dim=128+num_filters=128+filter_sizes=[3, 4, 5, 6, 7, 8]+seq_len=53+cnn_text_drop_value=0.5+cnn_dropout_value=0.4+transformer_dropout_value=0.5+num_classes=1+eval_loss=0.6212541232717798.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 13\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(model_path, weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[0;32m   2576\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[0;32m   2577\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   2578\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2579\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[0;32m   2580\u001B[0m             ),\n\u001B[0;32m   2581\u001B[0m         )\n\u001B[0;32m   2583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2584\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   2585\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2586\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[0;32m   2587\u001B[0m         )\n\u001B[0;32m   2588\u001B[0m     )\n\u001B[0;32m   2589\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ImageTextClassifier:\n\tMissing key(s) in state_dict: \"text_module.transformer_encoder.layers.0.self_attn.in_proj_weight\", \"text_module.transformer_encoder.layers.0.self_attn.in_proj_bias\", \"text_module.transformer_encoder.layers.0.self_attn.out_proj.weight\", \"text_module.transformer_encoder.layers.0.self_attn.out_proj.bias\", \"text_module.transformer_encoder.layers.0.linear1.weight\", \"text_module.transformer_encoder.layers.0.linear1.bias\", \"text_module.transformer_encoder.layers.0.linear2.weight\", \"text_module.transformer_encoder.layers.0.linear2.bias\", \"text_module.transformer_encoder.layers.0.norm1.weight\", \"text_module.transformer_encoder.layers.0.norm1.bias\", \"text_module.transformer_encoder.layers.0.norm2.weight\", \"text_module.transformer_encoder.layers.0.norm2.bias\", \"text_module.transformer_encoder.layers.1.self_attn.in_proj_weight\", \"text_module.transformer_encoder.layers.1.self_attn.in_proj_bias\", \"text_module.transformer_encoder.layers.1.self_attn.out_proj.weight\", \"text_module.transformer_encoder.layers.1.self_attn.out_proj.bias\", \"text_module.transformer_encoder.layers.1.linear1.weight\", \"text_module.transformer_encoder.layers.1.linear1.bias\", \"text_module.transformer_encoder.layers.1.linear2.weight\", \"text_module.transformer_encoder.layers.1.linear2.bias\", \"text_module.transformer_encoder.layers.1.norm1.weight\", \"text_module.transformer_encoder.layers.1.norm1.bias\", \"text_module.transformer_encoder.layers.1.norm2.weight\", \"text_module.transformer_encoder.layers.1.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"text_module.conv_layers.0.weight\", \"text_module.conv_layers.0.bias\", \"text_module.conv_layers.1.weight\", \"text_module.conv_layers.1.bias\", \"text_module.conv_layers.2.weight\", \"text_module.conv_layers.2.bias\", \"text_module.conv_layers.3.weight\", \"text_module.conv_layers.3.bias\", \"text_module.conv_layers.4.weight\", \"text_module.conv_layers.4.bias\", \"text_module.conv_layers.5.weight\", \"text_module.conv_layers.5.bias\", \"text_module.conv_layers.6.weight\", \"text_module.conv_layers.6.bias\", \"text_module.fc.weight\", \"text_module.fc.bias\". \n\tsize mismatch for text_module.embedding.weight: copying a param with shape torch.Size([3733, 128]) from checkpoint, the shape in current model is torch.Size([3662, 128]).\n\tsize mismatch for text_module.positional_encoding.pe: copying a param with shape torch.Size([1, 53, 128]) from checkpoint, the shape in current model is torch.Size([1, 45, 128])."
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# make_submission(model, test_dataloader)",
   "id": "6cc9a81e05125a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:17:37.242537Z",
     "start_time": "2024-12-23T18:17:37.237944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def hyperparameter_tuning(vocab_size, max_len, train_loader, val_loader, param_grid, training_method, num_epochs=100):\n",
    "\n",
    "    # Create all combinations of hyperparameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    for params in tqdm(param_combinations):\n",
    "        print(f\"Testing configuration: {params}\")\n",
    "        \n",
    "        try:\n",
    "            # Update model configuration\n",
    "            model_config = {\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"embedding_dim\": params[\"embedding_dim\"],\n",
    "                \"num_heads\": params[\"num_heads\"],\n",
    "                \"transformer_hidden_dim\": params[\"transformer_hidden_dim\"],\n",
    "                \"num_transformer_layers\": params[\"num_transformer_layers\"],\n",
    "                \"seq_len\": max_len,\n",
    "                \"cnn_dropout_value\": params[\"cnn_dropout_value\"],\n",
    "                \"transformer_dropout_value\": params[\"transformer_dropout_value\"],\n",
    "            }\n",
    "            \n",
    "            # Initialize model\n",
    "            model = ImageTextClassifier(**model_config)\n",
    "            model.to(device)\n",
    "            model.apply(initialize_weights)\n",
    "            \n",
    "            # Define criterion, optimizer, and scheduler\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(), \n",
    "                lr=params[\"lr\"], \n",
    "                weight_decay=params[\"weight_decay\"]\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, \n",
    "                T_0=params[\"T_0\"]\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            training_method(\n",
    "                model, criterion, optimizer, scheduler,\n",
    "                num_epochs=num_epochs,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader\n",
    "            )\n",
    "            print(f\"Completed configuration: {params}\")\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error with configuration: {params}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "        \n",
    "        finally:\n",
    "            # Reset GPU memory\n",
    "            print(\"Resetting GPU memory...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()"
   ],
   "id": "cf6ef0fe03c73deb",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:17:37.817401Z",
     "start_time": "2024-12-23T18:17:37.813851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    \"embedding_dim\": [128],\n",
    "    \"num_heads\": [4],\n",
    "    \"transformer_hidden_dim\": [512],\n",
    "    \"num_transformer_layers\": [8],\n",
    "    \"cnn_dropout_value\": [0.5],\n",
    "    \"transformer_dropout_value\": [0.7],\n",
    "    \"lr\": [5e-4, 5e-5],\n",
    "    \"weight_decay\": [1e-3, 1e-4],\n",
    "    \"T_0\": [20],\n",
    "    \"T_mult\": [1],\n",
    "    \"eta_min\": [5e-6],\n",
    "}\n",
    "\n",
    "total_combinations = math.prod(len(values) for values in param_grid.values())\n",
    "print(f\"Total combinations: {total_combinations}\")\n",
    "\n",
    "time_per_epoch = 11  # seconds\n",
    "num_epochs = 100  # epochs per configuration\n",
    "total_time_seconds = total_combinations * time_per_epoch * num_epochs\n",
    "\n",
    "# Convert to hours\n",
    "total_time_hours = total_time_seconds / 3600\n",
    "print(f\"Total time to hyper tune: {total_time_hours} hours\")"
   ],
   "id": "b9f5a5e907d2c63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 4\n",
      "Total time to hyper tune: 1.2222222222222223 hours\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-23T18:17:38.393955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = hyperparameter_tuning(\n",
    "    vocab_size=len(vocab),\n",
    "    max_len=max_len,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    param_grid=param_grid,\n",
    "    training_method=training_method,\n",
    "    num_epochs=100\n",
    ")"
   ],
   "id": "2b980f756d58665b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration: {'embedding_dim': 128, 'num_heads': 4, 'transformer_hidden_dim': 512, 'num_transformer_layers': 8, 'cnn_dropout_value': 0.5, 'transformer_dropout_value': 0.7, 'lr': 0.0005, 'weight_decay': 0.001, 'T_0': 20, 'T_mult': 1, 'eta_min': 5e-06}\n",
      "New best model with Loss: 0.6909 at epoch 1\n",
      "\n",
      "Epoch 1/100 - Training Loss: 0.7056 - Validation Loss: 0.6909 - Accuracy: 0.5247 - Precision: 0.5160 - Recall: 0.7947 - F1 Score: 0.6257 - Time: 12.18 - Lr: 4.97e-04\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 2/100 - Training Loss: 0.6942 - Validation Loss: 0.6956 - Accuracy: 0.5400 - Precision: 0.5826 - Recall: 0.2820 - F1 Score: 0.3801 - Time: 11.47 - Lr: 4.88e-04\n",
      "New best model with Loss: 0.6903 at epoch 3\n",
      "\n",
      "Epoch 3/100 - Training Loss: 0.6905 - Validation Loss: 0.6903 - Accuracy: 0.5293 - Precision: 0.5211 - Recall: 0.7247 - F1 Score: 0.6062 - Time: 11.55 - Lr: 4.73e-04\n",
      "New best model with Loss: 0.6895 at epoch 4\n",
      "\n",
      "Epoch 4/100 - Training Loss: 0.6896 - Validation Loss: 0.6895 - Accuracy: 0.5300 - Precision: 0.5203 - Recall: 0.7673 - F1 Score: 0.6202 - Time: 11.52 - Lr: 4.52e-04\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "dir_models = os.listdir(\"./models\")",
   "id": "b13f77f45721bd1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "best_models = [path[:-3] for path in dir_models if path.endswith(\".pt\")]",
   "id": "d4641d6ec9100cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "best_val_loss = [float(str(best_model.split(\"+\")[-1:]).split(\"=\")[1][:8]) for best_model in best_models]",
   "id": "b66b0b18ddd45eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_loss.sort()\n",
    "best_val_loss[:100]"
   ],
   "id": "f8072a414588fe28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "db3c79d743aaea6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
