{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-23T18:37:29.873057Z",
     "start_time": "2024-12-23T18:37:26.644345Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from torchvision import transforms\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Disable UserWarnings\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:37:29.893805Z",
     "start_time": "2024-12-23T18:37:29.874102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"./dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test.csv\")\n",
    "validation_df = pd.read_csv(\"./dataset/val.csv\")"
   ],
   "id": "68a1a9efa01eeb80",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:37:31.600425Z",
     "start_time": "2024-12-23T18:37:29.893805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "def text_preparation_with_spell_correction(sentence):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z.\n",
    "    pattern = re.compile(r\"[^a-z ]\")\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    # 3. Correct spelling\n",
    "    words = sentence.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is None:\n",
    "            corrected_word = '<unk>'  # Use '<unk>' if no correction found\n",
    "        corrected_words.append(corrected_word)\n",
    "    corrected_sentence = ' '.join(corrected_words)\n",
    "    \n",
    "    return corrected_sentence\n",
    "\n",
    "sentence_example = '\"A domesticated carnivvorous mzammal that typicbally hfaas a lons sfnout, an acxujte sense off osmell, noneetractaaln crlaws, anid xbarkring,y howlingu, or whining rvoiche.\"'\n",
    "print(text_preparation_with_spell_correction(sentence_example))"
   ],
   "id": "963056941ecf496",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a domesticated carnivorous mammal that typically haas a long snout an acute sense off smell <unk> claws and barking y howling or whining voice\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:37:31.621680Z",
     "start_time": "2024-12-23T18:37:31.600930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# Assume 'vocabulary' is a predefined list of words expected in your domain\n",
    "vocabulary = ['example', 'list', 'of', 'words', 'in', 'your', 'vocabulary']\n",
    "\n",
    "\n",
    "def find_closest_word(word, vocabulary):\n",
    "    return min(vocabulary, key=lambda v: levenshtein_distance(word, v))\n",
    "\n",
    "\n",
    "def text_preparation_with_levenshtein(sentence):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z.\n",
    "    pattern = re.compile(r\"[^a-z ]\")\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    # 3. Replace words with the closest in vocabulary\n",
    "    words = sentence.split()\n",
    "    closest_words = [find_closest_word(word, vocabulary) for word in words]\n",
    "    corrected_sentence = ' '.join(closest_words)\n",
    "    \n",
    "    return corrected_sentence"
   ],
   "id": "36c7625d793a331b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:37:31.626234Z",
     "start_time": "2024-12-23T18:37:31.621680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compile the regular expression pattern\n",
    "pattern = re.compile(r\"[^a-z ]\")\n",
    "\n",
    "\n",
    "def text_preparetion_simple(sentence):\n",
    "    # 1. Lowercase everything\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. Remove all symbols other than a-z.\n",
    "    sentence = re.sub(pattern, \" \", sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentence1 = \"A World War II-era bomber flying out of formation\"\n",
    "text_preparetion_simple(sentence1)"
   ],
   "id": "f47e808e14e16d34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a world war ii era bomber flying out of formation'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.001360Z",
     "start_time": "2024-12-23T18:37:31.626741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df['preprocessed_text'] = train_df['caption'].progress_apply(text_preparation_with_spell_correction)\n",
    "validation_df['preprocessed_text'] = validation_df['caption'].progress_apply(text_preparation_with_spell_correction)\n",
    "test_df['preprocessed_text'] = test_df['caption'].progress_apply(text_preparation_with_spell_correction)"
   ],
   "id": "decdaddece6dce3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 475.49it/s]\n",
      "100%|██████████| 3000/3000 [00:06<00:00, 457.69it/s]\n",
      "100%|██████████| 2000/2000 [05:11<00:00,  6.41it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.008965Z",
     "start_time": "2024-12-23T18:43:11.001869Z"
    }
   },
   "cell_type": "code",
   "source": "test_df",
   "id": "348bb7544ec995b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        id  \\\n",
       "0     7f8da4d4-5d37-4b83-afe8-e424bf567813   \n",
       "1     0f50dd85-9e08-4e4b-bab0-35c2ff27a865   \n",
       "2     c4cf245e-33e0-42b2-a04c-7c657f2c537c   \n",
       "3     5b149fa4-a764-49d3-8c5a-9ddf71d37ca7   \n",
       "4     d15c5ddb-dd79-4ce8-bd95-6bae2d005c7c   \n",
       "...                                    ...   \n",
       "1995  1f4535d1-c6e2-4490-a364-a3165944e393   \n",
       "1996  31bf3172-8374-4b18-993f-a8529eb9f900   \n",
       "1997  362645ef-7f02-4f6d-891d-5a5207f0125b   \n",
       "1998  0b57e533-d8a6-4d9b-82f2-88e5faef1a8f   \n",
       "1999  90563013-941c-45fc-a26d-a6efe5b0304a   \n",
       "\n",
       "                                                caption  image_id  \\\n",
       "0                          A red car and a white sheep.     23648   \n",
       "1                 A single clock is sitting on a table.    103430   \n",
       "2     A real life photography of super mario, 8k Ult...    471992   \n",
       "3     A device consisting of a circular canopy of cl...    266533   \n",
       "4     Paying for a quarter-sized pizza with a pizza-...    534923   \n",
       "...                                                 ...       ...   \n",
       "1995     A small blue book sitting on a large red book.    476173   \n",
       "1996                          A yellow colored giraffe.    159627   \n",
       "1997  An elephant is behind a tree. You can see the ...    131115   \n",
       "1998  A domesticated carnivorous mammal that typical...    331137   \n",
       "1999  A ldarge keybord msical instroument lwith a wo...    524525   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0                           a red car and a white sheep  \n",
       "1                  a single clock is sitting on a table  \n",
       "2     a real life photography of super mario k ultra he  \n",
       "3     a device consisting of a circular canopy of cl...  \n",
       "4     paying for a quarter sized pizza with a pizza ...  \n",
       "...                                                 ...  \n",
       "1995      a small blue book sitting on a large red book  \n",
       "1996                           a yellow colored giraffe  \n",
       "1997  an elephant is behind a tree you can see the t...  \n",
       "1998  a domesticated carnivorous mammal that typical...  \n",
       "1999  a large keyboard musical instrument with a wod...  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>image_id</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7f8da4d4-5d37-4b83-afe8-e424bf567813</td>\n",
       "      <td>A red car and a white sheep.</td>\n",
       "      <td>23648</td>\n",
       "      <td>a red car and a white sheep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0f50dd85-9e08-4e4b-bab0-35c2ff27a865</td>\n",
       "      <td>A single clock is sitting on a table.</td>\n",
       "      <td>103430</td>\n",
       "      <td>a single clock is sitting on a table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c4cf245e-33e0-42b2-a04c-7c657f2c537c</td>\n",
       "      <td>A real life photography of super mario, 8k Ult...</td>\n",
       "      <td>471992</td>\n",
       "      <td>a real life photography of super mario k ultra he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5b149fa4-a764-49d3-8c5a-9ddf71d37ca7</td>\n",
       "      <td>A device consisting of a circular canopy of cl...</td>\n",
       "      <td>266533</td>\n",
       "      <td>a device consisting of a circular canopy of cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d15c5ddb-dd79-4ce8-bd95-6bae2d005c7c</td>\n",
       "      <td>Paying for a quarter-sized pizza with a pizza-...</td>\n",
       "      <td>534923</td>\n",
       "      <td>paying for a quarter sized pizza with a pizza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1f4535d1-c6e2-4490-a364-a3165944e393</td>\n",
       "      <td>A small blue book sitting on a large red book.</td>\n",
       "      <td>476173</td>\n",
       "      <td>a small blue book sitting on a large red book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>31bf3172-8374-4b18-993f-a8529eb9f900</td>\n",
       "      <td>A yellow colored giraffe.</td>\n",
       "      <td>159627</td>\n",
       "      <td>a yellow colored giraffe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>362645ef-7f02-4f6d-891d-5a5207f0125b</td>\n",
       "      <td>An elephant is behind a tree. You can see the ...</td>\n",
       "      <td>131115</td>\n",
       "      <td>an elephant is behind a tree you can see the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0b57e533-d8a6-4d9b-82f2-88e5faef1a8f</td>\n",
       "      <td>A domesticated carnivorous mammal that typical...</td>\n",
       "      <td>331137</td>\n",
       "      <td>a domesticated carnivorous mammal that typical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>90563013-941c-45fc-a26d-a6efe5b0304a</td>\n",
       "      <td>A ldarge keybord msical instroument lwith a wo...</td>\n",
       "      <td>524525</td>\n",
       "      <td>a large keyboard musical instrument with a wod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.016041Z",
     "start_time": "2024-12-23T18:43:11.008965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_simple_vocab(sentences, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Manually create a vocabulary from a list of tokenized sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): List of sentences to build vocabulary from.\n",
    "        special_tokens (list of str): Special tokens like <pad>, <unk>.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A vocabulary mapping tokens to indices.\n",
    "        dict: An inverse vocabulary mapping indices to tokens.\n",
    "    \"\"\"\n",
    "    special_tokens = special_tokens or ['<pad>', '<unk>']\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Default index is the current vocab size\n",
    "    for token in special_tokens:\n",
    "        vocab[token]  # Add special tokens first\n",
    "    \n",
    "    # Add tokens from sentences\n",
    "    for sentence in sentences:\n",
    "        for token in sentence.split():\n",
    "            if token.strip():  # Exclude empty tokens\n",
    "                vocab[token]\n",
    "    \n",
    "    # Convert to a normal dict (no longer dynamic)\n",
    "    vocab = dict(vocab)\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    return vocab, inverse_vocab\n",
    "\n",
    "\n",
    "# Vectorize a sentence\n",
    "def vectorize_sentence(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a tensor of token indices using a given vocabulary,\n",
    "    ignoring empty tokens.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "        vocab (Vocab): Vocabulary to map tokens to indices.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Vectorized sentence as a tensor.\n",
    "    \"\"\"\n",
    "    # Ensure '<unk>' exists in the vocabulary\n",
    "    unk_idx = vocab.get('<unk>', -1)\n",
    "    if unk_idx == -1:\n",
    "        raise ValueError(\"The vocabulary must include '<unk>' for unknown tokens.\")\n",
    "    \n",
    "\n",
    "    # Split sentence into tokens and map them to indices\n",
    "    tokens = [token for token in sentence.split() if token.strip()]\n",
    "    return torch.tensor([vocab.get(token, unk_idx) for token in tokens], dtype=torch.long)\n",
    "\n",
    "\n",
    "class GaussianBlurTransform:\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = cv2.GaussianBlur(img, (self.kernel_size, self.kernel_size), 0)\n",
    "        return Image.fromarray(img)\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, images_path, train=True, max_len=None):\n",
    "        \"\"\"\n",
    "        Dataset for preprocessing image-text pairs.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing 'image_id', 'sentence', and optionally 'label'.\n",
    "            vocab (Vocab): Vocabulary for text vectorization.\n",
    "            train (bool): Whether this is a training dataset.\n",
    "            max_len (int): Maximum length for sentences. If None, no truncation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.train = train\n",
    "        self.max_len = max_len\n",
    "        self.images_path = images_path\n",
    "\n",
    "        # Define image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            # transforms.RandomResizedCrop(100, scale=(0.8, 1.0)),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            GaussianBlurTransform(kernel_size=3),\n",
    "            # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Process sentence\n",
    "        sentence = row['caption']\n",
    "        vectorized_sentence = vectorize_sentence(sentence, self.vocab)\n",
    "\n",
    "        # Pad or truncate the sentence\n",
    "        if len(vectorized_sentence) < self.max_len:\n",
    "            padding_length = self.max_len - len(vectorized_sentence)\n",
    "            pad_tensor = torch.full((padding_length,), self.vocab['<pad>'], dtype=torch.long)\n",
    "            vectorized_sentence = torch.cat((vectorized_sentence, pad_tensor), dim=0)\n",
    "        else:\n",
    "            vectorized_sentence = vectorized_sentence[:self.max_len]\n",
    "\n",
    "        # Process image\n",
    "        image_path = f\"{self.images_path}{row['image_id']}.jpg\"\n",
    "            \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
    "            image = self.image_transform(image)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "        # Handle labels (for training)\n",
    "        if self.train:\n",
    "            label = row['label']\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'labels': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'id': row['id']\n",
    "            }"
   ],
   "id": "b120e5be81dc41aa",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.042422Z",
     "start_time": "2024-12-23T18:43:11.016041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_sentence = [sentence.split(\" \") for sentence in train_df['preprocessed_text']]\n",
    "max_len = max(len([token for token in sentence.split(\" \")]) for sentence in train_df['preprocessed_text'])\n",
    "print(max_len)\n",
    "vocab, inverse_vocab = build_simple_vocab(train_df['preprocessed_text'])"
   ],
   "id": "434eac424e669b66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.046981Z",
     "start_time": "2024-12-23T18:43:11.043434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = PreprocessingDataset(train_df, vocab, train=True, max_len=max_len, images_path = \"./dataset/train_images/\")\n",
    "test_dataset = PreprocessingDataset(test_df, vocab, train=False, max_len=max_len, images_path = \"./dataset/test_images/\")\n",
    "val_dataset = PreprocessingDataset(validation_df, vocab, train=True, max_len=max_len, images_path = \"./dataset/val_images/\")"
   ],
   "id": "f6a9c5515a874c53",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.050024Z",
     "start_time": "2024-12-23T18:43:11.047489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "ef7e8ef05ced1727",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.062687Z",
     "start_time": "2024-12-23T18:43:11.050532Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "51c0d07bf84fb724",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.066721Z",
     "start_time": "2024-12-23T18:43:11.062687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_dropout_value):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, images):\n",
    "        img_features = self.layers(images)  # (Batch, 128, 1, 1)\n",
    "        img_features = self.flatten(img_features)  # (Batch, 128)\n",
    "        return img_features\n"
   ],
   "id": "f0cf44b7b652262c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.070803Z",
     "start_time": "2024-12-23T18:43:11.067229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a matrix of shape (max_len, embedding_dim) for positional encodings\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(float(max_len)) / embedding_dim))\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)  # Shape: (max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Sin for even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Cos for odd indices\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension: (1, max_len, embedding_dim)\n",
    "        self.register_buffer('pe', pe)  # Register as non-learnable buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to the input embeddings.\n",
    "        x: (Batch, SeqLen, EmbeddingDim)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :].to(x.device)"
   ],
   "id": "94a9c21800d85e33",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.074338Z",
     "start_time": "2024-12-23T18:43:11.071311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextModule(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, transformer_hidden_dim, num_transformer_layers, seq_len, transformer_dropout_value):\n",
    "        super(TextModule, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim=embedding_dim, max_len=seq_len)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=transformer_hidden_dim,\n",
    "                dropout=transformer_dropout_value,\n",
    "                activation='relu'\n",
    "            ),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, captions):\n",
    "        embedded_captions = self.embedding(captions)  # (Batch, SeqLen, EmbeddingDim)\n",
    "        pos_encoded_captions = self.positional_encoding(embedded_captions)  # Apply positional encoding\n",
    "        \n",
    "        # Transformer expects input shape (SeqLen, Batch, EmbeddingDim)\n",
    "        transformer_input = pos_encoded_captions.permute(1, 0, 2)  # (SeqLen, Batch, EmbeddingDim)\n",
    "        transformer_output = self.transformer_encoder(transformer_input)  # (SeqLen, Batch, EmbeddingDim)\n",
    "        text_features = transformer_output.mean(dim=0)  # Mean pooling over SeqLen: (Batch, EmbeddingDim)\n",
    "        return text_features"
   ],
   "id": "e6ea631b4226a0d8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.077369Z",
     "start_time": "2024-12-23T18:43:11.074843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, image_feature_dim, text_feature_dim, transformer_dropout_value, num_classes):\n",
    "        super(Head, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_feature_dim + text_feature_dim, 256)  # Combine image + text features\n",
    "        self.dropout = nn.Dropout(transformer_dropout_value)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, img_features, text_features):\n",
    "        combined_features = torch.cat((img_features, text_features), dim=1)  # (Batch, 128 + EmbeddingDim)\n",
    "        x = F.relu(self.fc1(combined_features))  # (Batch, 256)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)  # (Batch, num_classes)\n",
    "        return x.squeeze(1)  # Logits (not probabilities)"
   ],
   "id": "5366b540a2446756",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.080406Z",
     "start_time": "2024-12-23T18:43:11.077877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImageTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, transformer_hidden_dim, num_transformer_layers, seq_len, cnn_dropout_value, transformer_dropout_value, num_classes=1):\n",
    "        super(ImageTextClassifier, self).__init__()\n",
    "        self.cnn = CNN(cnn_dropout_value)\n",
    "        self.text_module = TextModule(\n",
    "            vocab_size, embedding_dim, num_heads,\n",
    "            transformer_hidden_dim, num_transformer_layers,\n",
    "            seq_len, transformer_dropout_value\n",
    "        )\n",
    "        self.head = Head(image_feature_dim=128, text_feature_dim=embedding_dim, transformer_dropout_value=transformer_dropout_value, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_features = self.cnn(images)  # Image feature extraction\n",
    "        text_features = self.text_module(captions)  # Text feature extraction\n",
    "        return self.head(img_features, text_features)  # Classification"
   ],
   "id": "7c2f52a2b797846b",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.083960Z",
     "start_time": "2024-12-23T18:43:11.080916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.TransformerEncoderLayer):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.ones_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)"
   ],
   "id": "340278a77ea962ef",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:43:11.180614Z",
     "start_time": "2024-12-23T18:43:11.083960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Test model code\n",
    "model = ImageTextClassifier(\n",
    "    vocab_size=10000,  # Example parameters\n",
    "    embedding_dim=128,\n",
    "    num_heads=8,\n",
    "    transformer_hidden_dim=512,\n",
    "    num_transformer_layers=6,\n",
    "    seq_len=max_len,\n",
    "    cnn_dropout_value=0.3,\n",
    "    transformer_dropout_value=0.3,\n",
    "    num_classes=1\n",
    ")\n",
    "\n",
    "# Apply weight initialization recursively\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Dummy input data\n",
    "images = torch.randn(16, 3, 100, 100)  # Batch of 16 RGB images of size 224x224\n",
    "captions = torch.randint(0, len(vocab), (16, max_len))  # Batch of 16 captions with max_len tokens each\n",
    "\n",
    "output = model(images, captions)\n",
    "print(output.shape)  # Should be (16) "
   ],
   "id": "61d242071b2bf57d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:14.312690Z",
     "start_time": "2024-12-23T20:20:14.286879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model_config = {\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"embedding_dim\": 256,\n",
    "    \"num_heads\": 4,\n",
    "    \"transformer_hidden_dim\": 256,\n",
    "    \"num_transformer_layers\": 8,\n",
    "    \"seq_len\": max_len,\n",
    "    \"cnn_dropout_value\": 0.5,\n",
    "    \"transformer_dropout_value\": 0.5,\n",
    "}\n",
    "model = ImageTextClassifier(**model_config)\n",
    "model.to(device)\n",
    "model.apply(initialize_weights)"
   ],
   "id": "b8e49cebdfb0724c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageTextClassifier(\n",
       "  (cnn): CNN(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU()\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): Dropout(p=0.5, inplace=False)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU()\n",
       "      (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): Dropout(p=0.5, inplace=False)\n",
       "      (14): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (text_module): TextModule(\n",
       "    (embedding): Embedding(3662, 256, padding_idx=0)\n",
       "    (positional_encoding): PositionalEncoding()\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Head(\n",
       "    (fc1): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:14.489903Z",
     "start_time": "2024-12-23T20:20:14.486872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn_params = model.cnn.parameters()  # Parameters of the CNN module\n",
    "text_params = model.text_module.parameters()  # Parameters of the text module\n",
    "head_params = model.head.parameters()\n",
    "\n",
    "cnn_optimizer = optim.AdamW(cnn_params, lr=1e-3, weight_decay=1e-4)\n",
    "text_optimizer = optim.AdamW(text_params, lr=1e-5, weight_decay=1e-5)\n",
    "head_optimizer = optim.AdamW(head_params, lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use logits directly\n"
   ],
   "id": "13ef31cc2a253c3c",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:14.692064Z",
     "start_time": "2024-12-23T20:20:14.689046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    cnn_optimizer, \n",
    "    T_0=5,  # Number of epochs before the first restart\n",
    "    T_mult=2,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=1e-4  # Minimum learning rate\n",
    ")\n",
    "\n",
    "text_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    text_optimizer,\n",
    "    T_0=5,  # Number of epochs before the first restart\n",
    "    T_mult=2,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=1e-5  # Minimum learning rate\n",
    ")\n",
    "\n",
    "head_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    head_optimizer,\n",
    "    T_0=5,  # Number of epochs before the first restart\n",
    "    T_mult=2,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=1e-4  # Minimum learning rate\n",
    ")"
   ],
   "id": "2fd4fe8235fc725d",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:20:14.851443Z",
     "start_time": "2024-12-23T20:20:14.842366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "def training_method(model, criterion, cnn_optimizer, text_optimizer, head_optimizer, cnn_scheduler, text_scheduler, head_scheduler, num_epochs, train_loader, val_loader, patience=5, delta = 0.2, loss_procentage_improvement=10):\n",
    "    train_losses = []  # List to store training losses\n",
    "    val_losses = []  # List to store validation losses\n",
    "    val_accuracies = []  # List to store validation accuracies\n",
    "    val_precisions = []  # List to store validation precisions\n",
    "    val_recalls = []  # List to store validation recalls\n",
    "    val_f1s = []  # List to store validation F1-scores\n",
    "    learning_rates = [] # List to store learning rate progression\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    initial_loss = float('inf')\n",
    "    best_model = None  # Store the best model\n",
    "    epochs_without_improvement = 0  # Track epochs without improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        ### TRAINING\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images = batch['images'].to(device)      # Images from batch\n",
    "            captions = batch['captions'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "            \n",
    "            cnn_optimizer.zero_grad()  # Reset gradients\n",
    "            text_optimizer.zero_grad()\n",
    "            head_optimizer.zero_grad()\n",
    "            \n",
    "            output = model(images, captions)  # Forward pass (logits)\n",
    "            loss = criterion(output, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            \n",
    "            cnn_optimizer.step()  # Update weights\n",
    "            text_optimizer.step()\n",
    "            head_optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item()  # Accumulate loss\n",
    "            \n",
    "        train_loss = training_loss / len(train_loader)  # Average training loss\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        ### VALIDATING\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        all_labels = []  # Ground truth labels for validation\n",
    "        all_preds = []   # Predictions for validation\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['images'].to(device)\n",
    "                captions = batch['captions'].to(device)\n",
    "                labels = batch['labels'].to(device).float()\n",
    "                \n",
    "                output = model(images, captions)  # Forward pass (logits)\n",
    "                loss = criterion(output, labels)  # Compute validation loss\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "                # Convert logits to probabilities and apply threshold\n",
    "                preds = (torch.sigmoid(output) > 0.5).float()\n",
    "                \n",
    "                # Store for statistics\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                \n",
    "        val_loss = validation_loss / len(val_loader)  # Average validation loss\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Compute validation statistics\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        val_recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        val_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        \n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        if epoch == 1:\n",
    "            initial_loss = val_loss\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)  # Save the best model\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            print(f\"New best model with Loss: {val_loss:.4f} at epoch {epoch + 1}\")\n",
    "        elif val_loss < best_val_loss + delta:\n",
    "            print(f\"Validation loss did not improve significantly\")            \n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Validation loss did not improve for {epochs_without_improvement} epoch(s).\")\n",
    "            # Stop training if validation loss does not improve for 'patience' epochs\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}. Best Loss: {best_val_loss:.4f}\")\n",
    "                break  # Exit training loop\n",
    "\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        cnn_scheduler.step()\n",
    "        text_scheduler.step()\n",
    "        head_scheduler.step()\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Training Loss: {train_loss:.4f} - \"\n",
    "              f\"Validation Loss: {val_loss:.4f} - \"\n",
    "              f\"Accuracy: {val_accuracy:.4f} - \"\n",
    "              f\"Precision: {val_precision:.4f} - \"\n",
    "              f\"Recall: {val_recall:.4f} - \"\n",
    "              f\"F1 Score: {val_f1:.4f} - \"\n",
    "              f\"Time: {end_time - start_time:.2f}\")\n",
    "\n",
    "    print('Training finished!')\n",
    "    \n",
    "    # save the model only if the best loss is lower than the first initial loss ( to see that the model actually improved with 10% loss )\n",
    "    if best_val_loss < (100 - loss_procentage_improvement) * initial_loss:\n",
    "        # Init plot&model save path\n",
    "        plt_save_path = \"models/\"\n",
    "        model_config['eval_loss'] = best_val_loss\n",
    "        for key, value in model_config.items():\n",
    "            plt_save_path += key + \"=\" + str(value) + \"+\"\n",
    "        plt_save_path = plt_save_path[:-1] + \".png\"\n",
    "        model_path = plt_save_path[:-4] + \".pt\"\n",
    "        \n",
    "        torch.save(best_model.state_dict(), model_path)\n",
    "        print(f\"Best model with Loss: {best_val_loss:.4f} saved.\")\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        # Plotting the losses and validation metrics over epochs\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(val_accuracies, label='Accuracy')\n",
    "        plt.plot(val_precisions, label='Precision')\n",
    "        plt.plot(val_recalls, label='Recall')\n",
    "        plt.plot(val_f1s, label='F1 Score')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Metric')\n",
    "        plt.title('Validation Metrics')\n",
    "        plt.legend()\n",
    "\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plt_save_path)\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(f\"Model wasn't saved because it didn't improve: {loss_procentage_improvement}%\")"
   ],
   "id": "6280acf86aa228a2",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T20:28:44.640374Z",
     "start_time": "2024-12-23T20:20:16.288352Z"
    }
   },
   "cell_type": "code",
   "source": "training_method(model, criterion, cnn_optimizer, text_optimizer, head_optimizer, cnn_scheduler, text_scheduler, head_scheduler, num_epochs=100, train_loader=train_dataloader, val_loader=val_dataloader)",
   "id": "ee2f8b5c78457e56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model with Loss: 0.7390 at epoch 1\n",
      "\n",
      "Epoch 1/100 - Training Loss: 0.7064 - Validation Loss: 0.7390 - Accuracy: 0.5160 - Precision: 0.6034 - Recall: 0.0933 - F1 Score: 0.1617 - Time: 12.25\n",
      "New best model with Loss: 0.7187 at epoch 2\n",
      "\n",
      "Epoch 2/100 - Training Loss: 0.6919 - Validation Loss: 0.7187 - Accuracy: 0.5143 - Precision: 0.5576 - Recall: 0.1387 - F1 Score: 0.2221 - Time: 12.31\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 3/100 - Training Loss: 0.6890 - Validation Loss: 0.7653 - Accuracy: 0.5133 - Precision: 0.5524 - Recall: 0.1407 - F1 Score: 0.2242 - Time: 12.17\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 4/100 - Training Loss: 0.6864 - Validation Loss: 0.8038 - Accuracy: 0.5103 - Precision: 0.5486 - Recall: 0.1167 - F1 Score: 0.1924 - Time: 12.25\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 5/100 - Training Loss: 0.6845 - Validation Loss: 0.7723 - Accuracy: 0.5203 - Precision: 0.5444 - Recall: 0.2493 - F1 Score: 0.3420 - Time: 12.44\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 6/100 - Training Loss: 0.6865 - Validation Loss: 0.8737 - Accuracy: 0.5117 - Precision: 0.5814 - Recall: 0.0833 - F1 Score: 0.1458 - Time: 12.32\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 7/100 - Training Loss: 0.6851 - Validation Loss: 0.8249 - Accuracy: 0.5213 - Precision: 0.6060 - Recall: 0.1220 - F1 Score: 0.2031 - Time: 12.31\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 8/100 - Training Loss: 0.6833 - Validation Loss: 0.7934 - Accuracy: 0.5217 - Precision: 0.5890 - Recall: 0.1433 - F1 Score: 0.2306 - Time: 12.23\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 9/100 - Training Loss: 0.6820 - Validation Loss: 0.7535 - Accuracy: 0.5137 - Precision: 0.6242 - Recall: 0.0687 - F1 Score: 0.1237 - Time: 12.17\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 10/100 - Training Loss: 0.6792 - Validation Loss: 0.8233 - Accuracy: 0.5253 - Precision: 0.5864 - Recall: 0.1720 - F1 Score: 0.2660 - Time: 12.11\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 11/100 - Training Loss: 0.6795 - Validation Loss: 0.7798 - Accuracy: 0.5223 - Precision: 0.5988 - Recall: 0.1353 - F1 Score: 0.2208 - Time: 12.01\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 12/100 - Training Loss: 0.6775 - Validation Loss: 0.7201 - Accuracy: 0.5467 - Precision: 0.5669 - Recall: 0.3953 - F1 Score: 0.4658 - Time: 12.20\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 13/100 - Training Loss: 0.6760 - Validation Loss: 0.7922 - Accuracy: 0.5310 - Precision: 0.6340 - Recall: 0.1467 - F1 Score: 0.2382 - Time: 12.53\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 14/100 - Training Loss: 0.6714 - Validation Loss: 0.8386 - Accuracy: 0.5303 - Precision: 0.6123 - Recall: 0.1653 - F1 Score: 0.2604 - Time: 12.50\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 15/100 - Training Loss: 0.6731 - Validation Loss: 0.7817 - Accuracy: 0.5323 - Precision: 0.6021 - Recall: 0.1907 - F1 Score: 0.2896 - Time: 12.37\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 16/100 - Training Loss: 0.6780 - Validation Loss: 0.7830 - Accuracy: 0.5310 - Precision: 0.5939 - Recall: 0.1960 - F1 Score: 0.2947 - Time: 12.42\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 17/100 - Training Loss: 0.6792 - Validation Loss: 0.7586 - Accuracy: 0.5240 - Precision: 0.5870 - Recall: 0.1620 - F1 Score: 0.2539 - Time: 12.47\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 18/100 - Training Loss: 0.6777 - Validation Loss: 0.8499 - Accuracy: 0.5267 - Precision: 0.6176 - Recall: 0.1400 - F1 Score: 0.2283 - Time: 12.32\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 19/100 - Training Loss: 0.6765 - Validation Loss: 0.7264 - Accuracy: 0.5563 - Precision: 0.5934 - Recall: 0.3580 - F1 Score: 0.4466 - Time: 12.14\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 20/100 - Training Loss: 0.6749 - Validation Loss: 0.8612 - Accuracy: 0.5197 - Precision: 0.6980 - Recall: 0.0693 - F1 Score: 0.1261 - Time: 12.19\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 21/100 - Training Loss: 0.6749 - Validation Loss: 0.7663 - Accuracy: 0.5363 - Precision: 0.6143 - Recall: 0.1953 - F1 Score: 0.2964 - Time: 12.09\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 22/100 - Training Loss: 0.6728 - Validation Loss: 0.7400 - Accuracy: 0.5450 - Precision: 0.5843 - Recall: 0.3120 - F1 Score: 0.4068 - Time: 12.30\n",
      "New best model with Loss: 0.7020 at epoch 23\n",
      "\n",
      "Epoch 23/100 - Training Loss: 0.6720 - Validation Loss: 0.7020 - Accuracy: 0.5500 - Precision: 0.5830 - Recall: 0.3513 - F1 Score: 0.4384 - Time: 12.24\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 24/100 - Training Loss: 0.6709 - Validation Loss: 0.7226 - Accuracy: 0.5533 - Precision: 0.5935 - Recall: 0.3387 - F1 Score: 0.4312 - Time: 12.07\n",
      "New best model with Loss: 0.6995 at epoch 25\n",
      "\n",
      "Epoch 25/100 - Training Loss: 0.6688 - Validation Loss: 0.6995 - Accuracy: 0.5637 - Precision: 0.5718 - Recall: 0.5073 - F1 Score: 0.5376 - Time: 12.10\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 26/100 - Training Loss: 0.6671 - Validation Loss: 0.7642 - Accuracy: 0.5473 - Precision: 0.5759 - Recall: 0.3593 - F1 Score: 0.4425 - Time: 12.16\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 27/100 - Training Loss: 0.6650 - Validation Loss: 0.8120 - Accuracy: 0.5357 - Precision: 0.6047 - Recall: 0.2060 - F1 Score: 0.3073 - Time: 12.22\n",
      "Validation loss did not improve for 1 epoch(s).\n",
      "\n",
      "Epoch 28/100 - Training Loss: 0.6643 - Validation Loss: 0.9200 - Accuracy: 0.5360 - Precision: 0.6311 - Recall: 0.1733 - F1 Score: 0.2720 - Time: 12.28\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 29/100 - Training Loss: 0.6627 - Validation Loss: 0.8072 - Accuracy: 0.5430 - Precision: 0.5760 - Recall: 0.3260 - F1 Score: 0.4163 - Time: 12.15\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 30/100 - Training Loss: 0.6606 - Validation Loss: 0.7528 - Accuracy: 0.5530 - Precision: 0.5904 - Recall: 0.3460 - F1 Score: 0.4363 - Time: 12.20\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 31/100 - Training Loss: 0.6600 - Validation Loss: 0.7644 - Accuracy: 0.5650 - Precision: 0.5813 - Recall: 0.4647 - F1 Score: 0.5165 - Time: 12.06\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 32/100 - Training Loss: 0.6573 - Validation Loss: 0.8391 - Accuracy: 0.5403 - Precision: 0.5984 - Recall: 0.2453 - F1 Score: 0.3480 - Time: 12.12\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 33/100 - Training Loss: 0.6556 - Validation Loss: 0.8393 - Accuracy: 0.5473 - Precision: 0.5814 - Recall: 0.3380 - F1 Score: 0.4275 - Time: 12.19\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 34/100 - Training Loss: 0.6535 - Validation Loss: 0.7922 - Accuracy: 0.5507 - Precision: 0.5839 - Recall: 0.3527 - F1 Score: 0.4397 - Time: 12.27\n",
      "Validation loss did not improve for 2 epoch(s).\n",
      "\n",
      "Epoch 35/100 - Training Loss: 0.6539 - Validation Loss: 0.9128 - Accuracy: 0.5433 - Precision: 0.6019 - Recall: 0.2560 - F1 Score: 0.3592 - Time: 12.21\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 36/100 - Training Loss: 0.6670 - Validation Loss: 0.7975 - Accuracy: 0.5480 - Precision: 0.5439 - Recall: 0.5947 - F1 Score: 0.5682 - Time: 12.33\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 37/100 - Training Loss: 0.6669 - Validation Loss: 0.7960 - Accuracy: 0.5533 - Precision: 0.6064 - Recall: 0.3040 - F1 Score: 0.4050 - Time: 12.14\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 38/100 - Training Loss: 0.6643 - Validation Loss: 0.7379 - Accuracy: 0.5630 - Precision: 0.5783 - Recall: 0.4653 - F1 Score: 0.5157 - Time: 12.20\n",
      "Validation loss did not improve for 3 epoch(s).\n",
      "\n",
      "Epoch 39/100 - Training Loss: 0.6649 - Validation Loss: 0.9502 - Accuracy: 0.5303 - Precision: 0.6253 - Recall: 0.1513 - F1 Score: 0.2437 - Time: 12.21\n",
      "Validation loss did not improve for 4 epoch(s).\n",
      "\n",
      "Epoch 40/100 - Training Loss: 0.6610 - Validation Loss: 1.1904 - Accuracy: 0.5333 - Precision: 0.6634 - Recall: 0.1353 - F1 Score: 0.2248 - Time: 12.22\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 41/100 - Training Loss: 0.6583 - Validation Loss: 0.8807 - Accuracy: 0.5330 - Precision: 0.6176 - Recall: 0.1733 - F1 Score: 0.2707 - Time: 12.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m training_method(model, criterion, cnn_optimizer, text_optimizer, head_optimizer, cnn_scheduler, text_scheduler, head_scheduler, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, train_loader\u001B[38;5;241m=\u001B[39mtrain_dataloader, val_loader\u001B[38;5;241m=\u001B[39mval_dataloader)\n",
      "Cell \u001B[1;32mIn[51], line 23\u001B[0m, in \u001B[0;36mtraining_method\u001B[1;34m(model, criterion, cnn_optimizer, text_optimizer, head_optimizer, cnn_scheduler, text_scheduler, head_scheduler, num_epochs, train_loader, val_loader, patience, delta, loss_procentage_improvement)\u001B[0m\n\u001B[0;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     21\u001B[0m training_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     24\u001B[0m     images \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)      \u001B[38;5;66;03m# Images from batch\u001B[39;00m\n\u001B[0;32m     25\u001B[0m     captions \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaptions\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Captions from batch\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[9], line 115\u001B[0m, in \u001B[0;36mPreprocessingDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    112\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimages_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mrow[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 115\u001B[0m     image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(image_path)\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Convert to RGB\u001B[39;00m\n\u001B[0;32m    116\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_transform(image)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\PIL\\Image.py:993\u001B[0m, in \u001B[0;36mImage.convert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m    990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;15\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;16\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;24\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    991\u001B[0m     deprecate(mode, \u001B[38;5;241m12\u001B[39m)\n\u001B[1;32m--> 993\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload()\n\u001B[0;32m    995\u001B[0m has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\n\u001B[0;32m    996\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    997\u001B[0m     \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    297\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[0;32m    299\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 300\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m decoder\u001B[38;5;241m.\u001B[39mdecode(b)\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:17:08.325416Z",
     "start_time": "2024-12-23T18:17:08.321854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = []\n",
    "predictions = []\n",
    "\n",
    "def make_submission(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            id = batch['id']\n",
    "            \n",
    "            output = model(images, captions)\n",
    "            preds = (torch.sigmoid(output) > 0.5).int()\n",
    "            \n",
    "            ids.extend(id)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "    df = pd.DataFrame({'id': ids, 'label': predictions})\n",
    "    df.to_csv('submission2.csv', index=False)"
   ],
   "id": "95402ff5c7357c82",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:17:08.741801Z",
     "start_time": "2024-12-23T18:17:08.325925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model_config = {\n",
    "#     \"vocab_size\": len(vocab),\n",
    "#     \"embedding_dim\": 128,\n",
    "#     \"num_heads\": 2,\n",
    "#     \"transformer_hidden_dim\": 256,\n",
    "#     \"num_transformer_layers\": 2,\n",
    "#     \"seq_len\": max_len,\n",
    "#     \"cnn_dropout_value\": 0.4,\n",
    "#     \"transformer_dropout_value\": 0.5,\n",
    "# }\n",
    "# model = ImageTextClassifier(**model_config)\n",
    "# model_path = \"models/vocab_size=3733+embedding_dim=128+num_filters=128+filter_sizes=[3, 4, 5, 6, 7, 8]+seq_len=53+cnn_text_drop_value=0.5+cnn_dropout_value=0.4+transformer_dropout_value=0.5+num_classes=1+eval_loss=0.6212541232717798.pt\"\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=True))"
   ],
   "id": "dd19aec7fc0cdddc",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ImageTextClassifier:\n\tMissing key(s) in state_dict: \"text_module.transformer_encoder.layers.0.self_attn.in_proj_weight\", \"text_module.transformer_encoder.layers.0.self_attn.in_proj_bias\", \"text_module.transformer_encoder.layers.0.self_attn.out_proj.weight\", \"text_module.transformer_encoder.layers.0.self_attn.out_proj.bias\", \"text_module.transformer_encoder.layers.0.linear1.weight\", \"text_module.transformer_encoder.layers.0.linear1.bias\", \"text_module.transformer_encoder.layers.0.linear2.weight\", \"text_module.transformer_encoder.layers.0.linear2.bias\", \"text_module.transformer_encoder.layers.0.norm1.weight\", \"text_module.transformer_encoder.layers.0.norm1.bias\", \"text_module.transformer_encoder.layers.0.norm2.weight\", \"text_module.transformer_encoder.layers.0.norm2.bias\", \"text_module.transformer_encoder.layers.1.self_attn.in_proj_weight\", \"text_module.transformer_encoder.layers.1.self_attn.in_proj_bias\", \"text_module.transformer_encoder.layers.1.self_attn.out_proj.weight\", \"text_module.transformer_encoder.layers.1.self_attn.out_proj.bias\", \"text_module.transformer_encoder.layers.1.linear1.weight\", \"text_module.transformer_encoder.layers.1.linear1.bias\", \"text_module.transformer_encoder.layers.1.linear2.weight\", \"text_module.transformer_encoder.layers.1.linear2.bias\", \"text_module.transformer_encoder.layers.1.norm1.weight\", \"text_module.transformer_encoder.layers.1.norm1.bias\", \"text_module.transformer_encoder.layers.1.norm2.weight\", \"text_module.transformer_encoder.layers.1.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"text_module.conv_layers.0.weight\", \"text_module.conv_layers.0.bias\", \"text_module.conv_layers.1.weight\", \"text_module.conv_layers.1.bias\", \"text_module.conv_layers.2.weight\", \"text_module.conv_layers.2.bias\", \"text_module.conv_layers.3.weight\", \"text_module.conv_layers.3.bias\", \"text_module.conv_layers.4.weight\", \"text_module.conv_layers.4.bias\", \"text_module.conv_layers.5.weight\", \"text_module.conv_layers.5.bias\", \"text_module.conv_layers.6.weight\", \"text_module.conv_layers.6.bias\", \"text_module.fc.weight\", \"text_module.fc.bias\". \n\tsize mismatch for text_module.embedding.weight: copying a param with shape torch.Size([3733, 128]) from checkpoint, the shape in current model is torch.Size([3662, 128]).\n\tsize mismatch for text_module.positional_encoding.pe: copying a param with shape torch.Size([1, 53, 128]) from checkpoint, the shape in current model is torch.Size([1, 45, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m ImageTextClassifier(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_config)\n\u001B[0;32m     12\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/vocab_size=3733+embedding_dim=128+num_filters=128+filter_sizes=[3, 4, 5, 6, 7, 8]+seq_len=53+cnn_text_drop_value=0.5+cnn_dropout_value=0.4+transformer_dropout_value=0.5+num_classes=1+eval_loss=0.6212541232717798.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 13\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(model_path, weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AI\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[0;32m   2576\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[0;32m   2577\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   2578\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2579\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[0;32m   2580\u001B[0m             ),\n\u001B[0;32m   2581\u001B[0m         )\n\u001B[0;32m   2583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2584\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   2585\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2586\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[0;32m   2587\u001B[0m         )\n\u001B[0;32m   2588\u001B[0m     )\n\u001B[0;32m   2589\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ImageTextClassifier:\n\tMissing key(s) in state_dict: \"text_module.transformer_encoder.layers.0.self_attn.in_proj_weight\", \"text_module.transformer_encoder.layers.0.self_attn.in_proj_bias\", \"text_module.transformer_encoder.layers.0.self_attn.out_proj.weight\", \"text_module.transformer_encoder.layers.0.self_attn.out_proj.bias\", \"text_module.transformer_encoder.layers.0.linear1.weight\", \"text_module.transformer_encoder.layers.0.linear1.bias\", \"text_module.transformer_encoder.layers.0.linear2.weight\", \"text_module.transformer_encoder.layers.0.linear2.bias\", \"text_module.transformer_encoder.layers.0.norm1.weight\", \"text_module.transformer_encoder.layers.0.norm1.bias\", \"text_module.transformer_encoder.layers.0.norm2.weight\", \"text_module.transformer_encoder.layers.0.norm2.bias\", \"text_module.transformer_encoder.layers.1.self_attn.in_proj_weight\", \"text_module.transformer_encoder.layers.1.self_attn.in_proj_bias\", \"text_module.transformer_encoder.layers.1.self_attn.out_proj.weight\", \"text_module.transformer_encoder.layers.1.self_attn.out_proj.bias\", \"text_module.transformer_encoder.layers.1.linear1.weight\", \"text_module.transformer_encoder.layers.1.linear1.bias\", \"text_module.transformer_encoder.layers.1.linear2.weight\", \"text_module.transformer_encoder.layers.1.linear2.bias\", \"text_module.transformer_encoder.layers.1.norm1.weight\", \"text_module.transformer_encoder.layers.1.norm1.bias\", \"text_module.transformer_encoder.layers.1.norm2.weight\", \"text_module.transformer_encoder.layers.1.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"text_module.conv_layers.0.weight\", \"text_module.conv_layers.0.bias\", \"text_module.conv_layers.1.weight\", \"text_module.conv_layers.1.bias\", \"text_module.conv_layers.2.weight\", \"text_module.conv_layers.2.bias\", \"text_module.conv_layers.3.weight\", \"text_module.conv_layers.3.bias\", \"text_module.conv_layers.4.weight\", \"text_module.conv_layers.4.bias\", \"text_module.conv_layers.5.weight\", \"text_module.conv_layers.5.bias\", \"text_module.conv_layers.6.weight\", \"text_module.conv_layers.6.bias\", \"text_module.fc.weight\", \"text_module.fc.bias\". \n\tsize mismatch for text_module.embedding.weight: copying a param with shape torch.Size([3733, 128]) from checkpoint, the shape in current model is torch.Size([3662, 128]).\n\tsize mismatch for text_module.positional_encoding.pe: copying a param with shape torch.Size([1, 53, 128]) from checkpoint, the shape in current model is torch.Size([1, 45, 128])."
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# make_submission(model, test_dataloader)",
   "id": "6cc9a81e05125a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:17:37.242537Z",
     "start_time": "2024-12-23T18:17:37.237944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def hyperparameter_tuning(vocab_size, max_len, train_loader, val_loader, param_grid, training_method, num_epochs=100):\n",
    "\n",
    "    # Create all combinations of hyperparameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    \n",
    "    for params in tqdm(param_combinations):\n",
    "        print(f\"Testing configuration: {params}\")\n",
    "        \n",
    "        try:\n",
    "            # Update model configuration\n",
    "            model_config = {\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"embedding_dim\": params[\"embedding_dim\"],\n",
    "                \"num_heads\": params[\"num_heads\"],\n",
    "                \"transformer_hidden_dim\": params[\"transformer_hidden_dim\"],\n",
    "                \"num_transformer_layers\": params[\"num_transformer_layers\"],\n",
    "                \"seq_len\": max_len,\n",
    "                \"cnn_dropout_value\": params[\"cnn_dropout_value\"],\n",
    "                \"transformer_dropout_value\": params[\"transformer_dropout_value\"],\n",
    "            }\n",
    "            \n",
    "            # Initialize model\n",
    "            model = ImageTextClassifier(**model_config)\n",
    "            model.to(device)\n",
    "            model.apply(initialize_weights)\n",
    "            \n",
    "            # Define criterion, optimizer, and scheduler\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(), \n",
    "                lr=params[\"lr\"], \n",
    "                weight_decay=params[\"weight_decay\"]\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, \n",
    "                T_0=params[\"T_0\"]\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            training_method(\n",
    "                model, criterion, optimizer, scheduler,\n",
    "                num_epochs=num_epochs,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader\n",
    "            )\n",
    "            print(f\"Completed configuration: {params}\")\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error with configuration: {params}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "        \n",
    "        finally:\n",
    "            # Reset GPU memory\n",
    "            print(\"Resetting GPU memory...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()"
   ],
   "id": "cf6ef0fe03c73deb",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:17:37.817401Z",
     "start_time": "2024-12-23T18:17:37.813851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    \"embedding_dim\": [128],\n",
    "    \"num_heads\": [4],\n",
    "    \"transformer_hidden_dim\": [512],\n",
    "    \"num_transformer_layers\": [8],\n",
    "    \"cnn_dropout_value\": [0.5],\n",
    "    \"transformer_dropout_value\": [0.7],\n",
    "    \"lr\": [5e-4, 5e-5],\n",
    "    \"weight_decay\": [1e-3, 1e-4],\n",
    "    \"T_0\": [20],\n",
    "    \"T_mult\": [1],\n",
    "    \"eta_min\": [5e-6],\n",
    "}\n",
    "\n",
    "total_combinations = math.prod(len(values) for values in param_grid.values())\n",
    "print(f\"Total combinations: {total_combinations}\")\n",
    "\n",
    "time_per_epoch = 11  # seconds\n",
    "num_epochs = 100  # epochs per configuration\n",
    "total_time_seconds = total_combinations * time_per_epoch * num_epochs\n",
    "\n",
    "# Convert to hours\n",
    "total_time_hours = total_time_seconds / 3600\n",
    "print(f\"Total time to hyper tune: {total_time_hours} hours\")"
   ],
   "id": "b9f5a5e907d2c63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 4\n",
      "Total time to hyper tune: 1.2222222222222223 hours\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-23T18:17:38.393955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = hyperparameter_tuning(\n",
    "    vocab_size=len(vocab),\n",
    "    max_len=max_len,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    param_grid=param_grid,\n",
    "    training_method=training_method,\n",
    "    num_epochs=100\n",
    ")"
   ],
   "id": "2b980f756d58665b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration: {'embedding_dim': 128, 'num_heads': 4, 'transformer_hidden_dim': 512, 'num_transformer_layers': 8, 'cnn_dropout_value': 0.5, 'transformer_dropout_value': 0.7, 'lr': 0.0005, 'weight_decay': 0.001, 'T_0': 20, 'T_mult': 1, 'eta_min': 5e-06}\n",
      "New best model with Loss: 0.6909 at epoch 1\n",
      "\n",
      "Epoch 1/100 - Training Loss: 0.7056 - Validation Loss: 0.6909 - Accuracy: 0.5247 - Precision: 0.5160 - Recall: 0.7947 - F1 Score: 0.6257 - Time: 12.18 - Lr: 4.97e-04\n",
      "Validation loss did not improve significantly\n",
      "\n",
      "Epoch 2/100 - Training Loss: 0.6942 - Validation Loss: 0.6956 - Accuracy: 0.5400 - Precision: 0.5826 - Recall: 0.2820 - F1 Score: 0.3801 - Time: 11.47 - Lr: 4.88e-04\n",
      "New best model with Loss: 0.6903 at epoch 3\n",
      "\n",
      "Epoch 3/100 - Training Loss: 0.6905 - Validation Loss: 0.6903 - Accuracy: 0.5293 - Precision: 0.5211 - Recall: 0.7247 - F1 Score: 0.6062 - Time: 11.55 - Lr: 4.73e-04\n",
      "New best model with Loss: 0.6895 at epoch 4\n",
      "\n",
      "Epoch 4/100 - Training Loss: 0.6896 - Validation Loss: 0.6895 - Accuracy: 0.5300 - Precision: 0.5203 - Recall: 0.7673 - F1 Score: 0.6202 - Time: 11.52 - Lr: 4.52e-04\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "dir_models = os.listdir(\"./models\")",
   "id": "b13f77f45721bd1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "best_models = [path[:-3] for path in dir_models if path.endswith(\".pt\")]",
   "id": "d4641d6ec9100cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "best_val_loss = [float(str(best_model.split(\"+\")[-1:]).split(\"=\")[1][:8]) for best_model in best_models]",
   "id": "b66b0b18ddd45eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_loss.sort()\n",
    "best_val_loss[:100]"
   ],
   "id": "f8072a414588fe28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "db3c79d743aaea6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
