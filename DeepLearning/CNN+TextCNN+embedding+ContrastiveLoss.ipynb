{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:56.439994Z",
     "start_time": "2025-01-10T14:34:56.435939Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from torchvision import transforms\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from textblob import TextBlob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Disable UserWarnings\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:56.532718Z",
     "start_time": "2025-01-10T14:34:56.511361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"./dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"./dataset/test.csv\")\n",
    "validation_df = pd.read_csv(\"./dataset/val.csv\")"
   ],
   "id": "68a1a9efa01eeb80",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:56.569757Z",
     "start_time": "2025-01-10T14:34:56.533732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import text_preprocessing\n",
    "\n",
    "train_df['preprocessed_text'] = train_df['caption'].progress_apply(text_preprocessing.text_preparetion_simple)\n",
    "validation_df['preprocessed_text'] = validation_df['caption'].progress_apply(text_preprocessing.text_preparetion_simple)\n",
    "test_df['preprocessed_text'] = test_df['caption'].progress_apply(text_preprocessing.text_preparetion_simple)"
   ],
   "id": "decdaddece6dce3d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 531934.56it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 455457.05it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 492809.78it/s]\n"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:56.575810Z",
     "start_time": "2025-01-10T14:34:56.569757Z"
    }
   },
   "cell_type": "code",
   "source": "train_df",
   "id": "348bb7544ec995b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        id  \\\n",
       "0     417812c5-0ce4-499d-b97d-4d28827239bc   \n",
       "1     5ac91fa3-55f2-4cb3-8c8f-ad84f78e6b36   \n",
       "2     d2705b90-8347-4cab-a7a6-654540d9a489   \n",
       "3     a3b33fe7-3085-4433-9c18-8814803891b4   \n",
       "4     1514b0e4-0665-45bc-ab32-52fce326cc29   \n",
       "...                                    ...   \n",
       "9995  1d1df243-485d-4b29-82c8-7e34c0de1f5c   \n",
       "9996  f7dfa883-e524-4974-b5ba-6b3c3db49087   \n",
       "9997  602e83dc-6539-4c1a-8d19-c1481b5c24bf   \n",
       "9998  d9ce2e8c-0831-466a-8756-4c40d772b1ce   \n",
       "9999  b22cdca0-79b3-4b37-a173-5176a32096f6   \n",
       "\n",
       "                                                caption  image_id  label  \\\n",
       "0     Wet elephants shake water onto people bathing ...    394330      0   \n",
       "1          Two men holding tennis racquets on the court    130849      0   \n",
       "2     A bird on a tree limb with mountains in the ba...    514790      0   \n",
       "3     A kitchen and dining room are featured along w...    182096      0   \n",
       "4        A fruit stand has various fruits on the table.     68788      1   \n",
       "...                                                 ...       ...    ...   \n",
       "9995     Several people stand in a field flying a kite.    522702      0   \n",
       "9996       A batter hitting a pitch at a baseball game.    441874      1   \n",
       "9997  A person on white surfboard next to group in a...    166716      0   \n",
       "9998  A baseball player getting ready to swing at th...    517601      0   \n",
       "9999                    a black cat is laying in a sink    394115      1   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0     wet elephants shake water onto people bathing ...  \n",
       "1          two men holding tennis racquets on the court  \n",
       "2     a bird on a tree limb with mountains in the ba...  \n",
       "3     a kitchen and dining room are featured along w...  \n",
       "4         a fruit stand has various fruits on the table  \n",
       "...                                                 ...  \n",
       "9995      several people stand in a field flying a kite  \n",
       "9996        a batter hitting a pitch at a baseball game  \n",
       "9997  a person on white surfboard next to group in a...  \n",
       "9998  a baseball player getting ready to swing at th...  \n",
       "9999                    a black cat is laying in a sink  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417812c5-0ce4-499d-b97d-4d28827239bc</td>\n",
       "      <td>Wet elephants shake water onto people bathing ...</td>\n",
       "      <td>394330</td>\n",
       "      <td>0</td>\n",
       "      <td>wet elephants shake water onto people bathing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ac91fa3-55f2-4cb3-8c8f-ad84f78e6b36</td>\n",
       "      <td>Two men holding tennis racquets on the court</td>\n",
       "      <td>130849</td>\n",
       "      <td>0</td>\n",
       "      <td>two men holding tennis racquets on the court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d2705b90-8347-4cab-a7a6-654540d9a489</td>\n",
       "      <td>A bird on a tree limb with mountains in the ba...</td>\n",
       "      <td>514790</td>\n",
       "      <td>0</td>\n",
       "      <td>a bird on a tree limb with mountains in the ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a3b33fe7-3085-4433-9c18-8814803891b4</td>\n",
       "      <td>A kitchen and dining room are featured along w...</td>\n",
       "      <td>182096</td>\n",
       "      <td>0</td>\n",
       "      <td>a kitchen and dining room are featured along w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514b0e4-0665-45bc-ab32-52fce326cc29</td>\n",
       "      <td>A fruit stand has various fruits on the table.</td>\n",
       "      <td>68788</td>\n",
       "      <td>1</td>\n",
       "      <td>a fruit stand has various fruits on the table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1d1df243-485d-4b29-82c8-7e34c0de1f5c</td>\n",
       "      <td>Several people stand in a field flying a kite.</td>\n",
       "      <td>522702</td>\n",
       "      <td>0</td>\n",
       "      <td>several people stand in a field flying a kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>f7dfa883-e524-4974-b5ba-6b3c3db49087</td>\n",
       "      <td>A batter hitting a pitch at a baseball game.</td>\n",
       "      <td>441874</td>\n",
       "      <td>1</td>\n",
       "      <td>a batter hitting a pitch at a baseball game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>602e83dc-6539-4c1a-8d19-c1481b5c24bf</td>\n",
       "      <td>A person on white surfboard next to group in a...</td>\n",
       "      <td>166716</td>\n",
       "      <td>0</td>\n",
       "      <td>a person on white surfboard next to group in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>d9ce2e8c-0831-466a-8756-4c40d772b1ce</td>\n",
       "      <td>A baseball player getting ready to swing at th...</td>\n",
       "      <td>517601</td>\n",
       "      <td>0</td>\n",
       "      <td>a baseball player getting ready to swing at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>b22cdca0-79b3-4b37-a173-5176a32096f6</td>\n",
       "      <td>a black cat is laying in a sink</td>\n",
       "      <td>394115</td>\n",
       "      <td>1</td>\n",
       "      <td>a black cat is laying in a sink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:56.584926Z",
     "start_time": "2025-01-10T14:34:56.576317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df\n",
    "\n",
    "\n",
    "def build_simple_vocab(sentences, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Manually create a vocabulary from a list of tokenized sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): List of sentences to build vocabulary from.\n",
    "        special_tokens (list of str): Special tokens like <pad>, <unk>.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A vocabulary mapping tokens to indices.\n",
    "        dict: An inverse vocabulary mapping indices to tokens.\n",
    "    \"\"\"\n",
    "    special_tokens = special_tokens or ['<pad>', '<unk>']\n",
    "    vocab = defaultdict(lambda: len(vocab))  # Default index is the current vocab size\n",
    "    for token in special_tokens:\n",
    "        vocab[token]  # Add special tokens first\n",
    "\n",
    "    # Add tokens from sentences\n",
    "    for sentence in sentences:\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            if token.strip():  # Exclude empty tokens\n",
    "                vocab[token]\n",
    "\n",
    "    # Convert to a normal dict (no longer dynamic)\n",
    "    vocab = dict(vocab)\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    return vocab, inverse_vocab\n",
    "\n",
    "\n",
    "# Vectorize a sentence\n",
    "def vectorize_sentence(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a tensor of token indices using a given vocabulary,\n",
    "    ignoring empty tokens.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "        vocab (Vocab): Vocabulary to map tokens to indices.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Vectorized sentence as a tensor.\n",
    "    \"\"\"\n",
    "    # Ensure '<unk>' exists in the vocabulary\n",
    "    unk_idx = vocab.get('<unk>', -1)\n",
    "    if unk_idx == -1:\n",
    "        raise ValueError(\"The vocabulary must include '<unk>' for unknown tokens.\")\n",
    "\n",
    "    # Split sentence into tokens and map them to indices\n",
    "    tokens = [token for token in sentence.split() if token.strip()]\n",
    "    return torch.tensor([vocab.get(token, unk_idx) for token in tokens], dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class PreprocessingDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, images_path, train=True, max_len=None, augmentation_prob=0.3):\n",
    "        \"\"\"\n",
    "        Dataset for preprocessing image-text pairs with TF-IDF vectorization.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing 'image_id', 'sentence', and optionally 'label'.\n",
    "            vectorizer (TfidfVectorizer): TF-IDF vectorizer for text.\n",
    "            images_path (str): Base path to the images.\n",
    "            train (bool): Whether this is a training dataset.\n",
    "            max_len (int): Maximum length for sentences in terms of features. Truncation isn't typical with TF-IDF.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.train = train\n",
    "        self.max_len = max_len\n",
    "        self.images_path = images_path\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "        # Define image transformations\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(100, scale=(0.8, 1.0)),\n",
    "            # transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Process sentence\n",
    "        sentence = row['caption']\n",
    "        if random.random() < self.augmentation_prob:\n",
    "            # sentence = self.augment_text(sentence)\n",
    "            sentence = self.synonym_replacement(sentence)\n",
    "        vectorized_sentence = vectorize_sentence(sentence, self.vocab)\n",
    "\n",
    "        # Pad or truncate the sentence\n",
    "        if len(vectorized_sentence) < self.max_len:\n",
    "            padding_length = self.max_len - len(vectorized_sentence)\n",
    "            pad_tensor = torch.full((padding_length,), self.vocab['<pad>'], dtype=torch.long)\n",
    "            vectorized_sentence = torch.cat((vectorized_sentence, pad_tensor), dim=0)\n",
    "        else:\n",
    "            vectorized_sentence = vectorized_sentence[:self.max_len]\n",
    "\n",
    "        # Process image\n",
    "        image_path = f\"{self.images_path}{row['image_id']}.jpg\"\n",
    "            \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Convert to RGB\n",
    "            image = self.image_transform(image)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "        # Handle labels (for training)\n",
    "        if self.train:\n",
    "            label = row['label']\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'labels': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'images': image,\n",
    "                'captions': vectorized_sentence,\n",
    "                'id': row['id']\n",
    "            }\n",
    "\n",
    "    def augment_text(self, text):\n",
    "        \"\"\" Augment text using synonym replacement and rephrasing. \"\"\"\n",
    "        if random.random() < 0.5:\n",
    "            return self.synonym_replacement(text)\n",
    "        else:\n",
    "            return self.rephrase_text(text)\n",
    "\n",
    "\n",
    "    def synonym_replacement(self, text):\n",
    "        # Tokenize the sentence\n",
    "        words = nltk.word_tokenize(text)\n",
    "        new_words = words.copy()\n",
    "    \n",
    "        # Find indices of words that can be replaced\n",
    "        replaceable = [i for i, word in enumerate(words) if wordnet.synsets(word)]\n",
    "        \n",
    "        # Randomly choose half of these words to replace\n",
    "        num_to_replace = len(replaceable) // 2\n",
    "        chosen_indices = random.sample(replaceable, num_to_replace)\n",
    "    \n",
    "        # Replace chosen words with synonyms\n",
    "        for i in chosen_indices:\n",
    "            synsets = wordnet.synsets(words[i])\n",
    "            if synsets:\n",
    "                # Choose a random synonym from the first synset\n",
    "                synonyms = list(set([lemma.name() for lemma in synsets[0].lemmas() if lemma.name() != words[i]]))\n",
    "                if synonyms:\n",
    "                    new_words[i] = random.choice(synonyms).replace('_', ' ')\n",
    "\n",
    "        # Reconstruct the sentence\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "\n",
    "    def rephrase_text(self, text):\n",
    "        blob = TextBlob(text)\n",
    "        return str(blob.correct())"
   ],
   "id": "b120e5be81dc41aa",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.077236Z",
     "start_time": "2025-01-10T14:34:56.585941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_sentence = [nltk.word_tokenize(sentence) for sentence in train_df['preprocessed_text']]\n",
    "max_len = max(len([token for token in sentence.split(\" \")]) for sentence in train_df['preprocessed_text'])\n",
    "print(max_len)\n",
    "vocab, inverse_vocab = build_simple_vocab(train_df['preprocessed_text'])"
   ],
   "id": "434eac424e669b66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.080291Z",
     "start_time": "2025-01-10T14:34:57.077743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = PreprocessingDataset(train_df, vocab, images_path = \"./dataset/train_images/\", train=True, max_len=max_len)\n",
    "val_dataset = PreprocessingDataset(validation_df, vocab, images_path = \"./dataset/val_images/\", train=True, max_len=max_len)\n",
    "test_dataset = PreprocessingDataset(test_df, vocab, images_path = \"./dataset/test_images/\", train=False, max_len=max_len)"
   ],
   "id": "f6a9c5515a874c53",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.084355Z",
     "start_time": "2025-01-10T14:34:57.080817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ],
   "id": "ef7e8ef05ced1727",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.117264Z",
     "start_time": "2025-01-10T14:34:57.084865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "images = batch['images']\n",
    "captions = batch['captions']\n",
    "print(images.shape)\n",
    "print(captions.shape)\n",
    "print(captions[0])"
   ],
   "id": "c15ef9418428fccb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 100, 100])\n",
      "torch.Size([16, 46])\n",
      "tensor([   1, 1838,  247,   55,  151,   46,   33,    1,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.119801Z",
     "start_time": "2025-01-10T14:34:57.117770Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "51c0d07bf84fb724",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.123834Z",
     "start_time": "2025-01-10T14:34:57.119801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ],
   "id": "2d2cc743de7dbeb2",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.128410Z",
     "start_time": "2025-01-10T14:34:57.124341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_dropout_value):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "\n",
    "        self.CNN_block = nn.Sequential(\n",
    "            self.CNN2d_block(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN2d_block(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN2d_block(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN2d_block(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN2d_block(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            \n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            SEBlock(256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, images):\n",
    "        img_features = self.CNN_block(images)  \n",
    "        img_features = self.flatten(img_features)  \n",
    "        return img_features\n",
    "    \n",
    "    def CNN2d_block(self, in_channels, out_channels, kernel_size, stride, padding, cnn_dropout_value):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            SEBlock(out_channels),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        "
   ],
   "id": "f0cf44b7b652262c",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.132473Z",
     "start_time": "2025-01-10T14:34:57.129432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SEBlock1D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock1D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)"
   ],
   "id": "d3b9361117eb34",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.137031Z",
     "start_time": "2025-01-10T14:34:57.132473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextModule(nn.Module):\n",
    "    def __init__(self, cnn_dropout_value, vocab_size, embedding_dim=32):\n",
    "        super(TextModule, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=vocab[\"<pad>\"])\n",
    "\n",
    "        self.CNN1d = nn.Sequential(\n",
    "            self.CNN1d_block(in_channels=embedding_dim, out_channels=32, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN1d_block(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN1d_block(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "            self.CNN1d_block(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, cnn_dropout_value=cnn_dropout_value),\n",
    "\n",
    "            nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            SEBlock1D(256),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, captions):\n",
    "        text_features = self.embedding(captions).permute(0, 2, 1)\n",
    "        text_features = self.CNN1d(text_features)  # \n",
    "        text_features = self.flatten(text_features)  # \n",
    "        return text_features\n",
    "    \n",
    "    def CNN1d_block(self, in_channels, out_channels, kernel_size, stride, padding, cnn_dropout_value):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.Dropout(cnn_dropout_value),\n",
    "            SEBlock1D(out_channels),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )"
   ],
   "id": "e6ea631b4226a0d8",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.140552Z",
     "start_time": "2025-01-10T14:34:57.137539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0, eps=1e-9):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = eps  # small epsilon for numerical stability\n",
    "\n",
    "    def forward(self, img_emb, text_emb, labels):\n",
    "        # Calculate the Euclidean distance between each pair of image and text embeddings\n",
    "        distances = (img_emb - text_emb).pow(2).sum(1).sqrt() + self.eps\n",
    "        \n",
    "        # Contrastive loss calculation\n",
    "        loss = 0.5 * (labels * distances.pow(2) + \n",
    "                      (1 - labels) * torch.clamp(self.margin - distances, min=0.0).pow(2))\n",
    "        \n",
    "        return loss.mean()\n"
   ],
   "id": "5366b540a2446756",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.144640Z",
     "start_time": "2025-01-10T14:34:57.141061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.TransformerEncoderLayer):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.ones_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)"
   ],
   "id": "340278a77ea962ef",
   "outputs": [],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.169994Z",
     "start_time": "2025-01-10T14:34:57.145147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model_config = {\n",
    "    \"cnn_dropout_value\": 0.2,\n",
    "    \"vocab_size\":len(vocab),\n",
    "}\n",
    "\n",
    "image_model = CNN(model_config[\"cnn_dropout_value\"])\n",
    "text_model = TextModule(model_config[\"cnn_dropout_value\"], len(vocab))\n",
    "criterion = ContrastiveLoss()\n",
    "image_model.apply(initialize_weights)\n",
    "text_model.apply(initialize_weights)\n",
    "image_model.to(device)\n",
    "text_model.to(device)\n",
    "\n",
    "\n",
    "# test modules\n",
    "dummy_images = torch.randn(16, 3, 100, 100).to(device)\n",
    "dummy_texts = torch.randint(0, len(vocab), (16, max_len)).to(device)\n",
    "print(image_model(dummy_images).shape)\n",
    "print(text_model(dummy_texts).shape)"
   ],
   "id": "b8e49cebdfb0724c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256])\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.173063Z",
     "start_time": "2025-01-10T14:34:57.170503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(list(image_model.parameters()) + list(text_model.parameters()),  lr=5e-5, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Number of epochs before the first restart\n",
    "    T_mult=1,  # Multiplicative factor for increasing restart period\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")"
   ],
   "id": "cbcdaae8ca3a8e4f",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:34:57.182665Z",
     "start_time": "2025-01-10T14:34:57.173570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def training_method(criterion, optimizer, scheduler, num_epochs, train_loader, val_loader, patience=5, delta=0.05, loss_procentage_improvement=10):\n",
    "    train_losses = []  # List to store training losses\n",
    "    val_losses = []  # List to store validation losses\n",
    "    val_accuracies = []  # List to store validation accuracies\n",
    "    val_precisions = []  # List to store validation precisions\n",
    "    val_recalls = []  # List to store validation recalls\n",
    "    val_f1s = []  # List to store validation F1-scores\n",
    "    learning_rates = []  # List to store learning rate progression\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    initial_loss = float('inf')\n",
    "    best_text_model = None\n",
    "    best_image_model = None\n",
    "    epochs_without_improvement = 0  # Track epochs without improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        ### TRAINING\n",
    "        image_model.train()\n",
    "        text_model.train()\n",
    "        training_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images = batch['images'].to(device)  # Images from batch\n",
    "            captions = batch['captions'].to(device)  # Captions from batch\n",
    "            labels = batch['labels'].to(device).float()  # Binary labels (0/1), converted to float\n",
    "\n",
    "            # Forward pass\n",
    "            img_features = image_model(images)\n",
    "            txt_features = text_model(captions)\n",
    "            loss = criterion(img_features, txt_features, labels)\n",
    "    \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "\n",
    "        train_loss = training_loss / len(train_loader.dataset)  # Average training loss\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        ### VALIDATING\n",
    "        image_model.eval()\n",
    "        text_model.eval()\n",
    "        validation_loss = 0.0\n",
    "        all_labels = []  # Ground truth labels for validation\n",
    "        all_preds = []  # Predictions for validation\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['images'].to(device)\n",
    "                captions = batch['captions'].to(device)\n",
    "                labels = batch['labels'].to(device).float()\n",
    "\n",
    "                # Forward pass\n",
    "                img_features = image_model(images)\n",
    "                txt_features = text_model(captions)\n",
    "                loss = criterion(img_features, txt_features, labels)\n",
    "                validation_loss += loss.item() * images.size(0)\n",
    "                # Compute distances and predictions\n",
    "                distances = torch.norm(img_features - txt_features, p=2, dim=1)\n",
    "                preds = (distances < 0.5).float()\n",
    "\n",
    "\n",
    "                # Store for statistics\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        val_loss = validation_loss / len(val_loader.dataset)  # Average validation loss\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Compute validation statistics\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        val_recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        val_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']  # Get the current learning rate\n",
    "        learning_rates.append(current_lr)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Training Loss: {train_loss:.4f} - \"\n",
    "              f\"Validation Loss: {val_loss:.4f} - \"\n",
    "              f\"Accuracy: {val_accuracy:.4f} - \"\n",
    "              f\"Precision: {val_precision:.4f} - \"\n",
    "              f\"Recall: {val_recall:.4f} - \"\n",
    "              f\"F1 Score: {val_f1:.4f} - \"\n",
    "              f\"Time: {end_time - start_time:.2f} - \"\n",
    "              f\"Lr: {current_lr:.2e}\")\n",
    "\n",
    "        if epoch == 1:\n",
    "            initial_loss = val_loss\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_text_model = copy.deepcopy(text_model)\n",
    "            best_image_model = copy.deepcopy(image_model)\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            print(f\"New best model with Loss: {val_loss:.4f} at epoch {epoch + 1}\")\n",
    "        elif val_loss < best_val_loss + delta:\n",
    "            print(f\"Validation loss did not improve significantly\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Validation loss did not improve for {epochs_without_improvement} epoch(s).\")\n",
    "            # Stop training if validation loss does not improve for 'patience' epochs\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}. Best Loss: {best_val_loss:.4f}\")\n",
    "                break  # Exit training loop\n",
    "\n",
    "    print('Training finished!')\n",
    "\n",
    "    # save the model only if the best loss is lower than the first initial loss ( to see that the model actually improved with 10% loss )\n",
    "    if best_val_loss < (100 - loss_procentage_improvement) * initial_loss:\n",
    "        # Init plot&model save path\n",
    "        # plt_save_path = \"models/\"\n",
    "        # model_config['eval_loss'] = best_val_loss\n",
    "        # for key, value in model_config.items():\n",
    "        #     plt_save_path += key + \"=\" + str(value) + \"+\"\n",
    "        # plt_save_path = plt_save_path[:-1] + \".png\"\n",
    "        # model_path = plt_save_path[:-4] + \".pt\"\n",
    "        # torch.save(best_model.state_dict(), model_path)\n",
    "        # print(f\"Best model with Loss: {best_val_loss:.4f} saved.\")\n",
    "        # print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        # Plotting the losses and validation metrics over epochs\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(val_accuracies, label='Accuracy')\n",
    "        plt.plot(val_precisions, label='Precision')\n",
    "        plt.plot(val_recalls, label='Recall')\n",
    "        plt.plot(val_f1s, label='F1 Score')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Metric')\n",
    "        plt.title('Validation Metrics')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(learning_rates, label='Learning Rate')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.title(\"Learning Rate Progression\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig(plt_save_path)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Model wasn't saved because it didn't improve: {loss_procentage_improvement}%\")\n",
    "    \n",
    "    return best_text_model, best_image_model"
   ],
   "id": "6280acf86aa228a2",
   "outputs": [],
   "execution_count": 179
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-10T14:34:57.182665Z"
    }
   },
   "cell_type": "code",
   "source": "best_text_model, best_image_model = training_method(criterion, optimizer, scheduler, num_epochs=50, train_loader=train_dataloader, val_loader=val_dataloader)",
   "id": "ee2f8b5c78457e56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def make_submission(test_loader, text_model, image_model):\n",
    "    image_model.eval()\n",
    "    text_model.eval()\n",
    "    ids = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            id = batch['id']\n",
    "\n",
    "            img_features = image_model(images)\n",
    "            txt_features = text_model(captions)\n",
    "            # Compute distances and predictions\n",
    "            distances = torch.norm(img_features - txt_features, p=2, dim=1)\n",
    "            preds = (distances < 0.5).int()\n",
    "\n",
    "            ids.extend(id)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "    df = pd.DataFrame({'id': ids, 'label': predictions})\n",
    "    df.to_csv('submission5.csv', index=False) \n",
    "    # TODO: \n",
    "    #  1.Send submission4.csv\n",
    "    #  2.Test with spell corection"
   ],
   "id": "95402ff5c7357c82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# # LOAD MODEL FROM PATH\n",
    "# model_config = {\n",
    "# ADD\n",
    "# }\n",
    "# model = ImageTextClassifier(**model_config)\n",
    "# model_path = \"vocab_size=3733+embedding_dim=128+num_filters=128+filter_sizes=[3, 4, 5, 6, 7, 8]+seq_len=53+cnn_text_drop_value=0.5+cnn_dropout_value=0.4+head_dropout_value=0.5+num_classes=1+eval_loss=0.6202353974606128.pt\"\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "# model.to(device)"
   ],
   "id": "595baf65e18e78cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "make_submission(test_dataloader, best_text_model, best_image_model)",
   "id": "6cc9a81e05125a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def hyperparameter_tuning(vocab_size, max_len, train_loader, val_loader, param_grid, training_method, num_epochs=200):\n",
    "    # Create all combinations of hyperparameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    for params in tqdm(param_combinations):\n",
    "        print(f\"Testing configuration: {params}\")\n",
    "\n",
    "        try:\n",
    "            # Update model configuration\n",
    "            model_config = {\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"embedding_dim\": params[\"embedding_dim\"],\n",
    "                \"num_filters\": params[\"num_filters\"],\n",
    "                \"filter_sizes\": params[\"filter_sizes\"],\n",
    "                \"seq_len\": max_len,\n",
    "                \"cnn_text_drop_value\": params[\"cnn_text_drop_value\"],\n",
    "                \"cnn_dropout_value\": params[\"cnn_dropout_value\"],\n",
    "                \"head_dropout_value\": params[\"head_dropout_value\"],\n",
    "            }\n",
    "\n",
    "            # Initialize model\n",
    "            model = ImageTextClassifier(**model_config)\n",
    "            model.to(device)\n",
    "            model.apply(initialize_weights)\n",
    "\n",
    "            # Define criterion, optimizer, and scheduler\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=params[\"lr\"],\n",
    "                weight_decay=params[\"weight_decay\"]\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer,\n",
    "                T_0=params[\"T_0\"],\n",
    "                eta_min=params[\"eta_min\"],\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            training_method(\n",
    "                model, criterion, optimizer, scheduler,\n",
    "                num_epochs=num_epochs,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader\n",
    "            )\n",
    "            print(f\"Completed configuration: {params}\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error with configuration: {params}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "\n",
    "        finally:\n",
    "            # Reset GPU memory\n",
    "            print(\"Resetting GPU memory...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()"
   ],
   "id": "cf6ef0fe03c73deb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    \"embedding_dim\": [128],\n",
    "    \"num_filters\": [16],\n",
    "    \"filter_sizes\": [[3, 4, 5], [3, 4, 5, 6, 7, 8, 9]],\n",
    "    \"head_dropout_value\": [0.5],\n",
    "    \"cnn_text_drop_value\": [0.5],\n",
    "    \"cnn_dropout_value\": [0.5],\n",
    "    \"lr\": [1e-5, 1e-4, 5e-4, 1e-3, 5e-3],  # Learning rate candidates\n",
    "    \"weight_decay\": [1e-6, 1e-5, 1e-4, 1e-3],  # Weight decay candidates\n",
    "    \"T_0\": [10],  # Number of epochs for the first cycle\n",
    "    \"T_mult\": [1],  # Cycle multiplier\n",
    "    \"eta_min\": [1e-6, 1e-5, 1e-4],  # Minimum learning rate\n",
    "}\n",
    "\n",
    "total_combinations = math.prod(len(values) for values in param_grid.values())\n",
    "print(f\"Total combinations: {total_combinations}\")\n",
    "\n",
    "time_per_epoch = 23  # seconds\n",
    "num_epochs = 100  # epochs per configuration\n",
    "total_time_seconds = total_combinations * time_per_epoch * num_epochs\n",
    "\n",
    "# Convert to hours\n",
    "total_time_hours = total_time_seconds / 3600\n",
    "print(f\"Total time to hyper tune: {total_time_hours} hours\")"
   ],
   "id": "b9f5a5e907d2c63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "results = hyperparameter_tuning(\n",
    "    vocab_size=len(vocab),\n",
    "    max_len=max_len,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    param_grid=param_grid,\n",
    "    training_method=training_method,\n",
    "    num_epochs=100\n",
    ")"
   ],
   "id": "2b980f756d58665b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "dir_models = os.listdir(\"./models\")",
   "id": "b13f77f45721bd1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "best_models = [path[:-3] for path in dir_models if path.endswith(\".pt\")]",
   "id": "d4641d6ec9100cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "best_val_loss = [float(str(best_model.split(\"+\")[-1:]).split(\"=\")[1][:8]) for best_model in best_models]",
   "id": "b66b0b18ddd45eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_loss.sort()\n",
    "best_val_loss[:100]"
   ],
   "id": "f8072a414588fe28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "db3c79d743aaea6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
