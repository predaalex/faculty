{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 1",
   "id": "562cde097770a053"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.427261Z",
     "start_time": "2024-06-11T18:50:35.421685Z"
    }
   },
   "source": [
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "dab294e327b11e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.457752Z",
     "start_time": "2024-06-11T18:50:35.433393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "car_image_names_path = './data/vehicles'\n",
    "car_image_names = [f\"{car_image_names_path}/{path}\" for path in os.listdir(car_image_names_path)]\n",
    "non_car_image_names_path = './data/non-vehicles'\n",
    "non_car_image_names = [f\"{non_car_image_names_path}/{path}\" for path in os.listdir(non_car_image_names_path)]\n",
    "car_image_names[:3], non_car_image_names[:3]"
   ],
   "id": "97d34314dc634b70",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.473458Z",
     "start_time": "2024-06-11T18:50:35.458776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "images.extend(car_image_names)\n",
    "labels.extend([1 for _ in range(len(car_image_names))])\n",
    "\n",
    "images.extend(non_car_image_names)\n",
    "labels.extend([0 for _ in range(len(non_car_image_names))])\n",
    "len(images), len(labels)"
   ],
   "id": "78f93572e8d272f",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.488803Z",
     "start_time": "2024-06-11T18:50:35.474506Z"
    }
   },
   "cell_type": "code",
   "source": "images[0]",
   "id": "fe08e6b2520d7664",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.504085Z",
     "start_time": "2024-06-11T18:50:35.489810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "len(x_train), len(x_test)"
   ],
   "id": "a7e003544ea56456",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.519324Z",
     "start_time": "2024-06-11T18:50:35.505135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "id": "7b387a922f75c9a0",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.534668Z",
     "start_time": "2024-06-11T18:50:35.520374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to match ResNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "training_dataset = VehicleDataset(x_train, y_train, transform=transformer)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "testing_dataset = VehicleDataset(x_test, y_test, transform=transformer)\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=64, shuffle=True)"
   ],
   "id": "e3f2bc71088f13fe",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.703556Z",
     "start_time": "2024-06-11T18:50:35.535673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet34(pretrained=True)"
   ],
   "id": "3bd70f80272fd880",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.719192Z",
     "start_time": "2024-06-11T18:50:35.704561Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "c0842f7a2707a985",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.734472Z",
     "start_time": "2024-06-11T18:50:35.720240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "num_classes = 1\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")"
   ],
   "id": "221f375bd02b9091",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.750149Z",
     "start_time": "2024-06-11T18:50:35.735476Z"
    }
   },
   "cell_type": "code",
   "source": "criterion = nn.BCELoss()",
   "id": "67d84632b8f0cae8",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.765310Z",
     "start_time": "2024-06-11T18:50:35.751662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "freeze_layers = 2 # the number of last layers to be unfreezed\n",
    "final_layers_params = []\n",
    "rest_of_model_params = []\n",
    "\n",
    "nr_of_layers = 0\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    nr_of_layers += 1\n",
    "    \n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    if idx >= nr_of_layers - freeze_layers:\n",
    "        print(idx)\n",
    "        final_layers_params.append(param)\n",
    "    else:\n",
    "        # param.requires_grad = False\n",
    "        rest_of_model_params.append(param)        "
   ],
   "id": "de6f0324687956fa",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.780579Z",
     "start_time": "2024-06-11T18:50:35.766361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Separate the parameters for the final layer and the rest of the model\n",
    "final_layer_params = list(model.fc.parameters())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': rest_of_model_params, 'lr': 1e-5},\n",
    "    {'params': final_layer_params, 'lr': 0.001}\n",
    "])\n",
    "epochs = 3"
   ],
   "id": "53974fdd1a7985d6",
   "execution_count": 52,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.796244Z",
     "start_time": "2024-06-11T18:50:35.781584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from tqdm import tqdm\n",
    "# \n",
    "# num_training_steps = epochs * len(training_dataloader)\n",
    "# progress_bar = tqdm(range(num_training_steps))"
   ],
   "id": "9fe9a03d6514e9de",
   "execution_count": 53,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:50:35.811427Z",
     "start_time": "2024-06-11T18:50:35.798298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "score_threshold = 0.8\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
    "    # Set the device to CUDA if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = (outputs >= score_threshold).float()\n",
    "            train_corrects += torch.sum(preds == labels.data)\n",
    "            # progress_bar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = train_corrects.double() / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                preds = (outputs >= score_threshold).float()\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    return model"
   ],
   "id": "d878726843a50577",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:51:07.676200Z",
     "start_time": "2024-06-11T18:50:35.812484Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_model(model, training_dataloader, testing_dataloader, criterion, optimizer, num_epochs=epochs)",
   "id": "4a8bb71a723cebc3",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:51:07.752463Z",
     "start_time": "2024-06-11T18:51:07.677233Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'model.pth')",
   "id": "d4323c1a9eab324d",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's fetch data from project training data",
   "id": "2894d4edf2a65083"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:51:07.767610Z",
     "start_time": "2024-06-11T18:51:07.753488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project_path = \"../train/Task1\"\n",
    "\n",
    "images_paths = []\n",
    "query_paths = []\n",
    "gt_query_paths = os.listdir(project_path + \"/ground-truth\")\n",
    "\n",
    "file_paths = os.listdir(project_path)\n",
    "for file_path in file_paths:\n",
    "    if file_path.endswith(\".jpg\"):\n",
    "        images_paths.append(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        query_paths.append(file_path)\n",
    "\n",
    "\n",
    "images_paths.sort()\n",
    "query_paths.sort()\n",
    "gt_query_paths.sort()\n",
    "\n",
    "print(images_paths, len(images_paths))\n",
    "print(query_paths, len(query_paths))\n",
    "print(gt_query_paths, len(gt_query_paths))"
   ],
   "id": "60a5827f9001e2c0",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test image",
   "id": "d8851eec48e828d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:52:40.723566Z",
     "start_time": "2024-06-11T18:52:40.703280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "test_img_path = 'test.png'\n",
    "\n",
    "img = Image.open(test_img_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "\n",
    "img = transformer(img)\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img = img.to(device)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(img)\n",
    "    predicted_class = (outputs >= score_threshold).float().item()\n",
    "    print(outputs)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ],
   "id": "2e6fe4034f575c8",
   "execution_count": 67,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:51:31.018360Z",
     "start_time": "2024-06-11T18:51:31.011790Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "495f9b6a815d09a3",
   "execution_count": 62,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
