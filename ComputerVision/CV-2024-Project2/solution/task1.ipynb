{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 1",
   "id": "562cde097770a053"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.754356Z",
     "start_time": "2024-06-15T09:53:55.568246Z"
    }
   },
   "source": [
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.769536Z",
     "start_time": "2024-06-15T09:53:57.755381Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "93c2921720cbabc9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "dab294e327b11e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.800340Z",
     "start_time": "2024-06-15T09:53:57.770569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "car_image_names_path = './data/vehicles'\n",
    "car_image_names = [f\"{car_image_names_path}/{path}\" for path in os.listdir(car_image_names_path)]\n",
    "non_car_image_names_path = './data/non-vehicles'\n",
    "non_car_image_names = [f\"{non_car_image_names_path}/{path}\" for path in os.listdir(non_car_image_names_path)]\n",
    "car_image_names[:3], non_car_image_names[:3]"
   ],
   "id": "97d34314dc634b70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['./data/vehicles/1.png',\n",
       "  './data/vehicles/10.png',\n",
       "  './data/vehicles/1000.png'],\n",
       " ['./data/non-vehicles/extra1911.png',\n",
       "  './data/non-vehicles/extra1912.png',\n",
       "  './data/non-vehicles/extra1913.png'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.815521Z",
     "start_time": "2024-06-15T09:53:57.801375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "images.extend(car_image_names)\n",
    "labels.extend([1 for _ in range(len(car_image_names))])\n",
    "\n",
    "images.extend(non_car_image_names)\n",
    "labels.extend([0 for _ in range(len(non_car_image_names))])\n",
    "len(images), len(labels)"
   ],
   "id": "78f93572e8d272f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17487, 17487)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.830705Z",
     "start_time": "2024-06-15T09:53:57.816562Z"
    }
   },
   "cell_type": "code",
   "source": "images[0]",
   "id": "fe08e6b2520d7664",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/vehicles/1.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.846343Z",
     "start_time": "2024-06-15T09:53:57.831730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "len(x_train), len(x_test)"
   ],
   "id": "a7e003544ea56456",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13989, 3498)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.861511Z",
     "start_time": "2024-06-15T09:53:57.847382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.images[idx]).float() / 255.0\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image.to(device), torch.tensor(label, device=device, dtype=torch.float)"
   ],
   "id": "7b387a922f75c9a0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.876668Z",
     "start_time": "2024-06-15T09:53:57.862545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match ResNet input size\n",
    "])\n",
    "\n",
    "training_dataset = VehicleDataset(x_train, y_train, transform=transformer)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "testing_dataset = VehicleDataset(x_test, y_test, transform=transformer)\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=64, shuffle=True)"
   ],
   "id": "e3f2bc71088f13fe",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:57.999499Z",
     "start_time": "2024-06-15T09:53:57.877698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = next(iter(training_dataloader))\n",
    "print(batch[0].shape, batch[1].shape)"
   ],
   "id": "12591879a3eb2fa0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224]) torch.Size([64])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:58.199235Z",
     "start_time": "2024-06-15T09:53:58.000548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet34(pretrained=True)"
   ],
   "id": "3bd70f80272fd880",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allex\\miniconda3\\envs\\CV\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\allex\\miniconda3\\envs\\CV\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:58.214428Z",
     "start_time": "2024-06-15T09:53:58.200274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "num_classes = 1\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")"
   ],
   "id": "221f375bd02b9091",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:58.229634Z",
     "start_time": "2024-06-15T09:53:58.215473Z"
    }
   },
   "cell_type": "code",
   "source": "criterion = nn.BCELoss()",
   "id": "67d84632b8f0cae8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:58.244819Z",
     "start_time": "2024-06-15T09:53:58.230668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "freeze_layers = 2 # the number of last layers to be unfreezed\n",
    "final_layers_params = []\n",
    "rest_of_model_params = []\n",
    "\n",
    "nr_of_layers = 0\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    nr_of_layers += 1\n",
    "    \n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    if idx >= nr_of_layers - freeze_layers:\n",
    "        print(idx)\n",
    "        final_layers_params.append(param)\n",
    "    else:\n",
    "        # param.requires_grad = False\n",
    "        rest_of_model_params.append(param)        "
   ],
   "id": "de6f0324687956fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "109\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:58.260524Z",
     "start_time": "2024-06-15T09:53:58.247378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Separate the parameters for the final layer and the rest of the model\n",
    "final_layer_params = list(model.fc.parameters())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': rest_of_model_params, 'lr': 1e-5},\n",
    "    {'params': final_layer_params, 'lr': 1e-3}\n",
    "])\n",
    "epochs = 3"
   ],
   "id": "53974fdd1a7985d6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:58.291453Z",
     "start_time": "2024-06-15T09:53:58.264589Z"
    }
   },
   "cell_type": "code",
   "source": "model.to(device)",
   "id": "6878fb1920b9f8ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:53:58.306618Z",
     "start_time": "2024-06-15T09:53:58.292491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "score_threshold = 0.8\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_corrects = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            labels = labels.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = (outputs >= score_threshold).float()\n",
    "            train_corrects += torch.sum(preds == labels.data)\n",
    "            # progress_bar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = train_corrects.double() / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                labels = labels.unsqueeze(1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                preds = (outputs >= score_threshold).float()\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    return model"
   ],
   "id": "d878726843a50577",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:55:39.508291Z",
     "start_time": "2024-06-15T09:53:58.307650Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_model(model, training_dataloader, testing_dataloader, criterion, optimizer, num_epochs=epochs)",
   "id": "4a8bb71a723cebc3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:29<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.0517, Train Accuracy: 0.9650\n",
      "Validation Loss: 0.0049, Validation Accuracy: 0.9986\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:29<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.0049, Train Accuracy: 0.9986\n",
      "Validation Loss: 0.0083, Validation Accuracy: 0.9983\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:30<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.0015, Train Accuracy: 0.9994\n",
      "Validation Loss: 0.0081, Validation Accuracy: 0.9983\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:55:39.615506Z",
     "start_time": "2024-06-15T09:55:39.509329Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'model.pth')",
   "id": "d4323c1a9eab324d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:55:39.677435Z",
     "start_time": "2024-06-15T09:55:39.616540Z"
    }
   },
   "cell_type": "code",
   "source": "model.load_state_dict(torch.load('model.pth', map_location=device))",
   "id": "da819c9c38e8f78f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:58:49.533622Z",
     "start_time": "2024-06-15T09:58:49.515413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "wrong_predictions_images = []\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_corrects = 0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        labels = labels.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= score_threshold).float()\n",
    "        \n",
    "        for img, pred, in zip(inputs, preds):\n",
    "            if pred == pred:\n",
    "                test_corrects += 1\n",
    "            else:\n",
    "                wrong_predictions_images.append(img)\n",
    "        \n",
    "    val_loss = test_loss / len(test_loader.dataset)\n",
    "    val_acc = test_corrects / len(test_loader.dataset)\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "    print(\"--------------------------------------------------------------------\")"
   ],
   "id": "954ba6efacfb1ffa",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:58:55.295256Z",
     "start_time": "2024-06-15T09:58:51.615573Z"
    }
   },
   "cell_type": "code",
   "source": "test_model(model, testing_dataloader)",
   "id": "99d9ab9d3f6f78bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0081, Validation Accuracy: 1.0000\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:58:57.948284Z",
     "start_time": "2024-06-15T09:58:57.930444Z"
    }
   },
   "cell_type": "code",
   "source": "wrong_predictions_images",
   "id": "4915b1dd28c9d614",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's fetch data from project training data",
   "id": "2894d4edf2a65083"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:55:39.784556Z",
     "start_time": "2024-06-15T09:55:39.770411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = \"../train/Task1\"\n",
    "\n",
    "images_paths = []\n",
    "query_paths = []\n",
    "gt_query_paths = os.listdir(data_path + \"/ground-truth\")\n",
    "\n",
    "file_paths = os.listdir(data_path)\n",
    "for file_path in file_paths:\n",
    "    if file_path.endswith(\".jpg\"):\n",
    "        images_paths.append(file_path)\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        query_paths.append(file_path)\n",
    "\n",
    "images_paths.sort()\n",
    "query_paths.sort()\n",
    "gt_query_paths.sort()\n",
    "\n",
    "print(images_paths, len(images_paths))\n",
    "print(query_paths, len(query_paths))\n",
    "print(gt_query_paths, len(gt_query_paths))"
   ],
   "id": "60a5827f9001e2c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01_1.jpg', '01_2.jpg', '01_3.jpg', '02_1.jpg', '02_2.jpg', '02_3.jpg', '03_1.jpg', '03_2.jpg', '03_3.jpg', '04_1.jpg', '04_2.jpg', '04_3.jpg', '05_1.jpg', '05_2.jpg', '05_3.jpg', '06_1.jpg', '06_2.jpg', '06_3.jpg', '07_1.jpg', '07_2.jpg', '07_3.jpg', '08_1.jpg', '08_2.jpg', '08_3.jpg', '09_1.jpg', '09_2.jpg', '09_3.jpg', '10_1.jpg', '10_2.jpg', '10_3.jpg', '11_1.jpg', '11_2.jpg', '11_3.jpg', '11_4.jpg', '12_1.jpg', '12_2.jpg', '12_3.jpg', '12_4.jpg', '13_1.jpg', '13_2.jpg', '13_3.jpg', '13_4.jpg', '14_1.jpg', '14_2.jpg', '14_3.jpg', '14_4.jpg', '15_1.jpg', '15_2.jpg', '15_3.jpg', '15_4.jpg'] 50\n",
      "['01_1_query.txt', '01_2_query.txt', '01_3_query.txt', '02_1_query.txt', '02_2_query.txt', '02_3_query.txt', '03_1_query.txt', '03_2_query.txt', '03_3_query.txt', '04_1_query.txt', '04_2_query.txt', '04_3_query.txt', '05_1_query.txt', '05_2_query.txt', '05_3_query.txt', '06_1_query.txt', '06_2_query.txt', '06_3_query.txt', '07_1_query.txt', '07_2_query.txt', '07_3_query.txt', '08_1_query.txt', '08_2_query.txt', '08_3_query.txt', '09_1_query.txt', '09_2_query.txt', '09_3_query.txt', '10_1_query.txt', '10_2_query.txt', '10_3_query.txt', '11_1_query.txt', '11_2_query.txt', '11_3_query.txt', '11_4_query.txt', '12_1_query.txt', '12_2_query.txt', '12_3_query.txt', '12_4_query.txt', '13_1_query.txt', '13_2_query.txt', '13_3_query.txt', '13_4_query.txt', '14_1_query.txt', '14_2_query.txt', '14_3_query.txt', '14_4_query.txt', '15_1_query.txt', '15_2_query.txt', '15_3_query.txt', '15_4_query.txt'] 50\n",
      "['01_1_gt.txt', '01_2_gt.txt', '01_3_gt.txt', '02_1_gt.txt', '02_2_gt.txt', '02_3_gt.txt', '03_1_gt.txt', '03_2_gt.txt', '03_3_gt.txt', '04_1_gt.txt', '04_2_gt.txt', '04_3_gt.txt', '05_1_gt.txt', '05_2_gt.txt', '05_3_gt.txt', '06_1_gt.txt', '06_2_gt.txt', '06_3_gt.txt', '07_1_gt.txt', '07_2_gt.txt', '07_3_gt.txt', '08_1_gt.txt', '08_2_gt.txt', '08_3_gt.txt', '09_1_gt.txt', '09_2_gt.txt', '09_3_gt.txt', '10_1_gt.txt', '10_2_gt.txt', '10_3_gt.txt', '11_1_gt.txt', '11_2_gt.txt', '11_3_gt.txt', '11_4_gt.txt', '12_1_gt.txt', '12_2_gt.txt', '12_3_gt.txt', '12_4_gt.txt', '13_1_gt.txt', '13_2_gt.txt', '13_3_gt.txt', '13_4_gt.txt', '14_1_gt.txt', '14_2_gt.txt', '14_3_gt.txt', '14_4_gt.txt', '15_1_gt.txt', '15_2_gt.txt', '15_3_gt.txt', '15_4_gt.txt'] 50\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test specific image",
   "id": "d8851eec48e828d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:55:40.524524Z",
     "start_time": "2024-06-15T09:55:39.785616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "test_img_path = 'test.png'\n",
    "\n",
    "img = read_image(test_img_path).float() / 255.0\n",
    "\n",
    "img = transformer(img)\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "img = img.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(img)\n",
    "    predicted_class = (outputs >= score_threshold).float().item()\n",
    "    print(outputs)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ],
   "id": "2e6fe4034f575c8",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[Errno 2] No such file or directory: 'test.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 6\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m\n\u001B[0;32m      4\u001B[0m test_img_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest.png\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 6\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mread_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_img_path\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n\u001B[0;32m      8\u001B[0m img \u001B[38;5;241m=\u001B[39m transformer(img)\n\u001B[0;32m      9\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\CV\\lib\\site-packages\\torchvision\\io\\image.py:275\u001B[0m, in \u001B[0;36mread_image\u001B[1;34m(path, mode, apply_exif_orientation)\u001B[0m\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_tracing():\n\u001B[0;32m    274\u001B[0m     _log_api_usage_once(read_image)\n\u001B[1;32m--> 275\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mread_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m decode_image(data, mode, apply_exif_orientation\u001B[38;5;241m=\u001B[39mapply_exif_orientation)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\CV\\lib\\site-packages\\torchvision\\io\\image.py:52\u001B[0m, in \u001B[0;36mread_file\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_tracing():\n\u001B[0;32m     51\u001B[0m     _log_api_usage_once(read_file)\n\u001B[1;32m---> 52\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_file\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\CV\\lib\\site-packages\\torch\\_ops.py:854\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[1;34m(self_, *args, **kwargs)\u001B[0m\n\u001B[0;32m    846\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(self_, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[0;32m    847\u001B[0m     \u001B[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001B[39;00m\n\u001B[0;32m    848\u001B[0m     \u001B[38;5;66;03m# named \"self\". This way, all the aten ops can be called by kwargs.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    852\u001B[0m     \u001B[38;5;66;03m# We save the function ptr as the `op` attribute on\u001B[39;00m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;66;03m# OpOverloadPacket to access it here.\u001B[39;00m\n\u001B[1;32m--> 854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m self_\u001B[38;5;241m.\u001B[39m_op(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(kwargs \u001B[38;5;129;01mor\u001B[39;00m {}))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [Errno 2] No such file or directory: 'test.png'"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T09:55:40.525563Z",
     "start_time": "2024-06-15T09:55:40.525563Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a82897b9fb87d1e8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
